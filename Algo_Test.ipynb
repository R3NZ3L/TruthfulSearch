{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "760902a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import regex as re\n",
    "from jellyfish import jaro_similarity\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb6bde8d-9493-4c2a-801c-5be1cfe30753",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"covid_philippines\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09e489b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets/\" + filename + \"/\" + filename + \".csv\"\n",
    "df = pd.read_csv(path).drop(\"Unnamed: 0\", axis=1)\n",
    "\n",
    "'''\n",
    "video_channel = df[[\"video_id\", \"video_title\", \"channel_id\"]]\n",
    "unique_channels = df[\"channel_name\"].unique()\n",
    "view_like_comment = np.array(df[[\"view_count\", \"like_count\", \"comment_count\"]])\n",
    "'''\n",
    "\n",
    "query_tail = [\n",
    "    \" LinkedIn\",\n",
    "    \" Wiki\",\n",
    "    \" Official Website\",\n",
    "    \" Facebook\",\n",
    "    \" Twitter\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04a91fc4-d9c3-4597-bae1-19a3ade404a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>description</th>\n",
       "      <th>video_dop</th>\n",
       "      <th>view_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>video_transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>Normally crowded streets of Philippine capital...</td>\n",
       "      <td>Subscribe to our YouTube channel for free here...</td>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>325303</td>\n",
       "      <td>3291</td>\n",
       "      <td>619</td>\n",
       "      <td>UC4SUWizzKc1tptprBkWjX2Q</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sYI97jv-pZg</td>\n",
       "      <td>PH records 2,725 new COVID cases from Dec. 12 ...</td>\n",
       "      <td>The World Tonight: The daily average of the Ph...</td>\n",
       "      <td>2023-12-18</td>\n",
       "      <td>2325</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>UCvi6hEzLM-Z_unKPSuuzKvg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DWxIvQlpJK8</td>\n",
       "      <td>Metro Manila to be placed on lockdown due to c...</td>\n",
       "      <td>Subscribe: https://www.youtube.com/@Rappler/\\n...</td>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>107238</td>\n",
       "      <td>750</td>\n",
       "      <td>19</td>\n",
       "      <td>UCdnZdQxYXnbN4uWJg96oGxw</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3YFpjgIQqEo</td>\n",
       "      <td>WATCH: DOH Usec. Tayag on rise of COVID-19 cas...</td>\n",
       "      <td>Dateline Philippines: Karmina Constantino talk...</td>\n",
       "      <td>2023-12-07</td>\n",
       "      <td>9808</td>\n",
       "      <td>81</td>\n",
       "      <td>17</td>\n",
       "      <td>UCvi6hEzLM-Z_unKPSuuzKvg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cPVE7QGS7As</td>\n",
       "      <td>Philippines faces worst COVID-19 crisis in Sou...</td>\n",
       "      <td>The Philippines has the highest number of coro...</td>\n",
       "      <td>2020-08-19</td>\n",
       "      <td>127851</td>\n",
       "      <td>906</td>\n",
       "      <td>533</td>\n",
       "      <td>UCNye-wNBqNL5ZzHSJj3l8Bg</td>\n",
       "      <td>joey dejos is checking whether his cash regis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                        video_title  \\\n",
       "0  aLZ85hb4wjE  Normally crowded streets of Philippine capital...   \n",
       "1  sYI97jv-pZg  PH records 2,725 new COVID cases from Dec. 12 ...   \n",
       "2  DWxIvQlpJK8  Metro Manila to be placed on lockdown due to c...   \n",
       "3  3YFpjgIQqEo  WATCH: DOH Usec. Tayag on rise of COVID-19 cas...   \n",
       "4  cPVE7QGS7As  Philippines faces worst COVID-19 crisis in Sou...   \n",
       "\n",
       "                                         description   video_dop  view_count  \\\n",
       "0  Subscribe to our YouTube channel for free here...  2020-04-02      325303   \n",
       "1  The World Tonight: The daily average of the Ph...  2023-12-18        2325   \n",
       "2  Subscribe: https://www.youtube.com/@Rappler/\\n...  2020-03-12      107238   \n",
       "3  Dateline Philippines: Karmina Constantino talk...  2023-12-07        9808   \n",
       "4  The Philippines has the highest number of coro...  2020-08-19      127851   \n",
       "\n",
       "   like_count  comment_count                channel_id  \\\n",
       "0        3291            619  UC4SUWizzKc1tptprBkWjX2Q   \n",
       "1          15              3  UCvi6hEzLM-Z_unKPSuuzKvg   \n",
       "2         750             19  UCdnZdQxYXnbN4uWJg96oGxw   \n",
       "3          81             17  UCvi6hEzLM-Z_unKPSuuzKvg   \n",
       "4         906            533  UCNye-wNBqNL5ZzHSJj3l8Bg   \n",
       "\n",
       "                                    video_transcript  \n",
       "0                                                NaN  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3                                                NaN  \n",
       "4   joey dejos is checking whether his cash regis...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fed69ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_id</th>\n",
       "      <th>channel_name</th>\n",
       "      <th>channel_dop</th>\n",
       "      <th>sub_count</th>\n",
       "      <th>total_videos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UC4SUWizzKc1tptprBkWjX2Q</td>\n",
       "      <td>South China Morning Post</td>\n",
       "      <td>2007-01-18</td>\n",
       "      <td>3820000</td>\n",
       "      <td>17050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UCvi6hEzLM-Z_unKPSuuzKvg</td>\n",
       "      <td>ANC 24/7</td>\n",
       "      <td>2010-01-29</td>\n",
       "      <td>1550000</td>\n",
       "      <td>73005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UCdnZdQxYXnbN4uWJg96oGxw</td>\n",
       "      <td>Rappler</td>\n",
       "      <td>2011-12-02</td>\n",
       "      <td>1990000</td>\n",
       "      <td>50666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UCNye-wNBqNL5ZzHSJj3l8Bg</td>\n",
       "      <td>Al Jazeera English</td>\n",
       "      <td>2006-11-23</td>\n",
       "      <td>12900000</td>\n",
       "      <td>111538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UCvRAX-ujvZ0eTMLGG2vki9w</td>\n",
       "      <td>INQUIRER.net</td>\n",
       "      <td>2012-03-15</td>\n",
       "      <td>1660000</td>\n",
       "      <td>34985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 channel_id              channel_name channel_dop  sub_count  \\\n",
       "0  UC4SUWizzKc1tptprBkWjX2Q  South China Morning Post  2007-01-18    3820000   \n",
       "1  UCvi6hEzLM-Z_unKPSuuzKvg                  ANC 24/7  2010-01-29    1550000   \n",
       "2  UCdnZdQxYXnbN4uWJg96oGxw                   Rappler  2011-12-02    1990000   \n",
       "3  UCNye-wNBqNL5ZzHSJj3l8Bg        Al Jazeera English  2006-11-23   12900000   \n",
       "4  UCvRAX-ujvZ0eTMLGG2vki9w              INQUIRER.net  2012-03-15    1660000   \n",
       "\n",
       "   total_videos  \n",
       "0         17050  \n",
       "1         73005  \n",
       "2         50666  \n",
       "3        111538  \n",
       "4         34985  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = path = \"datasets/\" + filename + \"/\" + filename + \"_channels.csv\"\n",
    "channel_df = pd.read_csv(path).drop(\"Unnamed: 0\", axis=1)\n",
    "channel_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "128c76ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of queries: 165\n"
     ]
    }
   ],
   "source": [
    "num_query = 0\n",
    "unique_channels = channel_df[[\"channel_id\", \"channel_name\"]]\n",
    "\n",
    "# Per channel name\n",
    "for channel_id in unique_channels[\"channel_id\"].to_list():\n",
    "    \n",
    "    # Per query type\n",
    "    for j in range(0, len(query_tail)):\n",
    "        num_query += 1\n",
    "\n",
    "print(\"Total number of queries: \" + str(num_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20aa3c4",
   "metadata": {},
   "source": [
    "## NOTE\n",
    "Verifiability score is computed <u>PER CHANNEL</u> <br>\n",
    "<br>\n",
    "Ranking is computed <u>PER VIDEO</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e41132b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9619d57b",
   "metadata": {},
   "source": [
    "Google resource initialization\n",
    "- Query and Channel name are manually declared as to simulate the search process for a single channel name\n",
    "- The actual loop for searching and verifying across a dataset of videos will be done in the .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae2d17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your personal API key here\n",
    "apiKey = 'AIzaSyCIplXpNgYZ2IS44ZYyEi-hXRu1gzl9I58'\n",
    "\n",
    "# Search engine ID\n",
    "cseKey = \"23c1c70a203ac4852\"\n",
    "\n",
    "google_resource = build(\"customsearch\", \"v1\", developerKey=apiKey).cse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e4c3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_name = \"CNN Philippines\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7954c2c2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0d052c",
   "metadata": {},
   "source": [
    "Finding a LinkedIn Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aacce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = channel_name + query_tail[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bfe0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = google_resource.list(\n",
    "    q=query,\n",
    "    cx=cseKey\n",
    ").execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f395f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'https:\\/\\/www\\.linkedin\\.com\\/(company|in)\\/.+' # Used to find specific profile links\n",
    "\n",
    "linkedIn = False\n",
    "for i in range(0,10):\n",
    "    if re.search(pattern, response.get(\"items\")[i].get(\"formattedUrl\")) != None:\n",
    "        # Get profile name from search result\n",
    "        profile_name = re.search(r'\\w+\\s(\\w+)?', response.get(\"items\")[i].get(\"htmlTitle\")).group()\n",
    "        \n",
    "        # Get similarity between found profile name and channel name\n",
    "        # This is to prevent false positives in finding a LinkedIn profile\n",
    "        similarity = round(jaro_similarity(channel_name, profile_name), 2)\n",
    "        \n",
    "        # If n% similar, consider LinkedIn profile as found\n",
    "        if similarity >= 0.80:\n",
    "            linkedIn = True\n",
    "            break\n",
    "    \n",
    "if linkedIn:\n",
    "    print(str(linkedIn) + \", at index [\" + str(i) + \"] with \" + str(similarity * 100) + \"% similarity.\")\n",
    "    print(\"Link found: \" + profile_name + \" @ \" + response.get(\"items\")[i].get(\"link\"))\n",
    "else:\n",
    "    print(\"No LinkedIn profile found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07275d58",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8b7325",
   "metadata": {},
   "source": [
    "Finding a Wiki page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729a1cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = channel_name + query_tail[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb5a479",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = google_resource.list(\n",
    "    q=query,\n",
    "    cx=cseKey\n",
    ").execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e43cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'https:\\/\\/\\w{2}.wikipedia\\.org\\/wiki\\/.+'\n",
    "\n",
    "wiki = False\n",
    "for i in range(0,10):\n",
    "    if re.search(pattern, response.get(\"items\")[i].get(\"formattedUrl\")) != None:\n",
    "        # Get Wiki page name from search result\n",
    "        title = response.get(\"items\")[i].get(\"title\")\n",
    "        page_name = re.search(r'.+(?=\\s-\\sWikipedia)', title).group()\n",
    "        \n",
    "        # Get similarity between found Wiki page name and channel name\n",
    "        # This is to prevent false positives in finding a Wiki page\n",
    "        similarity = round(jaro_similarity(channel_name, page_name), 2)\n",
    "        \n",
    "        # If n% similar, consider Wiki page as found\n",
    "        if similarity >= 0.80:\n",
    "            wiki = True\n",
    "            break\n",
    "    \n",
    "if wiki:\n",
    "    print(str(wiki) + \", at index [\" + str(i) + \"] with \" + str(similarity * 100) + \"% similarity.\")\n",
    "    print(\"Link found: \"+ title + \" @ \" + response.get(\"items\")[i].get(\"link\"))\n",
    "else:\n",
    "    print(\"No Wiki page found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f84b901",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a546b67",
   "metadata": {},
   "source": [
    "Finding a website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3e0ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = channel_name + query_tail[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c17499",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = google_resource.list(\n",
    "    q=query,\n",
    "    cx=cseKey\n",
    ").execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aa2439",
   "metadata": {},
   "outputs": [],
   "source": [
    "website = False\n",
    "\n",
    "for i in range(0, 10):\n",
    "    title = response.get(\"items\")[i].get(\"title\")\n",
    "    if channel_name.lower() in title.lower():\n",
    "        link = response.get(\"items\")[i].get(\"link\")\n",
    "        # RegEx to exclude YouTube, LinkedIn, and Wikipedia pages\n",
    "        pattern = r'https\\:\\/\\/(\\w{2}.wikipedia\\.org\\/wiki\\/.+|www\\.(youtube\\.com.+|linkedin\\.com.+))'\n",
    "        if re.search(pattern, link) == None:\n",
    "            # The first result among the filtered at this point is MOST LIKELY the official website\n",
    "            website = True\n",
    "            break\n",
    "\n",
    "if website:\n",
    "    print(str(website) + \", at index [\" + str(i) + \"]\")\n",
    "    print(\"Link found: \" + title + \" @ \" + link)\n",
    "else:\n",
    "    print(\"No official website found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ddd5e4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba31559a",
   "metadata": {},
   "source": [
    "Finding social media presence <br>\n",
    "Limited to these social media sites: <br>\n",
    "- Facebook\n",
    "- Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b882617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = channel_name + query_tail[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b398bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_response = google_resource.list(\n",
    "    q=query,\n",
    "    cx=cseKey\n",
    ").execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2804fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = channel_name + query_tail[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378312dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_response = google_resource.list(\n",
    "    q=query,\n",
    "    cx=cseKey\n",
    ").execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11a32d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching for a Facebook profile\n",
    "facebook = False\n",
    "\n",
    "for i in range(0, 10):\n",
    "    pattern = r'^https\\:\\/\\/www\\.facebook\\.com\\/.+\\/'\n",
    "    link = fb_response.get(\"items\")[i].get(\"formattedUrl\")\n",
    "    if re.search(pattern, link) != None:\n",
    "        title = fb_response.get(\"items\")[i].get(\"title\")\n",
    "        similarity = round(jaro_similarity(channel_name, title), 2)\n",
    "        \n",
    "        if similarity >= 0.80:\n",
    "            facebook = True\n",
    "            break\n",
    "            \n",
    "if facebook:\n",
    "    print(str(facebook) + \", at index [\" + str(i) + \"] with \" + str(similarity * 100) + \"% similarity.\")\n",
    "    print(\"Link found: \" + title + \" @ \" + link)\n",
    "else:\n",
    "    print(\"No Facebook profile found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919045e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching for a Twitter profile\n",
    "twitter = False\n",
    "\n",
    "for i in range(0, 10):\n",
    "    pattern = r'https\\:\\/\\/(twitter|x)\\.com\\/.+'\n",
    "    link = twitter_response.get(\"items\")[i].get(\"formattedUrl\")\n",
    "    if re.search(pattern, link) != None:\n",
    "        title = twitter_response.get(\"items\")[i].get(\"title\")\n",
    "        similarity = round(jaro_similarity(channel_name, title), 2)\n",
    "        \n",
    "        if similarity >= 0.80:\n",
    "            twitter = True\n",
    "            break\n",
    "            \n",
    "if twitter:\n",
    "    print(str(twitter) + \", at index [\" + str(i) + \"] with \" + str(similarity * 100) + \"% similarity.\")\n",
    "    print(\"Link found: \" + title + \" @ \" + link)re.search(r'.+(?=\\s-\\sWikipedia)', title)\n",
    "else:\n",
    "    print(\"No Twitter profile found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33553c6b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8387b4cc",
   "metadata": {},
   "source": [
    "Compiling everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1261e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def find_linkedIn(channel_name, query):\n",
    "    found = False\n",
    "    pattern = r'https:\\/\\/www\\.linkedin\\.com\\/(company|in)\\/.+'  # Used to find specific profile links\n",
    "\n",
    "    try:\n",
    "        li_response = google_resource.list(\n",
    "            q=query,\n",
    "            cx=cseKey\n",
    "        ).execute()\n",
    "\n",
    "        for i in range(0, 10):\n",
    "            link = li_response.get(\"items\")[i].get(\"formattedUrl\")\n",
    "            if re.search(pattern, link) is not None:\n",
    "                # Get profile name from search result\n",
    "                match = re.search(r'\\w+\\s(\\w+)?', li_response.get(\"items\")[i].get(\"title\"))\n",
    "                if match is not None:\n",
    "                    profile_name = match.group()\n",
    "\n",
    "                    # Get similarity between found profile name and channel name\n",
    "                    # This is to prevent false positives in finding a LinkedIn profile\n",
    "                    similarity = round(jaro_similarity(channel_name.lower(), profile_name.lower()), 2)\n",
    "\n",
    "                    # If n% similar, consider LinkedIn profile as found\n",
    "                    if similarity >= 0.80:\n",
    "                        found = True\n",
    "                        break\n",
    "    except HttpError:\n",
    "        pass\n",
    "\n",
    "    if not found:\n",
    "        return found, None\n",
    "    else:\n",
    "        return found, link\n",
    "\n",
    "\n",
    "def find_wiki(channel_name, query):\n",
    "    found = False\n",
    "    pattern = r'https:\\/\\/\\w{2}.wikipedia\\.org\\/wiki\\/.+'\n",
    "\n",
    "    try:\n",
    "        wiki_response = google_resource.list(\n",
    "            q=query,\n",
    "            cx=cseKey\n",
    "        ).execute()\n",
    "\n",
    "        for i in range(0, 10):\n",
    "            link = wiki_response.get(\"items\")[i].get(\"formattedUrl\")\n",
    "            if re.search(pattern, link) is not None:\n",
    "                # Get Wiki page name from search result\n",
    "                title = wiki_response.get(\"items\")[i].get(\"title\")\n",
    "                match = re.search(r'.+(?=\\s-\\sWikipedia)', title)\n",
    "                if match is not None:\n",
    "                    page_name = match.group()\n",
    "\n",
    "                    # Get similarity between found Wiki page name and channel name\n",
    "                    # This is to prevent false positives in finding a Wiki page\n",
    "                    similarity = round(jaro_similarity(channel_name.lower(), page_name.lower()), 2)\n",
    "\n",
    "                    # If n% similar, consider Wiki page as found\n",
    "                    if similarity >= 0.80:\n",
    "                        found = True\n",
    "                        break\n",
    "    except HttpError:\n",
    "        pass\n",
    "\n",
    "    if not found:\n",
    "        return found, None\n",
    "    else:\n",
    "        return found, link\n",
    "\n",
    "\n",
    "def find_website(channel_name, query):\n",
    "    found = False\n",
    "    # RegEx to exclude YouTube, LinkedIn, and Wikipedia pages\n",
    "    pattern = r'https\\:\\/\\/(\\w{2}.wikipedia\\.org\\/wiki\\/.+|www\\.(youtube\\.com.+|linkedin\\.com.+))'\n",
    "\n",
    "    try:\n",
    "        website_response = google_resource.list(\n",
    "            q=query,\n",
    "            cx=cseKey\n",
    "        ).execute()\n",
    "\n",
    "        for i in range(0, 10):\n",
    "            title = website_response.get(\"items\")[i].get(\"title\")\n",
    "            link = website_response.get(\"items\")[i].get(\"link\")\n",
    "            if channel_name.lower() in title.lower():\n",
    "                if re.search(pattern, link) is None:\n",
    "                    # The first result among the filtered at this point is MOST LIKELY the official website\n",
    "                    found = True\n",
    "                    break\n",
    "    except HttpError:\n",
    "        pass\n",
    "\n",
    "    if not found:\n",
    "        return found, None\n",
    "    else:\n",
    "        return found, link\n",
    "\n",
    "\n",
    "def find_fb(channel_name, query):\n",
    "    found = False\n",
    "    pattern = r'^https\\:\\/\\/www\\.facebook\\.com\\/.+\\/'\n",
    "\n",
    "    try:\n",
    "        fb_response = google_resource.list(\n",
    "            q=query,\n",
    "            cx=cseKey\n",
    "        ).execute()\n",
    "\n",
    "        for i in range(0, 10):\n",
    "            link = fb_response.get(\"items\")[i].get(\"formattedUrl\")\n",
    "            if re.search(pattern, link) is not None:\n",
    "                title = fb_response.get(\"items\")[i].get(\"title\")\n",
    "                similarity = round(jaro_similarity(channel_name.lower(), title.lower()), 2)\n",
    "\n",
    "                if similarity >= 0.80:\n",
    "                    found = True\n",
    "                    break\n",
    "    except HttpError:\n",
    "        pass\n",
    "\n",
    "    if not found:\n",
    "        return found, None\n",
    "    else:\n",
    "        return found, link\n",
    "\n",
    "\n",
    "def find_twitter(channel_name, query):\n",
    "    found = False\n",
    "    pattern = r'https\\:\\/\\/(twitter|x)\\.com\\/.+'\n",
    "\n",
    "    try:\n",
    "        twitter_response = google_resource.list(\n",
    "            q=query,\n",
    "            cx=cseKey\n",
    "        ).execute()\n",
    "\n",
    "        for i in range(0, 10):\n",
    "            link = twitter_response.get(\"items\")[i].get(\"formattedUrl\")\n",
    "            if re.search(pattern, link) is not None:\n",
    "                title = twitter_response.get(\"items\")[i].get(\"title\")\n",
    "                similarity = round(jaro_similarity(channel_name.lower(), title.lower()), 2)\n",
    "\n",
    "                if similarity >= 0.80:\n",
    "                    found = True\n",
    "                    break\n",
    "    except HttpError:\n",
    "        pass\n",
    "\n",
    "    if not found:\n",
    "        return found, None\n",
    "    else:\n",
    "        return found, link\n",
    "\n",
    "\n",
    "def check_desc(channel_name, videos_df, pattern):\n",
    "    # Get first 5 videos of channel from videos_df\n",
    "    videos_df = videos_df.loc[videos_df[\"channel_name\"] == channel_name].reset_index().drop(\"index\", axis=1).head()\n",
    "    found = (False, None)\n",
    "\n",
    "    # For each video\n",
    "    for i in range(0, videos_df.shape[0]):\n",
    "        # Get description\n",
    "        desc = repr(videos_df.iloc[i][\"description\"]).replace(\"\\\\n\", \" \").replace(\"  \", \" \")\n",
    "\n",
    "        # Using RegEx, find links using given pattern\n",
    "        match = re.search(pattern, desc)\n",
    "        if match is not None:\n",
    "            found = (True, match.group())\n",
    "            break\n",
    "\n",
    "    return found\n",
    "\n",
    "\n",
    "def find_sources(channel_names, channel_IDs, main_df):\n",
    "    pbar = tqdm(total=len(channel_names))\n",
    "    pbar.set_description(\"Finding sources...\")\n",
    "\n",
    "    source_scores = []\n",
    "    ss_cols = [\n",
    "        \"channel_id\", \"channel_name\",\n",
    "        \"profiles\", \"website\", \"social_media_presence\",\n",
    "        \"vs\"\n",
    "    ]\n",
    "\n",
    "    source_links = []\n",
    "    sl_cols = [\n",
    "        \"channel_id\", \"channel_name\",\n",
    "        \"LinkedIn\", \"Wiki\", \"Website\",\n",
    "        \"Twitter\", \"Facebook\"\n",
    "    ]\n",
    "\n",
    "    query_tail = [\n",
    "        \" LinkedIn\",\n",
    "        \" Wiki\",\n",
    "        \" Official Website\",\n",
    "        \" Facebook\",\n",
    "        \" Twitter\"\n",
    "    ]\n",
    "\n",
    "    # --- Patterns to search for links within video descriptions\n",
    "    linkedIn_pattern = r\"(?<=(Linked(in|In)\\:\\s))https:\\/\\/(www\\.)?linkedin\\.com\\/(company|in)\\/(\\w|\\w[-_])+\\/\"\n",
    "    website_pattern = r\"(?<=(W|w)ebsite((\\:)?\\s|\\sat\\s))https:\\/\\/\\w+(\\.(\\w|\\w[-_])+)?\\.\\w{3}(\\.\\w{2})?(\\/(\\w|\\w[-_])+)?\"\n",
    "    fb_pattern = r\"(?<=((F|f)acebook\\:\\s))https:\\/\\/(www\\.)?facebook\\.com\\/(\\w|\\w[-_])+\"\n",
    "    twitter_pattern = r\"(?<=((T|t)witter\\:\\s))https:\\/\\/(www\\.)?(twitter|x)\\.com\\/(\\w|\\w[-_])+\"\n",
    "    # ---\n",
    "\n",
    "    for channel_name in channel_names:\n",
    "        # --- Checking descriptions from channel's videos\n",
    "        linkedIn_found = check_desc(channel_name, main_df, linkedIn_pattern)\n",
    "        site_found = check_desc(channel_name, main_df, website_pattern)\n",
    "        fb_found = check_desc(channel_name, main_df, fb_pattern)\n",
    "        twitter_found = check_desc(channel_name, main_df, twitter_pattern)\n",
    "        # ---\n",
    "\n",
    "        # --- If link not found in descriptions, search via Google\n",
    "        if not linkedIn_found[0]:\n",
    "            linkedIn_found = find_linkedIn(channel_name, channel_name + query_tail[0])\n",
    "\n",
    "        if not site_found[0]:\n",
    "            site_found = find_website(channel_name, channel_name + query_tail[2])\n",
    "\n",
    "        if not fb_found[0]:\n",
    "            fb_found = find_fb(channel_name, channel_name + query_tail[3])\n",
    "\n",
    "        if not twitter_found[0]:\n",
    "            twitter_found = find_twitter(channel_name, channel_name + query_tail[4])\n",
    "\n",
    "        wiki_found = find_wiki(channel_name, channel_name + query_tail[1])\n",
    "        # ---\n",
    "\n",
    "        profiles = 0\n",
    "        website = 0\n",
    "        social_media_presence = 0\n",
    "\n",
    "        if linkedIn_found[0] and wiki_found[0]:\n",
    "            profiles = 3\n",
    "        elif linkedIn_found[0] and not wiki_found[0]:\n",
    "            profiles = 2\n",
    "        elif not linkedIn_found[0] and wiki_found[0]:\n",
    "            profiles = 1\n",
    "\n",
    "        if site_found[0]:\n",
    "            website = 2\n",
    "\n",
    "        if fb_found[0] or twitter_found[0]:\n",
    "            social_media_presence = 1\n",
    "\n",
    "        # Source scores ---\n",
    "        ss_record = [\n",
    "            channel_IDs.get(channel_name),  # channel_id\n",
    "            channel_name,  # channel_name\n",
    "            profiles,  # profiles\n",
    "            website,  # website\n",
    "            social_media_presence,  # social_media_presence\n",
    "            np.nan  # vs\n",
    "        ]\n",
    "        source_scores.append(ss_record)\n",
    "        # ---\n",
    "\n",
    "        # Source links ---\n",
    "        fb_link = None\n",
    "        twitter_link = None\n",
    "\n",
    "        if fb_found[0]:\n",
    "            fb_link = fb_found[1]\n",
    "\n",
    "        if twitter_found[0]:\n",
    "            twitter_link = twitter_found[1]\n",
    "\n",
    "        sl_record = [\n",
    "            channel_IDs.get(channel_name),\n",
    "            channel_name,\n",
    "            linkedIn_found[1],\n",
    "            wiki_found[1],\n",
    "            site_found[1],\n",
    "            twitter_link,\n",
    "            fb_link\n",
    "        ]\n",
    "        source_links.append(sl_record)\n",
    "        # ---\n",
    "\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    ss_nparray = np.array(source_scores)\n",
    "    sl_nparray = np.array(source_links)\n",
    "\n",
    "    ss_df = pd.DataFrame(ss_nparray, columns=ss_cols)\n",
    "    sl_df = pd.DataFrame(sl_nparray, columns=sl_cols)\n",
    "\n",
    "    ss_df.to_csv(\"source_scores.csv\")\n",
    "    sl_df.to_csv(\"source_links.csv\")\n",
    "\n",
    "    print(\"Complete.\")\n",
    "\n",
    "\n",
    "def topsis(scores, weights):\n",
    "    wndm = {}\n",
    "\n",
    "    for column in weights.keys():\n",
    "        temp_list = []\n",
    "        x = 0\n",
    "        for i in range(0, scores.shape[0]):\n",
    "            num = scores.iloc[i][column] ** 2\n",
    "            x += num\n",
    "        denominator = sqrt(x)\n",
    "        print(f\"{column}: {denominator}\")\n",
    "\n",
    "        # Normalize scores\n",
    "        for i in range(0, scores.shape[0]):\n",
    "            norm_score = scores.iloc[i][column] / denominator\n",
    "            temp_list.append(norm_score)\n",
    "\n",
    "        # Apply weight\n",
    "        for i in range(0, len(temp_list)):\n",
    "            temp_list[i] *= weights.get(column)\n",
    "\n",
    "        wndm.update({column: temp_list})\n",
    "\n",
    "    wndm_df = pd.DataFrame.from_dict(wndm)\n",
    "    ideal_best = wndm_df.max()\n",
    "    ideal_worst = wndm_df.min()\n",
    "\n",
    "    dist_from_best = []\n",
    "    dist_from_worst = []\n",
    "\n",
    "    # Euclidean distance from ideal best\n",
    "    for i in range(0, wndm_df.shape[0]):\n",
    "        temp_num = 0\n",
    "        for column in wndm_df.columns:\n",
    "            temp_num += (wndm_df.iloc[i][column] - ideal_best[column]) ** 2\n",
    "        dist_from_best.append(sqrt(temp_num))\n",
    "\n",
    "    # Euclidean distance from ideal worst\n",
    "    for i in range(0, wndm_df.shape[0]):\n",
    "        temp_num = 0\n",
    "        for column in wndm_df.columns:\n",
    "            temp_num += (wndm_df.iloc[i][column] - ideal_worst[column]) ** 2\n",
    "        dist_from_worst.append(sqrt(temp_num))\n",
    "\n",
    "    performance_rank = []\n",
    "    for i in range(0, wndm_df.shape[0]):\n",
    "        performance_rank.append(dist_from_worst[i] / (dist_from_best[i] + dist_from_worst[i]))\n",
    "\n",
    "    performance_rank = pd.Series(np.array(performance_rank))\n",
    "\n",
    "    return performance_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41c9ce1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df586c8",
   "metadata": {},
   "source": [
    "Testing find_sources (WARNING: Be mindful of daily quota for Custom Search API queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ec93b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = df.drop(\"Unnamed: 0\", axis=1)\n",
    "channel_names = main_df[\"channel_name\"].unique()\n",
    "channel_IDs = main_df[[\"channel_id\", \"channel_name\"]].groupby(\"channel_name\").first().to_dict().get(\"channel_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c39c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_sources(channel_names, channel_IDs, main_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9a68e0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973dbea1",
   "metadata": {},
   "source": [
    "Testing TOPSIS Algorithm - Getting video rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da59f308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001ecde7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ss_df = pd.read_csv(\"datasets/covid_philippines/source_scores.csv\")\n",
    "ss_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "ss_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8ddda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_df[[\"profiles\", \"website\", \"social_media_presence\", \"vs\"]].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9812e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = df.drop(\"Unnamed: 0\", axis=1)\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93484f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_dict = {}\n",
    "temp_dict = ss_df[[\"channel_id\", \"profiles\", \"website\", \"social_media_presence\"]].to_dict()\n",
    "for i in range(0, ss_df.shape[0]):\n",
    "    ss_dict[temp_dict.get(\"channel_id\").get(i)] = {\n",
    "        \"profiles\": temp_dict.get(\"profiles\").get(i),\n",
    "        \"website\": temp_dict.get(\"website\").get(i),\n",
    "        \"social_media_presence\": temp_dict.get(\"social_media_presence\").get(i)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab4084d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_dict = main_df.to_dict()\n",
    "main_dict[\"profiles\"] = {}\n",
    "main_dict[\"website\"] = {}\n",
    "main_dict[\"social_media_presence\"] = {}\n",
    "\n",
    "for i in range(0, main_df.shape[0]):\n",
    "    channel_id = main_dict.get(\"channel_id\").get(i)\n",
    "    if channel_id in ss_dict:\n",
    "        main_dict[\"profiles\"].update({i: ss_dict.get(channel_id).get(\"profiles\")})\n",
    "        main_dict[\"website\"].update({i: ss_dict.get(channel_id).get(\"website\")})\n",
    "        main_dict[\"social_media_presence\"].update({i: ss_dict.get(channel_id).get(\"social_media_presence\")})\n",
    "    else:\n",
    "        main_dict[\"profiles\"].update({i: 0.0})\n",
    "        main_dict[\"website\"].update({i: 0.0})\n",
    "        main_dict[\"social_media_presence\"].update({i: 0.0})\n",
    "\n",
    "main_df = pd.DataFrame.from_dict(main_dict)\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc914f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bff01fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    \"profiles\": 0.40,\n",
    "    \"website\": 0.25,\n",
    "    \"social_media_presence\": 0.10,\n",
    "    \"view_count\": 0.05,\n",
    "    \"like_count\": 0.05,\n",
    "    \"comment_count\": 0.05,\n",
    "    \"sub_count\": 0.05,\n",
    "    \"total_videos\": 0.05,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b655077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df[\"rank\"] = topsis(main_df, weights)\n",
    "main_df.sort_values(by=\"rank\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcbf739",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5884f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    \"profiles\": 0.40,\n",
    "    \"website\": 0.25,\n",
    "    \"social_media_presence\": 0.10,\n",
    "    \"view_count\": 0.05,\n",
    "    \"like_count\": 0.05,\n",
    "    \"comment_count\": 0.05,\n",
    "    \"sub_count\": 0.05,\n",
    "    \"total_videos\": 0.05,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23db458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = main_df[[\n",
    "    \"video_id\", \"channel_name\", \"profiles\", \"website\",\n",
    "    \"social_media_presence\", \"view_count\", \"like_count\",\n",
    "    \"comment_count\", \"sub_count\", \"total_videos\"]].head()\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe4ad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"rank\"] = topsis(test_df, weights)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335a3d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    \"profiles\": 0.50,\n",
    "    \"website\": 0.35,\n",
    "    \"social_media_presence\": 0.15\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5598b4ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df = ss_df[[\"channel_name\", \"profiles\", \"website\", \"social_media_presence\"]].head()\n",
    "test_df[\"vs\"] = topsis(test_df, weights)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa36d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_df = pd.read_csv(\"datasets/covid_philippines/source_links.csv\")\n",
    "sl_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "sl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c0685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = round(jaro_similarity(\"ANC 24/7\", \"ABS-CBN News Channel\"), 2)\n",
    "similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519f0bb3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c961a8",
   "metadata": {},
   "source": [
    "Checking video descriptions to look for profile links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21a3f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = df.drop(\"Unnamed: 0\", axis=1)\n",
    "channel_names = main_df[\"channel_name\"].unique()\n",
    "channel_IDs = main_df[[\"channel_id\", \"channel_name\"]].groupby(\"channel_name\").first().to_dict().get(\"channel_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62130e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedIn_pattern = r\"(?<=(Linked(in|In)\\:\\s))https:\\/\\/(www\\.)?linkedin\\.com\\/(company|in)\\/(\\w|\\w[-_])+\\/\"\n",
    "website_pattern = r\"(?<=(W|w)ebsite((\\:)?\\s|\\sat\\s))https:\\/\\/\\w+(\\.(\\w|\\w[-_])+)?\\.\\w{3}(\\.\\w{2})?(\\/(\\w|\\w[-_])+)?\"\n",
    "fb_pattern = r\"(?<=((F|f)acebook\\:\\s))https:\\/\\/(www\\.)?facebook\\.com\\/(\\w|\\w[-_])+\"\n",
    "twitter_pattern = r\"(?<=((T|t)witter\\:\\s))https:\\/\\/(www\\.)?(twitter|x)\\.com\\/(\\w|\\w[-_])+\"\n",
    "\n",
    "for channel_name in channel_names:\n",
    "    print(f\"[{channel_name}]\")\n",
    "    \n",
    "    linkedIn_found = check_desc(channel_name, main_df, linkedIn_pattern)\n",
    "    website_found = check_desc(channel_name, main_df, website_pattern)\n",
    "    fb_found = check_desc(channel_name, main_df, fb_pattern)\n",
    "    twitter_found = check_desc(channel_name, main_df, twitter_pattern)\n",
    "    \n",
    "    print(f\"Website: {str(website_found[0])} @ {website_found[1]}\")\n",
    "    print(f\"LinkedIn: {str(linkedIn_found[0])} @ {linkedIn_found[1]}\")\n",
    "    print(f\"Facebook: {str(fb_found[0])} @ {fb_found[1]}\")\n",
    "    print(f\"Twitter: {str(twitter_found[0])} @ {twitter_found[1]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa24488",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1ebb6da-2d1b-424f-b8fa-41d624e4f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf4878f3-280c-46aa-82e2-f9e1c46f71d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_about_links(pattern, links):\n",
    "    found = (False, None)\n",
    "    \n",
    "    for i in range(0, len(links)):\n",
    "        match = re.search(pattern, links[i][1])\n",
    "        if match is not None:\n",
    "            found = (True, match.group())\n",
    "            links.pop(i)\n",
    "            break\n",
    "    \n",
    "    return found, links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2933bc54-0850-499f-a30c-419948c42ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Patterns to search for links within About sections in channel pages\n",
    "about_website_pattern = r\"(W|w)ebsite\"\n",
    "about_fb_pattern = r\"facebook\\.com\\/.+\"\n",
    "about_linkedIn_pattern = r\"linkedin\\.com\\/(company|in)\\/.+\"\n",
    "about_twitter_pattern = r\"twitter\\.com\\/.+\"\n",
    "# ---\n",
    "\n",
    "for i in range(0, unique_channels.shape[0]):\n",
    "    channel_id = channel_df.iloc[i][\"channel_id\"]\n",
    "    channel_name = channel_df.iloc[i][\"channel_name\"]\n",
    "    \n",
    "    about_page = requests.get(f'https://www.youtube.com/channel/{channel_id}/about')\n",
    "    soup = BeautifulSoup(about_page.content, 'html.parser')\n",
    "    script_tags = soup.find_all(\"script\")\n",
    "\n",
    "    for script in script_tags:\n",
    "        results = re.search(r\"var ytInitialData = {.*}\", script.text)\n",
    "        if results is not None:\n",
    "            object = results.group(0).replace(\"var ytInitialData = \", \"\")\n",
    "            try:\n",
    "                link_information =  (json.loads(object) ['onResponseReceivedEndpoints'][0]\n",
    "                                    ['showEngagementPanelEndpoint']\n",
    "                                    ['engagementPanel']\n",
    "                                    ['engagementPanelSectionListRenderer']\n",
    "                                    ['content']\n",
    "                                    ['sectionListRenderer']\n",
    "                                    ['contents'][0]\n",
    "                                    ['itemSectionRenderer']\n",
    "                                    ['contents'][0]\n",
    "                                    ['aboutChannelRenderer']\n",
    "                                    ['metadata']\n",
    "                                    ['aboutChannelViewModel']\n",
    "                                    ['links']\n",
    "                                    )\n",
    "            except:\n",
    "                print(\"No links provided for \", {channel_name})\n",
    "            else:\n",
    "                # print(\"Available links provided for:\", {channel_name})\n",
    "                links = []\n",
    "                for link in link_information:   #print all available links from the about modal\n",
    "                    link_title = link['channelExternalLinkViewModel']['title']['content']\n",
    "                    url = link['channelExternalLinkViewModel']['link']['content']\n",
    "                    links.append([link_title, url])\n",
    "                \n",
    "                print(links)\n",
    "                \n",
    "                fb_found, links = check_about_links(about_fb_pattern, links)\n",
    "                twitter_found, links = check_about_links(about_twitter_pattern, links)\n",
    "                linkedIn_found, links = check_about_links(about_linkedIn_pattern, links)\n",
    "                \n",
    "                site_found = (False, None)\n",
    "                \n",
    "                for i in range(0, len(links)):\n",
    "                    match = re.search(about_website_pattern, links[i][0])\n",
    "                    if match is not None:\n",
    "                        site_found = (True, links[i][1])\n",
    "                        break\n",
    "                \n",
    "                for i in range(0, len(links)):\n",
    "                    link_title = links[i][0]\n",
    "                    similarity = round(jaro_similarity(channel_name, link_title), 2)\n",
    "                    if similarity >= 0.60:\n",
    "                        match = re.search(\"youtube\\.com\\/.+\", links[i][1])\n",
    "                        if match is None:\n",
    "                            site_found = (True, links[i][1])\n",
    "                            break\n",
    "                \n",
    "                print(fb_found)\n",
    "                print(twitter_found)\n",
    "                print(linkedIn_found)\n",
    "                print(site_found)\n",
    "                \n",
    "            print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eb01c3-1076-45ce-baae-f876094c3f8c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15175e5f-ab96-4db7-a93a-54c27c816529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_id</th>\n",
       "      <th>channel_name</th>\n",
       "      <th>LinkedIn</th>\n",
       "      <th>Wiki</th>\n",
       "      <th>Website</th>\n",
       "      <th>Twitter</th>\n",
       "      <th>Facebook</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UC4SUWizzKc1tptprBkWjX2Q</td>\n",
       "      <td>South China Morning Post</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UCvi6hEzLM-Z_unKPSuuzKvg</td>\n",
       "      <td>ANC 24/7</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UCdnZdQxYXnbN4uWJg96oGxw</td>\n",
       "      <td>Rappler</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UCNye-wNBqNL5ZzHSJj3l8Bg</td>\n",
       "      <td>Al Jazeera English</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UCvRAX-ujvZ0eTMLGG2vki9w</td>\n",
       "      <td>INQUIRER.net</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 channel_id              channel_name  LinkedIn   Wiki  \\\n",
       "0  UC4SUWizzKc1tptprBkWjX2Q  South China Morning Post      True   True   \n",
       "1  UCvi6hEzLM-Z_unKPSuuzKvg                  ANC 24/7     False  False   \n",
       "2  UCdnZdQxYXnbN4uWJg96oGxw                   Rappler      True   True   \n",
       "3  UCNye-wNBqNL5ZzHSJj3l8Bg        Al Jazeera English      True   True   \n",
       "4  UCvRAX-ujvZ0eTMLGG2vki9w              INQUIRER.net     False   True   \n",
       "\n",
       "   Website  Twitter  Facebook  \n",
       "0     True     True      True  \n",
       "1     True     True      True  \n",
       "2     True    False      True  \n",
       "3     True     True      True  \n",
       "4     True     True      True  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"datasets/\" + filename + \"/source_check.csv\"\n",
    "sc_df = pd.read_csv(path).drop(\"Unnamed: 0\", axis=1)\n",
    "sc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e690c386-71b2-4c61-b827-5b4771c079ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_id</th>\n",
       "      <th>channel_name</th>\n",
       "      <th>LinkedIn</th>\n",
       "      <th>Wiki</th>\n",
       "      <th>Website</th>\n",
       "      <th>Twitter</th>\n",
       "      <th>Facebook</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UC4SUWizzKc1tptprBkWjX2Q</td>\n",
       "      <td>South China Morning Post</td>\n",
       "      <td>https://www.linkedin.com/company/south-china-m...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/South_China_Morn...</td>\n",
       "      <td>https://scmp.com</td>\n",
       "      <td>https://twitter.com/scmpnews</td>\n",
       "      <td>https://facebook.com/scmp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UCvi6hEzLM-Z_unKPSuuzKvg</td>\n",
       "      <td>ANC 24/7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://news.abs-cbn.com/anc</td>\n",
       "      <td>https://twitter.com/ancalerts</td>\n",
       "      <td>https://www.facebook.com/ancalerts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UCdnZdQxYXnbN4uWJg96oGxw</td>\n",
       "      <td>Rappler</td>\n",
       "      <td>https://www.linkedin.com/company/rappler</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Rappler</td>\n",
       "      <td>https://www.rappler.com/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.facebook.com/rapplerdotcom/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UCNye-wNBqNL5ZzHSJj3l8Bg</td>\n",
       "      <td>Al Jazeera English</td>\n",
       "      <td>https://www.linkedin.com/company/aljazeera</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Al_Jazeera_English</td>\n",
       "      <td>https://www.aljazeera.com</td>\n",
       "      <td>https://twitter.com/AJEnglish</td>\n",
       "      <td>https://www.facebook.com/aljazeera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UCvRAX-ujvZ0eTMLGG2vki9w</td>\n",
       "      <td>INQUIRER.net</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Inquirer</td>\n",
       "      <td>https://www.inquirer.net/</td>\n",
       "      <td>https://twitter.com/inquirerdotnet</td>\n",
       "      <td>https://facebook.com/inquirerdotnet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 channel_id              channel_name  \\\n",
       "0  UC4SUWizzKc1tptprBkWjX2Q  South China Morning Post   \n",
       "1  UCvi6hEzLM-Z_unKPSuuzKvg                  ANC 24/7   \n",
       "2  UCdnZdQxYXnbN4uWJg96oGxw                   Rappler   \n",
       "3  UCNye-wNBqNL5ZzHSJj3l8Bg        Al Jazeera English   \n",
       "4  UCvRAX-ujvZ0eTMLGG2vki9w              INQUIRER.net   \n",
       "\n",
       "                                            LinkedIn  \\\n",
       "0  https://www.linkedin.com/company/south-china-m...   \n",
       "1                                                NaN   \n",
       "2           https://www.linkedin.com/company/rappler   \n",
       "3         https://www.linkedin.com/company/aljazeera   \n",
       "4                                                NaN   \n",
       "\n",
       "                                                Wiki  \\\n",
       "0  https://en.wikipedia.org/wiki/South_China_Morn...   \n",
       "1                                                NaN   \n",
       "2              https://en.wikipedia.org/wiki/Rappler   \n",
       "3   https://en.wikipedia.org/wiki/Al_Jazeera_English   \n",
       "4             https://en.wikipedia.org/wiki/Inquirer   \n",
       "\n",
       "                        Website                             Twitter  \\\n",
       "0              https://scmp.com        https://twitter.com/scmpnews   \n",
       "1  https://news.abs-cbn.com/anc       https://twitter.com/ancalerts   \n",
       "2      https://www.rappler.com/                                 NaN   \n",
       "3     https://www.aljazeera.com       https://twitter.com/AJEnglish   \n",
       "4     https://www.inquirer.net/  https://twitter.com/inquirerdotnet   \n",
       "\n",
       "                                  Facebook  \n",
       "0                https://facebook.com/scmp  \n",
       "1       https://www.facebook.com/ancalerts  \n",
       "2  https://www.facebook.com/rapplerdotcom/  \n",
       "3       https://www.facebook.com/aljazeera  \n",
       "4      https://facebook.com/inquirerdotnet  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"datasets/\" + filename + \"/source_links.csv\"\n",
    "sl_df = pd.read_csv(path).drop(\"Unnamed: 0\", axis=1)\n",
    "sl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2406cec-84fa-4990-8582-7d14ab358e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_id</th>\n",
       "      <th>channel_name</th>\n",
       "      <th>LinkedIn</th>\n",
       "      <th>Wiki</th>\n",
       "      <th>Website</th>\n",
       "      <th>Twitter</th>\n",
       "      <th>Facebook</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>UC4p_I9eiRewn2KoU-nawrDg</td>\n",
       "      <td>The Straits Times</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  channel_id       channel_name  LinkedIn   Wiki  Website  \\\n",
       "26  UC4p_I9eiRewn2KoU-nawrDg  The Straits Times     False  False     True   \n",
       "\n",
       "    Twitter  Facebook  \n",
       "26     True     False  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_df.loc[sc_df[\"channel_name\"] == \"The Straits Times\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa411bf3-90fa-487e-bb5d-aa2dedd25b06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22f49ea-8785-4006-80f9-144fc9f2c5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e488ff9f-7a9a-4b5e-9f1b-70a9d22fb746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6c1641-a386-4ac9-95d6-a14176b6e46f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d3a3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
