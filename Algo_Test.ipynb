{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "760902a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from googleapiclient.discovery import build\n",
    "from pprint import pprint\n",
    "import regex as re\n",
    "from jellyfish import jaro_similarity\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09e489b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"covid_philippines\"\n",
    "\n",
    "path = \"datasets/\" + filename + \"/\" + filename + \".csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "video_channel = df[[\"video_id\", \"video_title\", \"channel_id\", \"channel_title\"]]\n",
    "unique_channels = df[\"channel_title\"].unique()\n",
    "view_like_comment = np.array(df[[\"view_count\", \"like_count\", \"comment_count\"]])\n",
    "query_tail = [\n",
    "    \" LinkedIn\",\n",
    "    \" Wiki\",\n",
    "    \" Official Website\",\n",
    "    \" Facebook\",\n",
    "    \" Twitter\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fed69ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CNN Philippines', 'ANC 24/7', 'South China Morning Post',\n",
       "       'Rappler', 'INQUIRER.net', 'CNA Insider', 'Al Jazeera English',\n",
       "       'CNA', 'ABS-CBN News', 'Manila Bulletin Online',\n",
       "       'UNTV News and Rescue', 'BBC News', 'Global News', 'WION',\n",
       "       'The Telegraph', 'UNICEF USA', 'Reuters', 'DW News',\n",
       "       'UNICEF Philippines', 'GMA Integrated News', 'FRANCE 24 English',\n",
       "       'Voice of America', 'Bloomberg Quicktake', 'World Bank',\n",
       "       'The Straits Times', 'The Star', 'Behind Philippines', 'FEATR',\n",
       "       'Hindustan Times', 'Gulf News', 'Diseases Simplified', 'TVUP',\n",
       "       'Bongbong Marcos', 'Esco Lifesciences Group', 'Doctor Wessam Atif',\n",
       "       'Doc Fate Cunanan', 'Asian Development Bank',\n",
       "       'Adventures in America', 'Philstar News', 'HeyoLeah',\n",
       "       'MedCram - Medical Lectures Explained CLEARLY', 'FinnSnow',\n",
       "       'Cold Chain Innovation Hub Philippines'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6204ced7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>description</th>\n",
       "      <th>video_dop</th>\n",
       "      <th>view_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>channel_dop</th>\n",
       "      <th>sub_count</th>\n",
       "      <th>total_videos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>m3P-bmt3Uqw</td>\n",
       "      <td>JN.1 COVID-19 subvariant causing spike in cases</td>\n",
       "      <td>'An infectious disease expert says the new COV...</td>\n",
       "      <td>2023-12-25</td>\n",
       "      <td>6810</td>\n",
       "      <td>46</td>\n",
       "      <td>19</td>\n",
       "      <td>UCj6spMO3ybZPobE0T5perHA</td>\n",
       "      <td>CNN Philippines</td>\n",
       "      <td>2015-03-16</td>\n",
       "      <td>1400000</td>\n",
       "      <td>22903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>sYI97jv-pZg</td>\n",
       "      <td>PH records 2,725 new COVID cases from Dec. 12 ...</td>\n",
       "      <td>'The World Tonight: The daily average of the P...</td>\n",
       "      <td>2023-12-18</td>\n",
       "      <td>2132</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>UCvi6hEzLM-Z_unKPSuuzKvg</td>\n",
       "      <td>ANC 24/7</td>\n",
       "      <td>2010-01-29</td>\n",
       "      <td>1520000</td>\n",
       "      <td>71172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>Normally crowded streets of Philippine capital...</td>\n",
       "      <td>'Subscribe to our YouTube channel for free her...</td>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>323944</td>\n",
       "      <td>3285</td>\n",
       "      <td>619</td>\n",
       "      <td>UC4SUWizzKc1tptprBkWjX2Q</td>\n",
       "      <td>South China Morning Post</td>\n",
       "      <td>2007-01-18</td>\n",
       "      <td>3810000</td>\n",
       "      <td>16958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3YFpjgIQqEo</td>\n",
       "      <td>WATCH: DOH Usec. Tayag on rise of COVID-19 cas...</td>\n",
       "      <td>'Dateline Philippines: Karmina Constantino tal...</td>\n",
       "      <td>2023-12-07</td>\n",
       "      <td>9464</td>\n",
       "      <td>80</td>\n",
       "      <td>17</td>\n",
       "      <td>UCvi6hEzLM-Z_unKPSuuzKvg</td>\n",
       "      <td>ANC 24/7</td>\n",
       "      <td>2010-01-29</td>\n",
       "      <td>1520000</td>\n",
       "      <td>71172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>n-j5rK1XOUc</td>\n",
       "      <td>W.H.O.: COVID-19 remains as health threat | Ne...</td>\n",
       "      <td>'Government agencies are set to convene after ...</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>15037</td>\n",
       "      <td>90</td>\n",
       "      <td>25</td>\n",
       "      <td>UCj6spMO3ybZPobE0T5perHA</td>\n",
       "      <td>CNN Philippines</td>\n",
       "      <td>2015-03-16</td>\n",
       "      <td>1400000</td>\n",
       "      <td>22903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     video_id                                        video_title  \\\n",
       "0           0  m3P-bmt3Uqw    JN.1 COVID-19 subvariant causing spike in cases   \n",
       "1           1  sYI97jv-pZg  PH records 2,725 new COVID cases from Dec. 12 ...   \n",
       "2           2  aLZ85hb4wjE  Normally crowded streets of Philippine capital...   \n",
       "3           3  3YFpjgIQqEo  WATCH: DOH Usec. Tayag on rise of COVID-19 cas...   \n",
       "4           4  n-j5rK1XOUc  W.H.O.: COVID-19 remains as health threat | Ne...   \n",
       "\n",
       "                                         description   video_dop  view_count  \\\n",
       "0  'An infectious disease expert says the new COV...  2023-12-25        6810   \n",
       "1  'The World Tonight: The daily average of the P...  2023-12-18        2132   \n",
       "2  'Subscribe to our YouTube channel for free her...  2020-04-02      323944   \n",
       "3  'Dateline Philippines: Karmina Constantino tal...  2023-12-07        9464   \n",
       "4  'Government agencies are set to convene after ...  2023-05-08       15037   \n",
       "\n",
       "   like_count  comment_count                channel_id  \\\n",
       "0          46             19  UCj6spMO3ybZPobE0T5perHA   \n",
       "1          14              3  UCvi6hEzLM-Z_unKPSuuzKvg   \n",
       "2        3285            619  UC4SUWizzKc1tptprBkWjX2Q   \n",
       "3          80             17  UCvi6hEzLM-Z_unKPSuuzKvg   \n",
       "4          90             25  UCj6spMO3ybZPobE0T5perHA   \n",
       "\n",
       "              channel_title channel_dop  sub_count  total_videos  \n",
       "0           CNN Philippines  2015-03-16    1400000         22903  \n",
       "1                  ANC 24/7  2010-01-29    1520000         71172  \n",
       "2  South China Morning Post  2007-01-18    3810000         16958  \n",
       "3                  ANC 24/7  2010-01-29    1520000         71172  \n",
       "4           CNN Philippines  2015-03-16    1400000         22903  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "128c76ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of queries: 215\n"
     ]
    }
   ],
   "source": [
    "num_query = 0\n",
    "\n",
    "# Per channel name\n",
    "for channel_name in unique_channels:\n",
    "    \n",
    "    # Per query type\n",
    "    for j in range(0, len(query_tail)):\n",
    "        num_query += 1\n",
    "\n",
    "print(\"Total number of queries: \" + str(num_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20aa3c4",
   "metadata": {},
   "source": [
    "## NOTE\n",
    "Verifiability score is computed <u>PER CHANNEL</u> <br>\n",
    "<br>\n",
    "Ranking is computed <u>PER VIDEO</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e41132b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9619d57b",
   "metadata": {},
   "source": [
    "Google resource initialization\n",
    "- Query and Channel name are manually declared as to simulate the search process for a single channel name\n",
    "- The actual loop for searching and verifying across a dataset of videos will be done in the .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ae2d17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your personal API key here\n",
    "apiKey = 'AIzaSyCIplXpNgYZ2IS44ZYyEi-hXRu1gzl9I58'\n",
    "\n",
    "# Search engine ID\n",
    "cseKey = \"23c1c70a203ac4852\"\n",
    "\n",
    "google_resource = build(\"customsearch\", \"v1\", developerKey=apiKey).cse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e4c3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_name = \"CNN Philippines\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7954c2c2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0d052c",
   "metadata": {},
   "source": [
    "Finding a LinkedIn Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aacce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = channel_name + query_tail[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bfe0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = google_resource.list(\n",
    "    q=query,\n",
    "    cx=cseKey\n",
    ").execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f395f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'https:\\/\\/www\\.linkedin\\.com\\/(company|in)\\/.+' # Used to find specific profile links\n",
    "\n",
    "linkedIn = False\n",
    "for i in range(0,10):\n",
    "    if re.search(pattern, response.get(\"items\")[i].get(\"formattedUrl\")) != None:\n",
    "        # Get profile name from search result\n",
    "        profile_name = re.search(r'\\w+\\s(\\w+)?', response.get(\"items\")[i].get(\"htmlTitle\")).group()\n",
    "        \n",
    "        # Get similarity between found profile name and channel name\n",
    "        # This is to prevent false positives in finding a LinkedIn profile\n",
    "        similarity = round(jaro_similarity(channel_name, profile_name), 2)\n",
    "        \n",
    "        # If n% similar, consider LinkedIn profile as found\n",
    "        if similarity >= 0.80:\n",
    "            linkedIn = True\n",
    "            break\n",
    "    \n",
    "if linkedIn:\n",
    "    print(str(linkedIn) + \", at index [\" + str(i) + \"] with \" + str(similarity * 100) + \"% similarity.\")\n",
    "    print(\"Link found: \" + profile_name + \" @ \" + response.get(\"items\")[i].get(\"link\"))\n",
    "else:\n",
    "    print(\"No LinkedIn profile found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07275d58",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8b7325",
   "metadata": {},
   "source": [
    "Finding a Wiki page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729a1cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = channel_name + query_tail[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb5a479",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = google_resource.list(\n",
    "    q=query,\n",
    "    cx=cseKey\n",
    ").execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e43cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'https:\\/\\/\\w{2}.wikipedia\\.org\\/wiki\\/.+'\n",
    "\n",
    "wiki = False\n",
    "for i in range(0,10):\n",
    "    if re.search(pattern, response.get(\"items\")[i].get(\"formattedUrl\")) != None:\n",
    "        # Get Wiki page name from search result\n",
    "        title = response.get(\"items\")[i].get(\"title\")\n",
    "        page_name = re.search(r'.+(?=\\s-\\sWikipedia)', title).group()\n",
    "        \n",
    "        # Get similarity between found Wiki page name and channel name\n",
    "        # This is to prevent false positives in finding a Wiki page\n",
    "        similarity = round(jaro_similarity(channel_name, page_name), 2)\n",
    "        \n",
    "        # If n% similar, consider Wiki page as found\n",
    "        if similarity >= 0.80:\n",
    "            wiki = True\n",
    "            break\n",
    "    \n",
    "if wiki:\n",
    "    print(str(wiki) + \", at index [\" + str(i) + \"] with \" + str(similarity * 100) + \"% similarity.\")\n",
    "    print(\"Link found: \"+ title + \" @ \" + response.get(\"items\")[i].get(\"link\"))\n",
    "else:\n",
    "    print(\"No Wiki page found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f84b901",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a546b67",
   "metadata": {},
   "source": [
    "Finding a website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3e0ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = channel_name + query_tail[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c17499",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = google_resource.list(\n",
    "    q=query,\n",
    "    cx=cseKey\n",
    ").execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aa2439",
   "metadata": {},
   "outputs": [],
   "source": [
    "website = False\n",
    "\n",
    "for i in range(0, 10):\n",
    "    title = response.get(\"items\")[i].get(\"title\")\n",
    "    if channel_name.lower() in title.lower():\n",
    "        link = response.get(\"items\")[i].get(\"link\")\n",
    "        # RegEx to exclude YouTube, LinkedIn, and Wikipedia pages\n",
    "        pattern = r'https\\:\\/\\/(\\w{2}.wikipedia\\.org\\/wiki\\/.+|www\\.(youtube\\.com.+|linkedin\\.com.+))'\n",
    "        if re.search(pattern, link) == None:\n",
    "            # The first result among the filtered at this point is MOST LIKELY the official website\n",
    "            website = True\n",
    "            break\n",
    "\n",
    "if website:\n",
    "    print(str(website) + \", at index [\" + str(i) + \"]\")\n",
    "    print(\"Link found: \" + title + \" @ \" + link)\n",
    "else:\n",
    "    print(\"No official website found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ddd5e4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba31559a",
   "metadata": {},
   "source": [
    "Finding social media presence <br>\n",
    "Limited to these social media sites: <br>\n",
    "- Facebook\n",
    "- Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b882617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = channel_name + query_tail[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b398bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_response = google_resource.list(\n",
    "    q=query,\n",
    "    cx=cseKey\n",
    ").execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2804fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = channel_name + query_tail[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378312dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_response = google_resource.list(\n",
    "    q=query,\n",
    "    cx=cseKey\n",
    ").execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11a32d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching for a Facebook profile\n",
    "facebook = False\n",
    "\n",
    "for i in range(0, 10):\n",
    "    pattern = r'^https\\:\\/\\/www\\.facebook\\.com\\/.+\\/'\n",
    "    link = fb_response.get(\"items\")[i].get(\"formattedUrl\")\n",
    "    if re.search(pattern, link) != None:\n",
    "        title = fb_response.get(\"items\")[i].get(\"title\")\n",
    "        similarity = round(jaro_similarity(channel_name, title), 2)\n",
    "        \n",
    "        if similarity >= 0.80:\n",
    "            facebook = True\n",
    "            break\n",
    "            \n",
    "if facebook:\n",
    "    print(str(facebook) + \", at index [\" + str(i) + \"] with \" + str(similarity * 100) + \"% similarity.\")\n",
    "    print(\"Link found: \" + title + \" @ \" + link)\n",
    "else:\n",
    "    print(\"No Facebook profile found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919045e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching for a Twitter profile\n",
    "twitter = False\n",
    "\n",
    "for i in range(0, 10):\n",
    "    pattern = r'https\\:\\/\\/(twitter|x)\\.com\\/.+'\n",
    "    link = twitter_response.get(\"items\")[i].get(\"formattedUrl\")\n",
    "    if re.search(pattern, link) != None:\n",
    "        title = twitter_response.get(\"items\")[i].get(\"title\")\n",
    "        similarity = round(jaro_similarity(channel_name, title), 2)\n",
    "        \n",
    "        if similarity >= 0.80:\n",
    "            twitter = True\n",
    "            break\n",
    "            \n",
    "if twitter:\n",
    "    print(str(twitter) + \", at index [\" + str(i) + \"] with \" + str(similarity * 100) + \"% similarity.\")\n",
    "    print(\"Link found: \" + title + \" @ \" + link)re.search(r'.+(?=\\s-\\sWikipedia)', title)\n",
    "else:\n",
    "    print(\"No Twitter profile found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33553c6b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8387b4cc",
   "metadata": {},
   "source": [
    "Compiling everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1261e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_linkedIn(channel_name, query):\n",
    "    li_response = google_resource.list(\n",
    "        q=query,\n",
    "        cx=cseKey\n",
    "    ).execute()\n",
    "\n",
    "    found = False\n",
    "    pattern = r'https:\\/\\/www\\.linkedin\\.com\\/(company|in)\\/.+'  # Used to find specific profile links\n",
    "\n",
    "    for i in range(0, 10):\n",
    "        link = li_response.get(\"items\")[i].get(\"formattedUrl\")\n",
    "        if re.search(pattern, link) is not None:\n",
    "            # Get profile name from search result\n",
    "            match = re.search(r'\\w+\\s(\\w+)?', li_response.get(\"items\")[i].get(\"htmlTitle\"))\n",
    "            if match is not None:\n",
    "                profile_name = match.group()\n",
    "                \n",
    "                # Get similarity between found profile name and channel name\n",
    "                # This is to prevent false positives in finding a LinkedIn profile\n",
    "                similarity = round(jaro_similarity(channel_name.lower(), profile_name.lower()), 2)\n",
    "\n",
    "                # If n% similar, consider LinkedIn profile as found\n",
    "                if similarity >= 0.80:\n",
    "                    found = True\n",
    "                    break\n",
    "\n",
    "    if not found:\n",
    "        return found, None\n",
    "    else:\n",
    "        return found, link\n",
    "\n",
    "\n",
    "def find_wiki(channel_name, query):\n",
    "    wiki_response = google_resource.list(\n",
    "        q=query,\n",
    "        cx=cseKey\n",
    "    ).execute()\n",
    "\n",
    "    found = False\n",
    "    pattern = r'https:\\/\\/\\w{2}.wikipedia\\.org\\/wiki\\/.+'\n",
    "\n",
    "    for i in range(0, 10):\n",
    "        link = wiki_response.get(\"items\")[i].get(\"formattedUrl\")\n",
    "        if re.search(pattern, link) is not None:\n",
    "            # Get Wiki page name from search result\n",
    "            title = wiki_response.get(\"items\")[i].get(\"title\")\n",
    "            match = re.search(r'.+(?=\\s-\\sWikipedia)', title)\n",
    "            if match is not None:\n",
    "                page_name = match.group()\n",
    "                \n",
    "                # Get similarity between found Wiki page name and channel name\n",
    "                # This is to prevent false positives in finding a Wiki page\n",
    "                similarity = round(jaro_similarity(channel_name.lower(), page_name.lower()), 2)\n",
    "\n",
    "                # If n% similar, consider Wiki page as found\n",
    "                if similarity >= 0.80:\n",
    "                    found = True\n",
    "                    break\n",
    "\n",
    "    if not found:\n",
    "        return found, None\n",
    "    else:\n",
    "        return found, link\n",
    "\n",
    "\n",
    "def find_website(channel_name, query):\n",
    "    website_response = google_resource.list(\n",
    "        q=query,\n",
    "        cx=cseKey\n",
    "    ).execute()\n",
    "\n",
    "    found = False\n",
    "    # RegEx to exclude YouTube, LinkedIn, and Wikipedia pages\n",
    "    pattern = r'https\\:\\/\\/(\\w{2}.wikipedia\\.org\\/wiki\\/.+|www\\.(youtube\\.com.+|linkedin\\.com.+))'\n",
    "\n",
    "    for i in range(0, 10):\n",
    "        title = website_response.get(\"items\")[i].get(\"title\")\n",
    "        link = website_response.get(\"items\")[i].get(\"link\")\n",
    "        if channel_name.lower() in title.lower():\n",
    "            if re.search(pattern, link) is None:\n",
    "                # The first result among the filtered at this point is MOST LIKELY the official website\n",
    "                found = True\n",
    "                break\n",
    "\n",
    "    if not found:\n",
    "        return found, None\n",
    "    else:\n",
    "        return found, link\n",
    "\n",
    "\n",
    "def find_fb(channel_name, query):\n",
    "    fb_response = google_resource.list(\n",
    "        q=query,\n",
    "        cx=cseKey\n",
    "    ).execute()\n",
    "\n",
    "    found = False\n",
    "    pattern = r'^https\\:\\/\\/www\\.facebook\\.com\\/.+\\/'\n",
    "\n",
    "    for i in range(0, 10):\n",
    "        link = fb_response.get(\"items\")[i].get(\"formattedUrl\")\n",
    "        if re.search(pattern, link) is not None:\n",
    "            title = fb_response.get(\"items\")[i].get(\"title\")\n",
    "            similarity = round(jaro_similarity(channel_name.lower(), title.lower()), 2)\n",
    "\n",
    "            if similarity >= 0.80:\n",
    "                found = True\n",
    "                break\n",
    "\n",
    "    if not found:\n",
    "        return found, None\n",
    "    else:\n",
    "        return found, link\n",
    "\n",
    "\n",
    "def find_twitter(channel_name, query):\n",
    "    twitter_response = google_resource.list(\n",
    "        q=query,\n",
    "        cx=cseKey\n",
    "    ).execute()\n",
    "\n",
    "    found = False\n",
    "    pattern = r'https\\:\\/\\/(twitter|x)\\.com\\/.+'\n",
    "\n",
    "    for i in range(0, 10):\n",
    "        link = twitter_response.get(\"items\")[i].get(\"formattedUrl\")\n",
    "        if re.search(pattern, link) is not None:\n",
    "            title = twitter_response.get(\"items\")[i].get(\"title\")\n",
    "            similarity = round(jaro_similarity(channel_name.lower(), title.lower()), 2)\n",
    "\n",
    "            if similarity >= 0.80:\n",
    "                found = True\n",
    "                break\n",
    "\n",
    "    if not found:\n",
    "        return found, None\n",
    "    else:\n",
    "        return found, link\n",
    "\n",
    "\n",
    "def find_sources(channel_names, channel_IDs):\n",
    "    pbar = tqdm(total=len(channel_names))\n",
    "    pbar.set_description(\"Finding sources...\")\n",
    "\n",
    "    temp_list = []\n",
    "    columns = [\n",
    "        \"channel_id\", \"channel_title\",\n",
    "        \"profiles\", \"website\", \"social_media_presence\",\n",
    "        \"vs\"\n",
    "    ]\n",
    "\n",
    "    query_tail = [\n",
    "        \" LinkedIn\",\n",
    "        \" Wiki\",\n",
    "        \" Official Website\",\n",
    "        \" Facebook\",\n",
    "        \" Twitter\"\n",
    "    ]\n",
    "\n",
    "    for channel_name in channel_names:\n",
    "\n",
    "        linkedIn_found = find_linkedIn(channel_name, channel_name + query_tail[0])\n",
    "        wiki_found = find_wiki(channel_name, channel_name + query_tail[1])\n",
    "        site_found = find_website(channel_name, channel_name + query_tail[2])\n",
    "        fb_found = find_fb(channel_name, channel_name + query_tail[3])\n",
    "        twitter_found = find_twitter(channel_name, channel_name + query_tail[4])\n",
    "\n",
    "        profiles = 0\n",
    "        website = 0\n",
    "        social_media_presence = 0\n",
    "\n",
    "        if linkedIn_found[0] and wiki_found[0]:\n",
    "            profile = 3\n",
    "        elif linkedIn_found[0] and not wiki_found[0]:\n",
    "            profile = 2\n",
    "        elif not linkedIn_found[0] and wiki_found[0]:\n",
    "            profile = 1\n",
    "\n",
    "        if site_found[0]:\n",
    "            website = 2\n",
    "\n",
    "        if fb_found[0] or twitter_found[0]:\n",
    "            social_media_presence = 1\n",
    "\n",
    "        # '''\n",
    "        record = [\n",
    "            channel_name, # channel_id\n",
    "            channel_IDs.get(channel_name), # channel_title\n",
    "            profiles, # profiles\n",
    "            website, # website\n",
    "            social_media_presence, # social_media_presence\n",
    "            np.nan # vs\n",
    "        ]\n",
    "        \n",
    "        temp_list.append(record)\n",
    "        \n",
    "        # '''\n",
    "        \n",
    "        ''' For testing\n",
    "        print(f\"--- {channel_name} ---\")\n",
    "        print(\"PROFILES\")\n",
    "        print(f\"LinkedIn: {linkedIn_found[1]}\")\n",
    "        print(f\"Wiki: {wiki_found[1]}\")\n",
    "        print(\"\")\n",
    "\n",
    "        print(\"WEBSITE\")\n",
    "        print(f\"Official Website: {site_found[1]}\")\n",
    "        print(\"\")\n",
    "\n",
    "        print(\"SOCIAL MEDIA PRESENCE\")\n",
    "        if fb_found[0]:\n",
    "            print(f\"Facebook: {fb_found[1]}\")\n",
    "\n",
    "        if twitter_found[0]:\n",
    "            print(f\"Twitter: {twitter_found[1]}\")\n",
    "        print(\"\")\n",
    "        # '''\n",
    "        \n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    \n",
    "    # TODO: Convert temp_list into DataFrame after loop\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20b3bb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_names = df[\"channel_title\"].unique()\n",
    "channel_IDs = df[[\"channel_id\", \"channel_title\"]].groupby(\"channel_title\").first().to_dict().get(\"channel_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff32ab6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding sources...:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "HttpError",
     "evalue": "<HttpError 403 when requesting https://customsearch.googleapis.com/customsearch/v1?q=CNN+Philippines+LinkedIn&cx=23c1c70a203ac4852&key=AIzaSyBTRvkhM6ESdLHu0djfP39-IKHufQogxOI&alt=json returned \"Requests to this API customsearch method google.customsearch.v1.CustomSearchService.List are blocked.\". Details: \"[{'message': 'Requests to this API customsearch method google.customsearch.v1.CustomSearchService.List are blocked.', 'domain': 'global', 'reason': 'forbidden'}]\">",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25724/3272930940.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfind_sources\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchannel_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannel_IDs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25724/2931449271.py\u001b[0m in \u001b[0;36mfind_sources\u001b[1;34m(channel_names, channel_IDs)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mchannel_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchannel_names\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m         \u001b[0mlinkedIn_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_linkedIn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchannel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannel_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mquery_tail\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m         \u001b[0mwiki_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_wiki\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchannel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannel_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mquery_tail\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[0msite_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_website\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchannel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannel_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mquery_tail\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25724/2931449271.py\u001b[0m in \u001b[0;36mfind_linkedIn\u001b[1;34m(channel_name, query)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfind_linkedIn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchannel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     li_response = google_resource.list(\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[0mq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mcx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcseKey\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     ).execute()\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\googleapiclient\\_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mpositional_parameters_enforcement\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mPOSITIONAL_WARNING\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\googleapiclient\\http.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, http, num_retries)\u001b[0m\n\u001b[0;32m    936\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 938\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mHttpError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muri\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    939\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpostproc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHttpError\u001b[0m: <HttpError 403 when requesting https://customsearch.googleapis.com/customsearch/v1?q=CNN+Philippines+LinkedIn&cx=23c1c70a203ac4852&key=AIzaSyBTRvkhM6ESdLHu0djfP39-IKHufQogxOI&alt=json returned \"Requests to this API customsearch method google.customsearch.v1.CustomSearchService.List are blocked.\". Details: \"[{'message': 'Requests to this API customsearch method google.customsearch.v1.CustomSearchService.List are blocked.', 'domain': 'global', 'reason': 'forbidden'}]\">"
     ]
    }
   ],
   "source": [
    "find_sources(channel_names[0:5], channel_IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33c838c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c9daa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0975be85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405a4564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d3a3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
