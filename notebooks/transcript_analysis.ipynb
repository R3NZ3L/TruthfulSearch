{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de359323-94c7-42b4-b6e0-e6261bdca096",
   "metadata": {},
   "source": [
    "Guide for training LDA model was a video by Srinivasan, S. (2020)<br>\n",
    "Link: https://www.youtube.com/watch?v=25JOEnrz40c&list=PL0rtpP-8GFfR2orPIzBttl15_NfDhkujw&index=4&t=518s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32b0562c-9fd6-4749-b321-7f73a3243780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import regex as re\n",
    "\n",
    "from time import time\n",
    "from time import strftime\n",
    "from time import gmtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9073169d-b518-4b58-ab96-d276599b510a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition\n",
    "from stop_words import get_stop_words\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb0e18b-0e01-469a-a3db-00e971dc5585",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ecc2a7e-8dd6-4beb-ab34-b8296704d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"covid_vaccine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10132217-08dd-4802-8995-3fd807429f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>video_transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im3otpqYAiQ</td>\n",
       "      <td>Covid Vaccine Study Finds Links to Health Cond...</td>\n",
       "      <td>[CC may contain inaccuracies] In terms of how...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SkcAZfrYYXM</td>\n",
       "      <td>Two very rare COVID vaccine side effects detec...</td>\n",
       "      <td>okay we're going to finish with the guardian ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7MAlEYqWUTk</td>\n",
       "      <td>Being Nice to Anti-Vaxxers</td>\n",
       "      <td>so you're against the covert vaccine oh here ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uiwjAj0zfKQ</td>\n",
       "      <td>If You Get All 5 COVID Vaccines</td>\n",
       "      <td>and all right we're done now if you're feelin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jPs4_MeuX7U</td>\n",
       "      <td>New Covid vaccine study links jab to heart and...</td>\n",
       "      <td>a latest covid-19 study is providing answers ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                        video_title  \\\n",
       "0  im3otpqYAiQ  Covid Vaccine Study Finds Links to Health Cond...   \n",
       "1  SkcAZfrYYXM  Two very rare COVID vaccine side effects detec...   \n",
       "2  7MAlEYqWUTk                         Being Nice to Anti-Vaxxers   \n",
       "3  uiwjAj0zfKQ                    If You Get All 5 COVID Vaccines   \n",
       "4  jPs4_MeuX7U  New Covid vaccine study links jab to heart and...   \n",
       "\n",
       "                                    video_transcript  \n",
       "0   [CC may contain inaccuracies] In terms of how...  \n",
       "1   okay we're going to finish with the guardian ...  \n",
       "2   so you're against the covert vaccine oh here ...  \n",
       "3   and all right we're done now if you're feelin...  \n",
       "4   a latest covid-19 study is providing answers ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../datasets/covid_vaccine/covid_vaccine.csv\"\n",
    "df = pd.read_csv(path).drop(\"Unnamed: 0\", axis=1)[[\"video_id\", \"video_title\", \"video_transcript\"]].astype(str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f6027c-14f6-48ff-b332-1464e3a942a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "433bd1ec-8659-4236-82d1-7bb92c979821",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([  7,  15,  37,  43,  45,  48,  52,  56,  69,  77,  81,  83,  93,  95,\n",
       "       104, 111, 117, 122, 134, 136, 137, 143],\n",
       "      dtype='int64')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indices of nan transcripts\n",
    "drop_indices = df[[\"video_id\", \"video_transcript\"]].loc[df[\"video_transcript\"] == 'nan'].index\n",
    "drop_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb549f9d-a487-431e-90f8-b03ed46b1241",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop(drop_indices, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78ce657b-bf67-4523-adfd-01a66eb69eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = []\n",
    "replacements = []\n",
    "\n",
    "# [0] Removing occurances of \\xa0 and \\n\n",
    "patterns.append('(\\\\xa0|\\\\n)')\n",
    "replacements.append(' ')\n",
    "\n",
    "# [1] Removing text enclosed in brackets\n",
    "patterns.append('\\[(\\w|\\s)+\\]')\n",
    "replacements.append('')\n",
    "\n",
    "# [2] Replacing stray '000's to 'thousand'\n",
    "patterns.append('(?<=\\s)000(?=\\s)')\n",
    "replacements.append('thousand')\n",
    "\n",
    "# [3, 4] Mistranscriptions of the word 'COVID'\n",
    "patterns.append('(?<=\\s)(C|c)o(ve(r)?t|id)(?=\\s)')\n",
    "patterns.append('(C|c)overed(?=\\s(vacc|infe))')\n",
    "replacements.append('COVID')\n",
    "replacements.append('COVID')\n",
    "\n",
    "# [5] Mistranscriptions of the word 'COVID-19'\n",
    "patterns.append('(?<=\\s)(C|c)(oveted|o9|o\\s19)(?=\\s)')\n",
    "replacements.append('COVID19')\n",
    "\n",
    "# [6] Replacing '%' with the word 'percent'\n",
    "patterns.append('(?<=\\d)\\%')\n",
    "replacements.append(' percent')\n",
    "\n",
    "# [7] Removing 'Speaker %d:' occurances\n",
    "patterns.append('Speaker\\s\\d\\:')\n",
    "replacements.append('')\n",
    "\n",
    "# [8] Removing '[\\xa0__\\xa0]'\n",
    "patterns.append('\\[\\\\xa0\\_\\_\\\\xa0\\]')\n",
    "replacements.append('')\n",
    "\n",
    "# [9] Removing >> occurances\n",
    "patterns.append('\\>\\>(\\>+)?')\n",
    "replacements.append('')\n",
    "\n",
    "# [10] Removing 'Reporter:' occurances\n",
    "patterns.append('Reporter\\:')\n",
    "replacements.append('')\n",
    "\n",
    "# [11] Removing weird +@ occurances\n",
    "patterns.append('\\+\\@')\n",
    "replacements.append('')\n",
    "\n",
    "# [12] Removing stray - occurances\n",
    "patterns.append('(?<=\\s)\\-(\\-+)?(?=\\s)')\n",
    "replacements.append('')\n",
    "\n",
    "# [13] Removing text within parentheses\n",
    "patterns.append('\\((\\w|\\s)+\\)')\n",
    "replacements.append('')\n",
    "\n",
    "# [14] Combining stray instances of '19' with the word 'covid' if it exists next to it\n",
    "patterns.append('(covid|COVID)(\\s|-)?19')\n",
    "replacements.append('COVID19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47c2c80c-f338-4e0a-b011-1ac39d78185d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts = df[\"video_transcript\"].tolist()\n",
    "cleaned = []\n",
    "len(transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "200ebe0f-b88a-41bc-ab13-553d8505df0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for transcript in transcripts:\n",
    "    result = re.sub(patterns[0], replacements[0], transcript)\n",
    "    \n",
    "    for i in range(1, len(patterns)):\n",
    "        result = re.sub(patterns[i], replacements[i], result)\n",
    "    \n",
    "    cleaned.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9388285-3cb8-461c-a851-8e434533635e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "660dfe0d-5fe6-4765-b9f9-f2355976ff2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>video_transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im3otpqYAiQ</td>\n",
       "      <td>Covid Vaccine Study Finds Links to Health Cond...</td>\n",
       "      <td>In terms of how widespread the adverse event...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SkcAZfrYYXM</td>\n",
       "      <td>Two very rare COVID vaccine side effects detec...</td>\n",
       "      <td>okay we're going to finish with the guardian ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7MAlEYqWUTk</td>\n",
       "      <td>Being Nice to Anti-Vaxxers</td>\n",
       "      <td>so you're against the COVID vaccine oh here w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uiwjAj0zfKQ</td>\n",
       "      <td>If You Get All 5 COVID Vaccines</td>\n",
       "      <td>and all right we're done now if you're feelin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jPs4_MeuX7U</td>\n",
       "      <td>New Covid vaccine study links jab to heart and...</td>\n",
       "      <td>a latest COVID19 study is providing answers t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                        video_title  \\\n",
       "0  im3otpqYAiQ  Covid Vaccine Study Finds Links to Health Cond...   \n",
       "1  SkcAZfrYYXM  Two very rare COVID vaccine side effects detec...   \n",
       "2  7MAlEYqWUTk                         Being Nice to Anti-Vaxxers   \n",
       "3  uiwjAj0zfKQ                    If You Get All 5 COVID Vaccines   \n",
       "4  jPs4_MeuX7U  New Covid vaccine study links jab to heart and...   \n",
       "\n",
       "                                    video_transcript  \n",
       "0    In terms of how widespread the adverse event...  \n",
       "1   okay we're going to finish with the guardian ...  \n",
       "2   so you're against the COVID vaccine oh here w...  \n",
       "3   and all right we're done now if you're feelin...  \n",
       "4   a latest COVID19 study is providing answers t...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts_df = pd.DataFrame(\n",
    "    {\n",
    "        'video_id': df[\"video_id\"].tolist(),\n",
    "        'video_title': df[\"video_title\"].tolist(),\n",
    "        'video_transcript': cleaned\n",
    "    }\n",
    ")\n",
    "transcripts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4869dea4-7340-4bac-a098-39036f135422",
   "metadata": {},
   "source": [
    "---\n",
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8289125-a285-443d-82be-fcf9057cacad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(transcripts_df, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86c87ed3-5e8d-47f8-a8da-2da2053301aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: (89, 3)\n",
      "Test set size: (39, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f3e677-d2e1-4960-8b28-a836c8d504b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66dce008-05d9-4bcf-87b7-1f95117da40a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcripts = X_train['video_transcript'].tolist()\n",
    "stop_words = get_stop_words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8489923b-66b1-4220-886e-4d64447fd5b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_and_lemmatize(transcript):\n",
    "    tokens = [word.lower() for word in word_tokenize(transcript) if len(word) > 3]\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for token in tokens:\n",
    "        lemmas.append(wnl.lemmatize(token))\n",
    "    \n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "654733ce-5c72-46dc-a91c-5c7ad5d433fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    tokenizer=tokenize_and_lemmatize,\n",
    "    stop_words=stop_words,\n",
    "    max_df=0.85,\n",
    "    min_df=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83f3db3d-1007-48ef-9e2e-b82d8503422f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geloa\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\geloa\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'must'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actually</th>\n",
       "      <th>already</th>\n",
       "      <th>also</th>\n",
       "      <th>another</th>\n",
       "      <th>around</th>\n",
       "      <th>back</th>\n",
       "      <th>believe</th>\n",
       "      <th>better</th>\n",
       "      <th>blood</th>\n",
       "      <th>body</th>\n",
       "      <th>...</th>\n",
       "      <th>want</th>\n",
       "      <th>week</th>\n",
       "      <th>well</th>\n",
       "      <th>whether</th>\n",
       "      <th>will</th>\n",
       "      <th>work</th>\n",
       "      <th>working</th>\n",
       "      <th>world</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89 rows × 135 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    actually  already  also  another  around  back  believe  better  blood  \\\n",
       "0          0        0     0        0       0     1        0       0      1   \n",
       "1          1        0     5        2       0     0        0       2      4   \n",
       "2          1        0     2        0       0     0        0       0      0   \n",
       "3          3        0     3        3       1     3        0       0      0   \n",
       "4          0        0     0        0       0     0        0       0      0   \n",
       "..       ...      ...   ...      ...     ...   ...      ...     ...    ...   \n",
       "84         6        5     7        6      10     4        2       1      1   \n",
       "85         5        2     5        1       1     1        0       1      0   \n",
       "86         2        1     2        1       0     4        0       1      0   \n",
       "87         0        0     0        0       0     0        0       0      0   \n",
       "88         1        1     1        2       0     0        0       1      0   \n",
       "\n",
       "    body  ...  want  week  well  whether  will  work  working  world  yeah  \\\n",
       "0      0  ...     0     1     0        0     0     0        0      0     0   \n",
       "1      1  ...     1     3     2        0     0     2        0      0     0   \n",
       "2      0  ...     0     0     0        0     1     0        0      0     0   \n",
       "3      0  ...     1     0     0        1     0     5        1      2     1   \n",
       "4      0  ...     0     0     0        0     4     0        0      0     0   \n",
       "..   ...  ...   ...   ...   ...      ...   ...   ...      ...    ...   ...   \n",
       "84     3  ...     7     6     6        3     5     7        3      6     3   \n",
       "85     3  ...     0     2     4        1     4     1        2      0     0   \n",
       "86     0  ...     1     2     4        1     3     1        0      1     0   \n",
       "87     0  ...     1     1     0        0     0     0        0      0     0   \n",
       "88     0  ...     5     0     5        0    10     2        0      4     1   \n",
       "\n",
       "    year  \n",
       "0      1  \n",
       "1      2  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "..   ...  \n",
       "84     5  \n",
       "85     6  \n",
       "86     1  \n",
       "87     1  \n",
       "88     1  \n",
       "\n",
       "[89 rows x 135 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = vectorizer.fit_transform(transcripts)\n",
    "matrix_df = pd.DataFrame(data=matrix.toarray(), columns = vectorizer.get_feature_names_out())\n",
    "matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d119e99e-de1f-4ec7-a504-9f6e9a57a053",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['actually', 'already', 'also', 'another', 'around', 'back',\n",
       "       'believe', 'better', 'blood', 'body', 'called', 'care', 'case',\n",
       "       'cause', 'come', 'coming', 'country', 'course', 'covid', 'covid19',\n",
       "       'data', 'day', 'different', 'doctor', 'doe', 'done', 'effect',\n",
       "       'even', 'every', 'fact', 'feel', 'find', 'first', 'five', 'found',\n",
       "       'getting', 'give', 'given', 'going', 'good', 'group', 'health',\n",
       "       'heart', 'help', 'higher', 'important', 'infection', 'information',\n",
       "       'just', 'keep', 'kind', 'know', 'last', 'life', 'like', 'little',\n",
       "       'long', 'look', 'made', 'make', 'many', 'mean', 'medical', 'might',\n",
       "       'million', 'month', 'mrna', 'much', 'need', 'never', 'news',\n",
       "       'next', 'number', 'okay', 'pandemic', 'patient', 'people',\n",
       "       'percent', 'point', 'problem', 'question', 'really', 'report',\n",
       "       'research', 'right', 'risk', 'safe', 'said', 'say', 'saying',\n",
       "       'second', 'seen', 'shot', 'show', 'side', 'small', 'someone',\n",
       "       'something', 'start', 'state', 'still', 'story', 'study', 'sure',\n",
       "       'system', 'take', 'talk', 'talking', 'tell', 'term', 'thank',\n",
       "       'thing', 'think', 'thought', 'thousand', 'three', 'time', 'today',\n",
       "       'told', 'trying', 'understand', 'vaccinated', 'vaccination',\n",
       "       'video', 'virus', 'want', 'week', 'well', 'whether', 'will',\n",
       "       'work', 'working', 'world', 'yeah', 'year'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = np.array(vectorizer.get_feature_names_out())\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b2f5730-1543-42f9-a522-6f1e961dc305",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lda = decomposition.LatentDirichletAllocation(n_components=10, max_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cd22739-b468-4bf1-a1e8-56c434d65a9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc_vectors = lda.fit_transform(matrix)\n",
    "h1 = lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07a9993e-42b6-4b2d-97e4-eb31d1688bec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.04172058e-01, 4.16796446e-03, 4.16775862e-03, 4.16689572e-03,\n",
       "        8.33664640e-02, 5.11114395e-01, 4.16886233e-03, 4.16808168e-03,\n",
       "        4.17087207e-03, 2.76336648e-01],\n",
       "       [4.87961546e-04, 4.87972837e-04, 4.87919525e-04, 1.13521218e-01,\n",
       "        4.87953393e-04, 4.87950257e-04, 4.87926588e-04, 2.84775087e-01,\n",
       "        3.26457381e-02, 5.66130272e-01],\n",
       "       [5.90743576e-01, 2.55075962e-01, 3.57282026e-03, 3.57260971e-03,\n",
       "        3.57151631e-03, 3.57284318e-03, 1.29173386e-01, 3.57255806e-03,\n",
       "        3.57236188e-03, 3.57236683e-03]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vectors[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "953d3b99-75ca-458a-9f85-8734463af16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.25482326,  0.10001946, 15.01137852,  0.1000068 , 18.97997304,\n",
       "       19.2593669 ,  0.10001169,  0.10000407,  0.10002454, 44.16704374,\n",
       "        0.10000218,  0.10000787,  0.10000615,  0.10000457,  0.10001185,\n",
       "        0.100007  ,  0.10000832,  0.10000729, 24.93964483, 23.04548946,\n",
       "        0.10000481, 12.24875066,  0.10000456, 18.62942298,  0.10000563,\n",
       "        0.10001149,  0.10000974,  0.10000538,  0.10001003,  0.10000488,\n",
       "       26.58916879,  0.10000787, 27.41253437, 60.93855093,  0.10000636,\n",
       "       20.21591019, 14.15231561,  0.10000956,  0.10000914,  0.10001287,\n",
       "        0.10000361, 44.83568776,  0.1000075 , 42.13576707,  0.10000373,\n",
       "        0.10000414,  0.10001001,  9.90572737, 65.89373249,  0.10004845,\n",
       "       15.88604088, 22.36817876,  0.10000801, 10.60929644,  0.10000837,\n",
       "        0.10000755,  0.10001032, 13.61248922,  0.1000042 , 21.68981437,\n",
       "        0.10001482,  0.10000721, 15.16802528,  0.10000836,  0.10000237,\n",
       "        0.10001079,  0.10000335,  0.10000618, 25.11296364,  0.10000581,\n",
       "       37.61847039,  0.10000597, 36.00721083,  0.10001361,  0.10000755,\n",
       "        0.10000536, 23.48347187,  0.10000597,  0.10000802,  0.10000523,\n",
       "        0.10000526,  0.10000492, 14.93615526,  0.10000422, 55.03006526,\n",
       "        0.10000285,  0.10000526,  0.10000611, 21.09883938,  0.10000404,\n",
       "        0.10001972,  0.10000337, 92.99140479,  0.10000761,  0.10000794,\n",
       "        0.10000604,  0.1000046 ,  0.10000671,  0.10000423, 43.93819369,\n",
       "       11.33411516,  0.10000693,  0.10000487,  0.1000142 ,  0.10000572,\n",
       "       26.00271759,  0.10000435,  0.10001346,  0.10001052,  0.10000802,\n",
       "        0.10000367, 38.69080378,  0.10000658,  0.10000485,  0.10000632,\n",
       "       32.96245604, 13.71183739,  0.10000409,  0.10000854,  0.10000601,\n",
       "        0.10000579,  0.10001012, 16.33086437,  0.10000912,  0.10000487,\n",
       "        0.10000803, 23.24891112, 21.48390491,  0.100008  , 64.83603246,\n",
       "        0.10000622,  0.10000273,  1.20414098,  0.10000436,  0.10000799])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e65b1a37-9100-4228-91a7-2c5296ee837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from the guide\n",
    "num_words = 15\n",
    "top_words = lambda t: [vocabulary[i] for i in np.argsort(t)[:-num_words-1:-1]]\n",
    "topic_words = ([top_words(t) for t in h1])\n",
    "topics = [' '.join(t) for t in topic_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9af40592-d1fa-4993-a9d4-3373a93b4f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shot just will five right health body state help thing news number three first feel',\n",
       " 'vaccinated data people risk group might percent just look vaccination time effect actually study side',\n",
       " 'health question many risk care study report also covid19 doe say just mrna getting different',\n",
       " 'like know think yeah getting just feel right time really doe going day kind said',\n",
       " 'blood heart research small side seen made said last another pandemic month country also trying',\n",
       " 'covid know people risk study like million data really thing right vaccination kind number okay',\n",
       " 'people long think virus thing patient different well really going work just know like infection',\n",
       " 'people know just like going will thing year want well think covid said time right',\n",
       " 'mrna virus body system make shot also covid19 called around second important work information start',\n",
       " 'people covid19 vaccination risk case cause will covid also doe video medical blood different made']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d7d297-7b59-4cae-8a73-d037b0372c6c",
   "metadata": {},
   "source": [
    "---\n",
    "### Top2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a588fad7-4148-4566-b440-680d520b9413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from top2vec import Top2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a422379-1579-4aa5-b093-8a103a8a3f8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>video_transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im3otpqYAiQ</td>\n",
       "      <td>Covid Vaccine Study Finds Links to Health Cond...</td>\n",
       "      <td>In terms of how widespread the adverse event...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SkcAZfrYYXM</td>\n",
       "      <td>Two very rare COVID vaccine side effects detec...</td>\n",
       "      <td>okay we're going to finish with the guardian ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7MAlEYqWUTk</td>\n",
       "      <td>Being Nice to Anti-Vaxxers</td>\n",
       "      <td>so you're against the COVID vaccine oh here w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uiwjAj0zfKQ</td>\n",
       "      <td>If You Get All 5 COVID Vaccines</td>\n",
       "      <td>and all right we're done now if you're feelin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jPs4_MeuX7U</td>\n",
       "      <td>New Covid vaccine study links jab to heart and...</td>\n",
       "      <td>a latest COVID19 study is providing answers t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>o2HLkFi4Qtw</td>\n",
       "      <td>5 things NOT TO DO after getting the COVID-19 ...</td>\n",
       "      <td>HAPPENING HAPPENING ACROSS HAPPENING ACROSS T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>o-yTrL5aszM</td>\n",
       "      <td>#shorts - COVID Vaccine &amp;amp; Green Card appli...</td>\n",
       "      <td>so what's the current update on COVID19 vacci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>nhb1zIYXUP8</td>\n",
       "      <td>Nurse faints after getting COVID vaccine</td>\n",
       "      <td>meanwhile we're seeing different reactions to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>It7VNzhAqOs</td>\n",
       "      <td>Blood Clots after COVID Vaccine</td>\n",
       "      <td>Hello, welcome to my channel Medicine with  D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Mnn4ZqeKxVM</td>\n",
       "      <td>Tucker Carlson Tonight 2/23/24 | Tucker Carlso...</td>\n",
       "      <td>them just two years later but we should becau...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        video_id                                        video_title  \\\n",
       "0    im3otpqYAiQ  Covid Vaccine Study Finds Links to Health Cond...   \n",
       "1    SkcAZfrYYXM  Two very rare COVID vaccine side effects detec...   \n",
       "2    7MAlEYqWUTk                         Being Nice to Anti-Vaxxers   \n",
       "3    uiwjAj0zfKQ                    If You Get All 5 COVID Vaccines   \n",
       "4    jPs4_MeuX7U  New Covid vaccine study links jab to heart and...   \n",
       "..           ...                                                ...   \n",
       "123  o2HLkFi4Qtw  5 things NOT TO DO after getting the COVID-19 ...   \n",
       "124  o-yTrL5aszM  #shorts - COVID Vaccine &amp; Green Card appli...   \n",
       "125  nhb1zIYXUP8           Nurse faints after getting COVID vaccine   \n",
       "126  It7VNzhAqOs                    Blood Clots after COVID Vaccine   \n",
       "127  Mnn4ZqeKxVM  Tucker Carlson Tonight 2/23/24 | Tucker Carlso...   \n",
       "\n",
       "                                      video_transcript  \n",
       "0      In terms of how widespread the adverse event...  \n",
       "1     okay we're going to finish with the guardian ...  \n",
       "2     so you're against the COVID vaccine oh here w...  \n",
       "3     and all right we're done now if you're feelin...  \n",
       "4     a latest COVID19 study is providing answers t...  \n",
       "..                                                 ...  \n",
       "123   HAPPENING HAPPENING ACROSS HAPPENING ACROSS T...  \n",
       "124   so what's the current update on COVID19 vacci...  \n",
       "125   meanwhile we're seeing different reactions to...  \n",
       "126   Hello, welcome to my channel Medicine with  D...  \n",
       "127   them just two years later but we should becau...  \n",
       "\n",
       "[128 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666973db-1fce-4d16-9707-b81ee205babc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install tensorflow tensorflow_hub\n",
    "t2v_model = Top2Vec(transcripts_df[\"video_transcript\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cbcfd6-1be6-41b5-adbd-db4e0728a8f5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ba4b73f-b33b-4232-9292-8ce8ef14ab21",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'is_nltk_available' from 'transformers.utils.import_utils' (C:\\Users\\geloa\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BERTopic\n\u001b[0;32m      2\u001b[0m topic_model \u001b[38;5;241m=\u001b[39m BERTopic()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bertopic\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bertopic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BERTopic\n\u001b[0;32m      3\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.16.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBERTopic\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bertopic\\_bertopic.py:48\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plotting\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseCluster\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseEmbedder\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrepresentation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_mmr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mmr\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m select_backend\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bertopic\\backend\\__init__.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Multimodal Embeddings\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multimodal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiModalBackend\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install bertopic[vision]` \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bertopic\\backend\\_multimodal.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Union\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseEmbedder\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMultiModalBackend\u001b[39;00m(BaseEmbedder):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sentence_transformers\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.4.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence-transformers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentencesDataset, ParallelSentencesDataset\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLoggingHandler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceTransformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sentence_transformers\\datasets\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDenoisingAutoEncoderDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DenoisingAutoEncoderDataset\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mNoDuplicatesDataLoader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NoDuplicatesDataLoader\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mParallelSentencesDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sentence_transformers\\datasets\\DenoisingAutoEncoderDataset.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mInputExample\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InputExample\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_nltk_available, NLTK_IMPORT_ERROR\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDenoisingAutoEncoderDataset\u001b[39;00m(Dataset):\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    The DenoisingAutoEncoderDataset returns InputExamples in the format: texts=[noise_fn(sentence), sentence]\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    It is used in combination with the DenoisingAutoEncoderLoss: Here, a decoder tries to re-construct the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m    :param noise_fn: A noise function: Given a string, it returns a string with noise, e.g. deleted words\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'is_nltk_available' from 'transformers.utils.import_utils' (C:\\Users\\geloa\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py)"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "topic_model = BERTopic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9c030b-4418-4378-85b4-93b37e63d408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f955847-e5b4-4f5f-b088-3a0a0c2289d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
