{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de359323-94c7-42b4-b6e0-e6261bdca096",
   "metadata": {},
   "source": [
    "Guide for training LDA model was a video by Srinivasan, S. (2020)<br>\n",
    "Link: https://www.youtube.com/watch?v=25JOEnrz40c&list=PL0rtpP-8GFfR2orPIzBttl15_NfDhkujw&index=4&t=518s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32b0562c-9fd6-4749-b321-7f73a3243780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import regex as re\n",
    "\n",
    "from time import time\n",
    "from time import strftime\n",
    "from time import gmtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9073169d-b518-4b58-ab96-d276599b510a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition\n",
    "from stop_words import get_stop_words\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb0e18b-0e01-469a-a3db-00e971dc5585",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ecc2a7e-8dd6-4beb-ab34-b8296704d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"covid_vaccine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10132217-08dd-4802-8995-3fd807429f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>video_transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2IXl4qJGrRk</td>\n",
       "      <td>A man deliberately got 217 Covid shots. Hereâ€™s...</td>\n",
       "      <td>A German man has puzzled scientists after he ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HtTalpY-J-M</td>\n",
       "      <td>COVID: German man vaccinated 217 times had no ...</td>\n",
       "      <td>a 62-year-old German man from magur claims he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jPs4_MeuX7U</td>\n",
       "      <td>New Covid vaccine study links jab to heart and...</td>\n",
       "      <td>a latest covid-19 study is providing answers ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WhiBpmH1mE4</td>\n",
       "      <td>MAN GETS 217 COVID VACCINES! ðŸ˜±ðŸ˜±ðŸ˜± THIS is What ...</td>\n",
       "      <td>a 62-year-old man who lives in Germany uh got...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LfmhYVCCGhc</td>\n",
       "      <td>Joe Rogan says tons of people &amp;quot;died sudde...</td>\n",
       "      <td>Speaker 1: This is really painful to watch. O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                        video_title  \\\n",
       "0  2IXl4qJGrRk  A man deliberately got 217 Covid shots. Hereâ€™s...   \n",
       "1  HtTalpY-J-M  COVID: German man vaccinated 217 times had no ...   \n",
       "2  jPs4_MeuX7U  New Covid vaccine study links jab to heart and...   \n",
       "3  WhiBpmH1mE4  MAN GETS 217 COVID VACCINES! ðŸ˜±ðŸ˜±ðŸ˜± THIS is What ...   \n",
       "4  LfmhYVCCGhc  Joe Rogan says tons of people &quot;died sudde...   \n",
       "\n",
       "                                    video_transcript  \n",
       "0   A German man has puzzled scientists after he ...  \n",
       "1   a 62-year-old German man from magur claims he...  \n",
       "2   a latest covid-19 study is providing answers ...  \n",
       "3   a 62-year-old man who lives in Germany uh got...  \n",
       "4   Speaker 1: This is really painful to watch. O...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../datasets/covid_vaccine/videos.csv\"\n",
    "df = pd.read_csv(path).drop(\"Unnamed: 0\", axis=1)[[\"video_id\", \"video_title\", \"video_transcript\"]].astype(str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f6027c-14f6-48ff-b332-1464e3a942a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "433bd1ec-8659-4236-82d1-7bb92c979821",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([ 14,  15,  18,  20,  24,  26,  40,  59,  64,  71,  72,  80,  81,  96,\n",
       "        99, 101, 109, 111, 114, 124, 125, 138, 158, 159, 184, 190],\n",
       "      dtype='int64')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indices of nan transcripts\n",
    "drop_indices = df[[\"video_id\", \"video_transcript\"]].loc[df[\"video_transcript\"] == 'nan'].index\n",
    "drop_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb549f9d-a487-431e-90f8-b03ed46b1241",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop(drop_indices, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78ce657b-bf67-4523-adfd-01a66eb69eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = []\n",
    "replacements = []\n",
    "\n",
    "# [0] Removing occurances of \\xa0 and \\n\n",
    "patterns.append('(\\\\xa0|\\\\n)')\n",
    "replacements.append(' ')\n",
    "\n",
    "# [1] Removing text enclosed in brackets\n",
    "patterns.append('\\[(\\w|\\s)+\\]')\n",
    "replacements.append('')\n",
    "\n",
    "# [2] Replacing stray '000's to 'thousand'\n",
    "patterns.append('(?<=\\s)000(?=\\s)')\n",
    "replacements.append('thousand')\n",
    "\n",
    "# [3, 4] Mistranscriptions of the word 'COVID'\n",
    "patterns.append('(?<=\\s)(C|c)o(ve(r)?t|id)(?=\\s)')\n",
    "patterns.append('(C|c)overed(?=\\s(vacc|infe))')\n",
    "replacements.append('COVID')\n",
    "replacements.append('COVID')\n",
    "\n",
    "# [5] Mistranscriptions of the word 'COVID-19'\n",
    "patterns.append('(?<=\\s)(C|c)(oveted|o9|o\\s19)(?=\\s)')\n",
    "replacements.append('COVID19')\n",
    "\n",
    "# [6] Replacing '%' with the word 'percent'\n",
    "patterns.append('(?<=\\d)\\%')\n",
    "replacements.append(' percent')\n",
    "\n",
    "# [7] Removing 'Speaker %d:' occurances\n",
    "patterns.append('Speaker\\s\\d\\:')\n",
    "replacements.append('')\n",
    "\n",
    "# [8] Removing '[\\xa0__\\xa0]'\n",
    "patterns.append('\\[\\\\xa0\\_\\_\\\\xa0\\]')\n",
    "replacements.append('')\n",
    "\n",
    "# [9] Removing >> occurances\n",
    "patterns.append('\\>\\>(\\>+)?')\n",
    "replacements.append('')\n",
    "\n",
    "# [10] Removing 'Reporter:' occurances\n",
    "patterns.append('Reporter\\:')\n",
    "replacements.append('')\n",
    "\n",
    "# [11] Removing weird +@ occurances\n",
    "patterns.append('\\+\\@')\n",
    "replacements.append('')\n",
    "\n",
    "# [12] Removing stray - occurances\n",
    "patterns.append('(?<=\\s)\\-(\\-+)?(?=\\s)')\n",
    "replacements.append('')\n",
    "\n",
    "# [13] Removing text within parentheses\n",
    "patterns.append('\\((\\w|\\s)+\\)')\n",
    "replacements.append('')\n",
    "\n",
    "# [14] Combining stray instances of '19' with the word 'covid' if it exists next to it\n",
    "patterns.append('(covid|COVID)(\\s|-)?19')\n",
    "replacements.append('COVID19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47c2c80c-f338-4e0a-b011-1ac39d78185d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts = df[\"video_transcript\"].tolist()\n",
    "cleaned = []\n",
    "len(transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "200ebe0f-b88a-41bc-ab13-553d8505df0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for transcript in transcripts:\n",
    "    result = re.sub(patterns[0], replacements[0], transcript)\n",
    "    \n",
    "    for i in range(1, len(patterns)):\n",
    "        result = re.sub(patterns[i], replacements[i], result)\n",
    "    \n",
    "    cleaned.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9388285-3cb8-461c-a851-8e434533635e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "660dfe0d-5fe6-4765-b9f9-f2355976ff2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>video_transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2IXl4qJGrRk</td>\n",
       "      <td>A man deliberately got 217 Covid shots. Hereâ€™s...</td>\n",
       "      <td>A German man has puzzled scientists after he ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HtTalpY-J-M</td>\n",
       "      <td>COVID: German man vaccinated 217 times had no ...</td>\n",
       "      <td>a 62-year-old German man from magur claims he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jPs4_MeuX7U</td>\n",
       "      <td>New Covid vaccine study links jab to heart and...</td>\n",
       "      <td>a latest COVID19 study is providing answers t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WhiBpmH1mE4</td>\n",
       "      <td>MAN GETS 217 COVID VACCINES! ðŸ˜±ðŸ˜±ðŸ˜± THIS is What ...</td>\n",
       "      <td>a 62-year-old man who lives in Germany uh got...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LfmhYVCCGhc</td>\n",
       "      <td>Joe Rogan says tons of people &amp;quot;died sudde...</td>\n",
       "      <td>This is really painful to watch. On  the bon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                        video_title  \\\n",
       "0  2IXl4qJGrRk  A man deliberately got 217 Covid shots. Hereâ€™s...   \n",
       "1  HtTalpY-J-M  COVID: German man vaccinated 217 times had no ...   \n",
       "2  jPs4_MeuX7U  New Covid vaccine study links jab to heart and...   \n",
       "3  WhiBpmH1mE4  MAN GETS 217 COVID VACCINES! ðŸ˜±ðŸ˜±ðŸ˜± THIS is What ...   \n",
       "4  LfmhYVCCGhc  Joe Rogan says tons of people &quot;died sudde...   \n",
       "\n",
       "                                    video_transcript  \n",
       "0   A German man has puzzled scientists after he ...  \n",
       "1   a 62-year-old German man from magur claims he...  \n",
       "2   a latest COVID19 study is providing answers t...  \n",
       "3   a 62-year-old man who lives in Germany uh got...  \n",
       "4    This is really painful to watch. On  the bon...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts_df = pd.DataFrame(\n",
    "    {\n",
    "        'video_id': df[\"video_id\"].tolist(),\n",
    "        'video_title': df[\"video_title\"].tolist(),\n",
    "        'video_transcript': cleaned\n",
    "    }\n",
    ")\n",
    "transcripts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f3e677-d2e1-4960-8b28-a836c8d504b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66dce008-05d9-4bcf-87b7-1f95117da40a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcripts = transcripts_df['video_transcript'].tolist()\n",
    "stop_words = get_stop_words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8489923b-66b1-4220-886e-4d64447fd5b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_and_lemmatize(transcript):\n",
    "    tokens = [word.lower() for word in word_tokenize(transcript) if len(word) > 3]\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for token in tokens:\n",
    "        lemmas.append(wnl.lemmatize(token))\n",
    "    \n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "654733ce-5c72-46dc-a91c-5c7ad5d433fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    tokenizer=tokenize_and_lemmatize,\n",
    "    stop_words=stop_words,\n",
    "    max_df=0.85,\n",
    "    min_df=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83f3db3d-1007-48ef-9e2e-b82d8503422f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geloa\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\geloa\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'must'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actually</th>\n",
       "      <th>already</th>\n",
       "      <th>also</th>\n",
       "      <th>another</th>\n",
       "      <th>around</th>\n",
       "      <th>back</th>\n",
       "      <th>believe</th>\n",
       "      <th>better</th>\n",
       "      <th>blood</th>\n",
       "      <th>body</th>\n",
       "      <th>...</th>\n",
       "      <th>want</th>\n",
       "      <th>week</th>\n",
       "      <th>well</th>\n",
       "      <th>whether</th>\n",
       "      <th>will</th>\n",
       "      <th>work</th>\n",
       "      <th>working</th>\n",
       "      <th>world</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89 rows Ã— 135 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    actually  already  also  another  around  back  believe  better  blood  \\\n",
       "0          0        0     0        0       0     1        0       0      1   \n",
       "1          1        0     5        2       0     0        0       2      4   \n",
       "2          1        0     2        0       0     0        0       0      0   \n",
       "3          3        0     3        3       1     3        0       0      0   \n",
       "4          0        0     0        0       0     0        0       0      0   \n",
       "..       ...      ...   ...      ...     ...   ...      ...     ...    ...   \n",
       "84         6        5     7        6      10     4        2       1      1   \n",
       "85         5        2     5        1       1     1        0       1      0   \n",
       "86         2        1     2        1       0     4        0       1      0   \n",
       "87         0        0     0        0       0     0        0       0      0   \n",
       "88         1        1     1        2       0     0        0       1      0   \n",
       "\n",
       "    body  ...  want  week  well  whether  will  work  working  world  yeah  \\\n",
       "0      0  ...     0     1     0        0     0     0        0      0     0   \n",
       "1      1  ...     1     3     2        0     0     2        0      0     0   \n",
       "2      0  ...     0     0     0        0     1     0        0      0     0   \n",
       "3      0  ...     1     0     0        1     0     5        1      2     1   \n",
       "4      0  ...     0     0     0        0     4     0        0      0     0   \n",
       "..   ...  ...   ...   ...   ...      ...   ...   ...      ...    ...   ...   \n",
       "84     3  ...     7     6     6        3     5     7        3      6     3   \n",
       "85     3  ...     0     2     4        1     4     1        2      0     0   \n",
       "86     0  ...     1     2     4        1     3     1        0      1     0   \n",
       "87     0  ...     1     1     0        0     0     0        0      0     0   \n",
       "88     0  ...     5     0     5        0    10     2        0      4     1   \n",
       "\n",
       "    year  \n",
       "0      1  \n",
       "1      2  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "..   ...  \n",
       "84     5  \n",
       "85     6  \n",
       "86     1  \n",
       "87     1  \n",
       "88     1  \n",
       "\n",
       "[89 rows x 135 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = vectorizer.fit_transform(transcripts)\n",
    "matrix_df = pd.DataFrame(data=matrix.toarray(), columns = vectorizer.get_feature_names_out())\n",
    "matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d119e99e-de1f-4ec7-a504-9f6e9a57a053",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['actually', 'already', 'also', 'another', 'around', 'back',\n",
       "       'believe', 'better', 'blood', 'body', 'called', 'care', 'case',\n",
       "       'cause', 'come', 'coming', 'country', 'course', 'covid', 'covid19',\n",
       "       'data', 'day', 'different', 'doctor', 'doe', 'done', 'effect',\n",
       "       'even', 'every', 'fact', 'feel', 'find', 'first', 'five', 'found',\n",
       "       'getting', 'give', 'given', 'going', 'good', 'group', 'health',\n",
       "       'heart', 'help', 'higher', 'important', 'infection', 'information',\n",
       "       'just', 'keep', 'kind', 'know', 'last', 'life', 'like', 'little',\n",
       "       'long', 'look', 'made', 'make', 'many', 'mean', 'medical', 'might',\n",
       "       'million', 'month', 'mrna', 'much', 'need', 'never', 'news',\n",
       "       'next', 'number', 'okay', 'pandemic', 'patient', 'people',\n",
       "       'percent', 'point', 'problem', 'question', 'really', 'report',\n",
       "       'research', 'right', 'risk', 'safe', 'said', 'say', 'saying',\n",
       "       'second', 'seen', 'shot', 'show', 'side', 'small', 'someone',\n",
       "       'something', 'start', 'state', 'still', 'story', 'study', 'sure',\n",
       "       'system', 'take', 'talk', 'talking', 'tell', 'term', 'thank',\n",
       "       'thing', 'think', 'thought', 'thousand', 'three', 'time', 'today',\n",
       "       'told', 'trying', 'understand', 'vaccinated', 'vaccination',\n",
       "       'video', 'virus', 'want', 'week', 'well', 'whether', 'will',\n",
       "       'work', 'working', 'world', 'yeah', 'year'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = np.array(vectorizer.get_feature_names_out())\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b2f5730-1543-42f9-a522-6f1e961dc305",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lda = decomposition.LatentDirichletAllocation(n_components=10, max_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cd22739-b468-4bf1-a1e8-56c434d65a9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc_vectors = lda.fit_transform(matrix)\n",
    "h1 = lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07a9993e-42b6-4b2d-97e4-eb31d1688bec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.04172058e-01, 4.16796446e-03, 4.16775862e-03, 4.16689572e-03,\n",
       "        8.33664640e-02, 5.11114395e-01, 4.16886233e-03, 4.16808168e-03,\n",
       "        4.17087207e-03, 2.76336648e-01],\n",
       "       [4.87961546e-04, 4.87972837e-04, 4.87919525e-04, 1.13521218e-01,\n",
       "        4.87953393e-04, 4.87950257e-04, 4.87926588e-04, 2.84775087e-01,\n",
       "        3.26457381e-02, 5.66130272e-01],\n",
       "       [5.90743576e-01, 2.55075962e-01, 3.57282026e-03, 3.57260971e-03,\n",
       "        3.57151631e-03, 3.57284318e-03, 1.29173386e-01, 3.57255806e-03,\n",
       "        3.57236188e-03, 3.57236683e-03]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vectors[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "953d3b99-75ca-458a-9f85-8734463af16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.25482326,  0.10001946, 15.01137852,  0.1000068 , 18.97997304,\n",
       "       19.2593669 ,  0.10001169,  0.10000407,  0.10002454, 44.16704374,\n",
       "        0.10000218,  0.10000787,  0.10000615,  0.10000457,  0.10001185,\n",
       "        0.100007  ,  0.10000832,  0.10000729, 24.93964483, 23.04548946,\n",
       "        0.10000481, 12.24875066,  0.10000456, 18.62942298,  0.10000563,\n",
       "        0.10001149,  0.10000974,  0.10000538,  0.10001003,  0.10000488,\n",
       "       26.58916879,  0.10000787, 27.41253437, 60.93855093,  0.10000636,\n",
       "       20.21591019, 14.15231561,  0.10000956,  0.10000914,  0.10001287,\n",
       "        0.10000361, 44.83568776,  0.1000075 , 42.13576707,  0.10000373,\n",
       "        0.10000414,  0.10001001,  9.90572737, 65.89373249,  0.10004845,\n",
       "       15.88604088, 22.36817876,  0.10000801, 10.60929644,  0.10000837,\n",
       "        0.10000755,  0.10001032, 13.61248922,  0.1000042 , 21.68981437,\n",
       "        0.10001482,  0.10000721, 15.16802528,  0.10000836,  0.10000237,\n",
       "        0.10001079,  0.10000335,  0.10000618, 25.11296364,  0.10000581,\n",
       "       37.61847039,  0.10000597, 36.00721083,  0.10001361,  0.10000755,\n",
       "        0.10000536, 23.48347187,  0.10000597,  0.10000802,  0.10000523,\n",
       "        0.10000526,  0.10000492, 14.93615526,  0.10000422, 55.03006526,\n",
       "        0.10000285,  0.10000526,  0.10000611, 21.09883938,  0.10000404,\n",
       "        0.10001972,  0.10000337, 92.99140479,  0.10000761,  0.10000794,\n",
       "        0.10000604,  0.1000046 ,  0.10000671,  0.10000423, 43.93819369,\n",
       "       11.33411516,  0.10000693,  0.10000487,  0.1000142 ,  0.10000572,\n",
       "       26.00271759,  0.10000435,  0.10001346,  0.10001052,  0.10000802,\n",
       "        0.10000367, 38.69080378,  0.10000658,  0.10000485,  0.10000632,\n",
       "       32.96245604, 13.71183739,  0.10000409,  0.10000854,  0.10000601,\n",
       "        0.10000579,  0.10001012, 16.33086437,  0.10000912,  0.10000487,\n",
       "        0.10000803, 23.24891112, 21.48390491,  0.100008  , 64.83603246,\n",
       "        0.10000622,  0.10000273,  1.20414098,  0.10000436,  0.10000799])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e65b1a37-9100-4228-91a7-2c5296ee837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from the guide\n",
    "num_words = 15\n",
    "top_words = lambda t: [vocabulary[i] for i in np.argsort(t)[:-num_words-1:-1]]\n",
    "topic_words = ([top_words(t) for t in h1])\n",
    "topics = [' '.join(t) for t in topic_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9af40592-d1fa-4993-a9d4-3373a93b4f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shot just will five right health body state help thing news number three first feel',\n",
       " 'vaccinated data people risk group might percent just look vaccination time effect actually study side',\n",
       " 'health question many risk care study report also covid19 doe say just mrna getting different',\n",
       " 'like know think yeah getting just feel right time really doe going day kind said',\n",
       " 'blood heart research small side seen made said last another pandemic month country also trying',\n",
       " 'covid know people risk study like million data really thing right vaccination kind number okay',\n",
       " 'people long think virus thing patient different well really going work just know like infection',\n",
       " 'people know just like going will thing year want well think covid said time right',\n",
       " 'mrna virus body system make shot also covid19 called around second important work information start',\n",
       " 'people covid19 vaccination risk case cause will covid also doe video medical blood different made']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d7d297-7b59-4cae-8a73-d037b0372c6c",
   "metadata": {},
   "source": [
    "---\n",
    "### Top2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a588fad7-4148-4566-b440-680d520b9413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from top2vec import Top2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ff5bbd0-6317-40f1-9936-9f7a7c37399b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" A German man has puzzled scientists after he deliberately got more than 200 COVID19 vaccinations. 217 to be exact, over two and a half years. That's a shot every four days, roughly. CNN health reporter Meg Tyrrell is here to explain more. This guy sort of became a voluntary science experiment Yeah, of course. They called this guy a hyper vaccinated individual and researchers sort of found out about him through media reports as he was already more than 200 shots into this self experiment he was doing. And they asked if they could study him. He was actually being investigated by the authorities in Germany for potential fraud. They suspected maybe he was trying to sell vaccination cards, but no charges were ever filed against him there. Now, this guy is 62 years old, as you said. 217 COVID19 vaccinations within the span of 29 months. If you look at the pattern of some of these and we've got a bar chart here that shows the dates of some of these shots and these are not even all of them, but if you just look in this one year span, he was getting maybe one or two per month as the beginning of the year was happening as he gets into January of 20, 22. He got 48 shots within that one month period. Most days of that month he was getting two shots per day, one in each arm. He continued with a large number of shots in February, then dropped down to just six in March. Of course, scientists are wondering what the effect was on him. First of all, he didn't report any vaccine related side effects at all, which is sort of crazy to think about when you think about how you feel about getting a COVID shot the next day. Secondly, his immune response did show an increase in immune cells like antibodies and T cells, but not necessarily a better or worse immune response, even after more than 200 COVID vaccines. And finally, guys, he never got SARS-CoV-2, the virus that causes COVID. But of course, the researchers say they don't know if that's because of so many vaccines, because of just the first few vaccines or because of his behavior or something else. But this leading to quite an interesting and of one person experiment, guys. Yeah, maybe tough to answer why someone would get 217 COVID vaccinations in two years but walk us through the the saying current recommendations on how many COVID19 vaccines to get Yeah it goes without saying it is not 200. Right now in the United States the general recommendation is one updated each fall similarly to how we get the flu vaccine and we know that only a fraction of people are actually getting annual COVID shot at this point. The CDC did also just come out and recommend an additional shot now for people over the age of 65 to increase their immunity. So you're talking about a couple shots per year against COVID really at the most. This guy going way outside of those bounds. But luckily he seems to be okay exponentially, to say the least. Carol, thanks so much for the update on this really odd story. Thanks, Mike.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts = transcripts_df['video_transcript'].tolist()\n",
    "transcripts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0ba42cd-ef9b-4d4c-bbef-80d23cc00540",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "universal-sentence-encoder-multilingual is not available.\n\nTry: pip install top2vec[sentence_encoders]\n\nAlternatively try: pip install tensorflow tensorflow_hub tensorflow_text",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m Top2Vec(transcripts)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\top2vec\\Top2Vec.py:608\u001b[0m, in \u001b[0;36mTop2Vec.__init__\u001b[1;34m(self, documents, min_count, topic_merge_delta, ngram_vocab, ngram_vocab_args, embedding_model, embedding_model_path, embedding_batch_size, split_documents, document_chunker, chunk_length, max_num_chunks, chunk_overlap_ratio, chunk_len_coverage_ratio, sentencizer, speed, use_corpus_file, document_ids, keep_documents, workers, tokenizer, use_embedding_model_tokenizer, umap_args, gpu_umap, hdbscan_args, gpu_hdbscan, index_topics, verbose)\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model \u001b[38;5;241m=\u001b[39m embedding_model\n\u001b[1;32m--> 608\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_import_status()\n\u001b[0;32m    610\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPre-processing documents for training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    612\u001b[0m \u001b[38;5;66;03m# preprocess documents\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\top2vec\\Top2Vec.py:1134\u001b[0m, in \u001b[0;36mTop2Vec._check_import_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model \u001b[38;5;129;01min\u001b[39;00m use_models:\n\u001b[0;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _HAVE_TENSORFLOW:\n\u001b[1;32m-> 1134\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not available.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1135\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry: pip install top2vec[sentence_encoders]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1136\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlternatively try: pip install tensorflow tensorflow_hub tensorflow_text\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model \u001b[38;5;129;01min\u001b[39;00m sbert_models:\n\u001b[0;32m   1138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _HAVE_TORCH:\n",
      "\u001b[1;31mImportError\u001b[0m: universal-sentence-encoder-multilingual is not available.\n\nTry: pip install top2vec[sentence_encoders]\n\nAlternatively try: pip install tensorflow tensorflow_hub tensorflow_text"
     ]
    }
   ],
   "source": [
    "model = Top2Vec(transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469bcbd0-b3d9-4054-90cd-b8ff08dcc398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81f6d4e-a05a-4b7e-b241-7e11221bbd6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64cbcfd6-1be6-41b5-adbd-db4e0728a8f5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f955847-e5b4-4f5f-b088-3a0a0c2289d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
