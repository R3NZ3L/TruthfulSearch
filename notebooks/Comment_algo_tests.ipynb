{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import translators as ts\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>Loved the calmness of Manilla.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>After this pandemic I think every country shou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>manila looks beautifull with less people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>The lockdown makes the city look like a place ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>India also same</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                            comment\n",
       "0  aLZ85hb4wjE                     Loved the calmness of Manilla.\n",
       "1  aLZ85hb4wjE  After this pandemic I think every country shou...\n",
       "2  aLZ85hb4wjE           manila looks beautifull with less people\n",
       "3  aLZ85hb4wjE  The lockdown makes the city look like a place ...\n",
       "4  aLZ85hb4wjE                                    India also same"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../datasets/covid_philippines/covid_philippines_comments.csv\").drop(\"Unnamed: 0\", axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_comments = {}\n",
    "translated_comments[\"video_id\"] = {}\n",
    "translated_comments[\"comment\"] = {}\n",
    "video_id_list = df[\"video_id\"].to_list()\n",
    "comments_list = df[\"comment\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating...:   0%|          | 0/1216 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1216/1216 [07:06<00:00,  2.85it/s]\n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(total=len(video_id_list))\n",
    "pbar.set_description(\"Translating...\")\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    if comments_list[i] != None:\n",
    "        new_comment = comments_list[i]\n",
    "        try:\n",
    "            lang = detect(comments_list[i]) #added langdetect since it errors if there are many entries to translate, so now it will ontly tranlate if comment not english\n",
    "            if lang != 'en':\n",
    "                new_comment = ts.translate_text(comments_list[i], 'google', to_language = 'en')\n",
    "                \n",
    "        except:\n",
    "            # No change; get same comment from list\n",
    "            pass\n",
    "            \n",
    "        finally:\n",
    "            translated_comments[\"video_id\"][i] = video_id_list[i]\n",
    "            translated_comments[\"comment\"][i] = new_comment\n",
    "            pbar.update(1)\n",
    "\n",
    "translated_df = pd.DataFrame.from_dict(translated_comments)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>Loved the calmness of Manilla.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>After this pandemic I think every country shou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>manila looks beautifull with less people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>The lockdown makes the city look like a place ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>India also same</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>This covid will be a never-ending fuckery as l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>A new variant is inevitable.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>The man that should resign from his office is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>Is that the people who got been vaccinated.,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>The toxic of comments section ðŸ˜„ \\r\\n During th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1216 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         video_id                                            comment\n",
       "0     aLZ85hb4wjE                     Loved the calmness of Manilla.\n",
       "1     aLZ85hb4wjE  After this pandemic I think every country shou...\n",
       "2     aLZ85hb4wjE           manila looks beautifull with less people\n",
       "3     aLZ85hb4wjE  The lockdown makes the city look like a place ...\n",
       "4     aLZ85hb4wjE                                    India also same\n",
       "...           ...                                                ...\n",
       "1211  5DvMPgoKZmM  This covid will be a never-ending fuckery as l...\n",
       "1212  5DvMPgoKZmM                       A new variant is inevitable.\n",
       "1213  5DvMPgoKZmM  The man that should resign from his office is ...\n",
       "1214  5DvMPgoKZmM       Is that the people who got been vaccinated.,\n",
       "1215  5DvMPgoKZmM  The toxic of comments section ðŸ˜„ \\r\\n During th...\n",
       "\n",
       "[1216 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(video_id_list, comments_list):\n",
    "    translated_comments = {}\n",
    "    translated_comments[\"video_id\"] = {}\n",
    "    translated_comments[\"comment\"] = {}\n",
    "    \n",
    "    pbar = tqdm(total=len(video_id_list))\n",
    "    pbar.set_description(\"Translating...\")\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        if comments_list[i] != None:\n",
    "            new_comment = comments_list[i]\n",
    "            try:\n",
    "                lang = detect(comments_list[i]) #added langdetect since it errors if there are many entries to translate, so now it will ontly tranlate if comment not english\n",
    "                if lang != 'en':\n",
    "                    new_comment = ts.translate_text(comments_list[i], 'google', to_language = 'en')\n",
    "\n",
    "            except:\n",
    "                # No change; get same comment from list\n",
    "                pass\n",
    "\n",
    "            finally:\n",
    "                translated_comments[\"video_id\"][i] = video_id_list[i]\n",
    "                translated_comments[\"comment\"][i] = new_comment\n",
    "                pbar.update(1)\n",
    "\n",
    "    translated_df = pd.DataFrame.from_dict(translated_comments)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DETECT SPAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0:Not Spam\n",
    "- 1:Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COMMENT_ID    1508\n",
       "AUTHOR        1508\n",
       "DATE          1508\n",
       "CONTENT       1508\n",
       "CLASS         1508\n",
       "dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset source: https://archive.ics.uci.edu/dataset/380/youtube+spam+collection\n",
    "\n",
    "psy_model_df = pd.read_csv(\"../datasets/model_train/Youtube01-Psy.csv\")\n",
    "lmfao_model_df = pd.read_csv(\"../datasets/model_train/Youtube03-LMFAO.csv\")\n",
    "kp_model_df = pd.read_csv(\"../datasets/model_train/Youtube02-KatyPerry.csv\")\n",
    "shakira_df = pd.read_csv(\"../datasets/model_train/Youtube05-Shakira.csv\")\n",
    "model_df = pd.concat([psy_model_df,lmfao_model_df,kp_model_df,shakira_df])\n",
    "model_df.reset_index(inplace=True, drop=True) \n",
    "model_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07T06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
       "      <td>adam riyati</td>\n",
       "      <td>2013-11-07T12:37:15</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>2013-11-08T17:34:21</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z13jhp0bxqncu512g22wvzkasxmvvzjaz04</td>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>2013-11-09T08:28:43</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ï»¿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z13fwbwp1oujthgqj04chlngpvzmtt3r3dw</td>\n",
       "      <td>GsMega</td>\n",
       "      <td>2013-11-10T16:05:38</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .ï»¿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>_2viQ_Qnc6_1Hq9MGlefkBIszt9rYD3S_CozADvMhQ4</td>\n",
       "      <td>Dinova Sharon</td>\n",
       "      <td>2013-07-13T14:44:00.700000</td>\n",
       "      <td>well done shakira</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>_2viQ_Qnc6-bMSjqyL1NKj57ROicCSJV5SwTrw-RFFA</td>\n",
       "      <td>Katie Mettam</td>\n",
       "      <td>2013-07-13T13:27:39.441000</td>\n",
       "      <td>I love this song because we sing it at Camp al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>_2viQ_Qnc6-pY-1yR6K2FhmC5i48-WuNx5CumlHLDAI</td>\n",
       "      <td>Sabina Pearson-Smith</td>\n",
       "      <td>2013-07-13T13:14:30.021000</td>\n",
       "      <td>I love this song for two reasons: 1.it is abou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>_2viQ_Qnc6_yBt8UGMWyg3vh0PulTqcqyQtdE7d4Fl0</td>\n",
       "      <td>Aishlin Maciel</td>\n",
       "      <td>2013-07-13T11:17:52.308000</td>\n",
       "      <td>Shakira u are so wiredo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507</th>\n",
       "      <td>_2viQ_Qnc685RPw1aSa1tfrIuHXRvAQ2rPT9R06KTqA</td>\n",
       "      <td>Latin Bosch</td>\n",
       "      <td>2013-07-12T22:33:27.916000</td>\n",
       "      <td>Shakira is the best dancer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1362 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       COMMENT_ID                AUTHOR  \\\n",
       "0     LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU             Julius NM   \n",
       "1     LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A           adam riyati   \n",
       "2     LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8      Evgeny Murashkin   \n",
       "3             z13jhp0bxqncu512g22wvzkasxmvvzjaz04       ElNino Melendez   \n",
       "4             z13fwbwp1oujthgqj04chlngpvzmtt3r3dw                GsMega   \n",
       "...                                           ...                   ...   \n",
       "1502  _2viQ_Qnc6_1Hq9MGlefkBIszt9rYD3S_CozADvMhQ4         Dinova Sharon   \n",
       "1503  _2viQ_Qnc6-bMSjqyL1NKj57ROicCSJV5SwTrw-RFFA          Katie Mettam   \n",
       "1504  _2viQ_Qnc6-pY-1yR6K2FhmC5i48-WuNx5CumlHLDAI  Sabina Pearson-Smith   \n",
       "1506  _2viQ_Qnc6_yBt8UGMWyg3vh0PulTqcqyQtdE7d4Fl0        Aishlin Maciel   \n",
       "1507  _2viQ_Qnc685RPw1aSa1tfrIuHXRvAQ2rPT9R06KTqA           Latin Bosch   \n",
       "\n",
       "                            DATE  \\\n",
       "0            2013-11-07T06:20:48   \n",
       "1            2013-11-07T12:37:15   \n",
       "2            2013-11-08T17:34:21   \n",
       "3            2013-11-09T08:28:43   \n",
       "4            2013-11-10T16:05:38   \n",
       "...                          ...   \n",
       "1502  2013-07-13T14:44:00.700000   \n",
       "1503  2013-07-13T13:27:39.441000   \n",
       "1504  2013-07-13T13:14:30.021000   \n",
       "1506  2013-07-13T11:17:52.308000   \n",
       "1507  2013-07-12T22:33:27.916000   \n",
       "\n",
       "                                                CONTENT  CLASS  \n",
       "0     Huh, anyway check out this you[tube] channel: ...      1  \n",
       "1     Hey guys check out my new channel and our firs...      1  \n",
       "2                just for test I have to say murdev.com      1  \n",
       "3      me shaking my sexy ass on my channel enjoy ^_^ ï»¿      1  \n",
       "4               watch?v=vtaRGgvGtWQ   Check this out .ï»¿      1  \n",
       "...                                                 ...    ...  \n",
       "1502                                  well done shakira      0  \n",
       "1503  I love this song because we sing it at Camp al...      0  \n",
       "1504  I love this song for two reasons: 1.it is abou...      0  \n",
       "1506                            Shakira u are so wiredo      0  \n",
       "1507                         Shakira is the best dancer      0  \n",
       "\n",
       "[1362 rows x 5 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df.drop_duplicates(subset=\"CONTENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COMMENT_ID    1508\n",
       "AUTHOR        1508\n",
       "DATE          1508\n",
       "CONTENT       1508\n",
       "CLASS         1508\n",
       "dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Melanie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "stop_words = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Huh, anyway check out this you[tube] channel: ...\n",
       "1       Hey guys check out my new channel and our firs...\n",
       "2                  just for test I have to say murdev.com\n",
       "3        me shaking my sexy ass on my channel enjoy ^_^ ï»¿\n",
       "4                 watch?v=vtaRGgvGtWQ   Check this out .ï»¿\n",
       "                              ...                        \n",
       "1503    I love this song because we sing it at Camp al...\n",
       "1504    I love this song for two reasons: 1.it is abou...\n",
       "1505                                                  wow\n",
       "1506                              Shakira u are so wiredo\n",
       "1507                           Shakira is the best dancer\n",
       "Name: CONTENT, Length: 1508, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df[\"CONTENT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df['CONTENT'] = model_df['CONTENT'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "\n",
    "ps = PorterStemmer() \n",
    "for w in model_df[\"CONTENT\"]: \n",
    "    model_df['CONTENT'] = model_df[\"CONTENT\"].apply(ps.stem) \n",
    "    model_df['CONTENT'] = model_df[\"CONTENT\"].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         huh, anyway check you[tube] channel: kobyoshi02\n",
       "1       hey guys check new channel first vid us monkey...\n",
       "2                                     test say murdev.com\n",
       "3                    shaking sexy ass channel enjoy ^_^ ï»¿\n",
       "4                            watch?v=vtarggvgtwq check .ï»¿\n",
       "                              ...                        \n",
       "1503                           love song sing camp time!!\n",
       "1504    love song two reasons: 1.it africa 2.i born be...\n",
       "1505                                                  wow\n",
       "1506                                     shakira u wiredo\n",
       "1507                                    shakira best danc\n",
       "Name: CONTENT, Length: 1508, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df[\"CONTENT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=model_df[\"CONTENT\"]\n",
    "y=model_df[\"CLASS\"]\n",
    "\n",
    "cv= CountVectorizer()\n",
    "\n",
    "x=cv.fit_transform(x)\n",
    "\n",
    "x_train, x_test, y_train, y_test  = train_test_split(x,y,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9699381078691424\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       544\n",
      "           1       0.97      0.97      0.97       587\n",
      "\n",
      "    accuracy                           0.97      1131\n",
      "   macro avg       0.97      0.97      0.97      1131\n",
      "weighted avg       0.97      0.97      0.97      1131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spam_nb = MultinomialNB()\n",
    "spam_nb.fit(x_train,y_train)\n",
    "\n",
    "predictions=spam_nb.predict(x_train)\n",
    "accuracy = accuracy_score(y_train, predictions)\n",
    "print(f\"Train Accuracy: {accuracy}\")\n",
    "\n",
    "class_report = classification_report(y_train, predictions)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9230769230769231\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93       204\n",
      "           1       0.93      0.90      0.91       173\n",
      "\n",
      "    accuracy                           0.92       377\n",
      "   macro avg       0.92      0.92      0.92       377\n",
      "weighted avg       0.92      0.92      0.92       377\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions=spam_nb.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "class_report = classification_report(y_test, predictions)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test with another dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>z12rwfnyyrbsefonb232i5ehdxzkjzjs2</td>\n",
       "      <td>Lisa Wellas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+447935454150 lovely girl talk to me xxxï»¿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>z130wpnwwnyuetxcn23xf5k5ynmkdpjrj04</td>\n",
       "      <td>jason graham</td>\n",
       "      <td>2015-05-29T02:26:10.652000</td>\n",
       "      <td>I always end up coming back to this song&lt;br /&gt;ï»¿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>z13vsfqirtavjvu0t22ezrgzyorwxhpf3</td>\n",
       "      <td>Ajkal Khan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>my sister just received over 6,500 new &lt;a rel=...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z12wjzc4eprnvja4304cgbbizuved35wxcs</td>\n",
       "      <td>Dakota Taylor</td>\n",
       "      <td>2015-05-29T02:13:07.810000</td>\n",
       "      <td>Coolï»¿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z13xjfr42z3uxdz2223gx5rrzs3dt5hna</td>\n",
       "      <td>Jihad Naser</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hello I&amp;#39;am from Palastineï»¿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            COMMENT_ID         AUTHOR  \\\n",
       "0    z12rwfnyyrbsefonb232i5ehdxzkjzjs2    Lisa Wellas   \n",
       "1  z130wpnwwnyuetxcn23xf5k5ynmkdpjrj04   jason graham   \n",
       "2    z13vsfqirtavjvu0t22ezrgzyorwxhpf3     Ajkal Khan   \n",
       "3  z12wjzc4eprnvja4304cgbbizuved35wxcs  Dakota Taylor   \n",
       "4    z13xjfr42z3uxdz2223gx5rrzs3dt5hna    Jihad Naser   \n",
       "\n",
       "                         DATE  \\\n",
       "0                         NaN   \n",
       "1  2015-05-29T02:26:10.652000   \n",
       "2                         NaN   \n",
       "3  2015-05-29T02:13:07.810000   \n",
       "4                         NaN   \n",
       "\n",
       "                                             CONTENT  CLASS  \n",
       "0          +447935454150 lovely girl talk to me xxxï»¿      1  \n",
       "1    I always end up coming back to this song<br />ï»¿      0  \n",
       "2  my sister just received over 6,500 new <a rel=...      1  \n",
       "3                                              Coolï»¿      0  \n",
       "4                     Hello I&#39;am from Palastineï»¿      1  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset source: https://archive.ics.uci.edu/dataset/380/youtube+spam+collection\n",
    "\n",
    "em_model_df = pd.read_csv(\"../datasets/model_train/Youtube04-Eminem.csv\")\n",
    "em_model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=em_model_df[\"CONTENT\"]\n",
    "y=em_model_df[\"CLASS\"]\n",
    "\n",
    "x=cv.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8325892857142857\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.69      0.79       203\n",
      "           1       0.79      0.95      0.86       245\n",
      "\n",
      "    accuracy                           0.83       448\n",
      "   macro avg       0.85      0.82      0.83       448\n",
      "weighted avg       0.85      0.83      0.83       448\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions=spam_nb.predict(x)\n",
    "accuracy = accuracy_score(y, predictions)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "class_report = classification_report(y, predictions)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try model on translated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_translated_df = translated_df.copy()#in case we want to preserve the original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_df[\"comment\"] = translated_df[\"comment\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "\n",
    "ps = PorterStemmer() \n",
    "for w in translated_df[\"comment\"]: \n",
    "    translated_df[\"comment\"] = translated_df[\"comment\"].apply(ps.stem) \n",
    "    translated_df[\"comment\"] = translated_df[\"comment\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>loved calmness manilla.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>after pandemic i think every country lockdown ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>manila looks beautifull less peopl</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>the lockdown makes city look like place i want...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>india also</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                            comment  spam\n",
       "0  aLZ85hb4wjE                            loved calmness manilla.     0\n",
       "1  aLZ85hb4wjE  after pandemic i think every country lockdown ...     0\n",
       "2  aLZ85hb4wjE                 manila looks beautifull less peopl     0\n",
       "3  aLZ85hb4wjE  the lockdown makes city look like place i want...     0\n",
       "4  aLZ85hb4wjE                                         india also     1"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed = cv.transform(translated_df[\"comment\"])\n",
    "\n",
    "translated_df[\"spam\"]=spam_nb.predict(transformed)\n",
    "translated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "video_id    563\n",
       "comment     563\n",
       "spam        563\n",
       "dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_df[translated_df[\"spam\"]==1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "video_id    653\n",
       "comment     653\n",
       "spam        653\n",
       "dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_df[translated_df[\"spam\"]==0].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seems like there are too many tagged as spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>india also</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>the president promised he'll best ease traffic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>2020 year nature fight back reduce human emiss...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sYI97jv-pZg</td>\n",
       "      <td>ala n covid tngina</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3YFpjgIQqEo</td>\n",
       "      <td>this cold make worse make money</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>Wjj__vIdew0</td>\n",
       "      <td>nest</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>please, option get vaccinated, it. it still po...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>a new variant inevitable.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>the man resign office nonetheless present head...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>the toxic comments section ðŸ˜„ during time, poor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>563 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         video_id                                            comment  spam\n",
       "4     aLZ85hb4wjE                                         india also     1\n",
       "5     aLZ85hb4wjE  the president promised he'll best ease traffic...     1\n",
       "9     aLZ85hb4wjE  2020 year nature fight back reduce human emiss...     1\n",
       "11    sYI97jv-pZg                                 ala n covid tngina     1\n",
       "12    3YFpjgIQqEo                    this cold make worse make money     1\n",
       "...           ...                                                ...   ...\n",
       "1204  Wjj__vIdew0                                               nest     1\n",
       "1210  5DvMPgoKZmM  please, option get vaccinated, it. it still po...     1\n",
       "1212  5DvMPgoKZmM                          a new variant inevitable.     1\n",
       "1213  5DvMPgoKZmM  the man resign office nonetheless present head...     1\n",
       "1215  5DvMPgoKZmM  the toxic comments section ðŸ˜„ during time, poor...     1\n",
       "\n",
       "[563 rows x 3 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_df[translated_df[\"spam\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>loved calmness manilla.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>after pandemic i think every country lockdown ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>manila looks beautifull less peopl</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>the lockdown makes city look like place i want...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>i went viewing spot antipolo, beautiful, almos...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>it's almost 2 years this, many people honestly...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>\"cases\" mean absolutely nothing whatsoever, re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>honestly, i hope things least get slightly bet...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>this covid never-ending fuckery long media kee...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>is people got vaccinated.,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>653 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         video_id                                            comment  spam\n",
       "0     aLZ85hb4wjE                            loved calmness manilla.     0\n",
       "1     aLZ85hb4wjE  after pandemic i think every country lockdown ...     0\n",
       "2     aLZ85hb4wjE                 manila looks beautifull less peopl     0\n",
       "3     aLZ85hb4wjE  the lockdown makes city look like place i want...     0\n",
       "6     aLZ85hb4wjE  i went viewing spot antipolo, beautiful, almos...     0\n",
       "...           ...                                                ...   ...\n",
       "1207  5DvMPgoKZmM  it's almost 2 years this, many people honestly...     0\n",
       "1208  5DvMPgoKZmM  \"cases\" mean absolutely nothing whatsoever, re...     0\n",
       "1209  5DvMPgoKZmM  honestly, i hope things least get slightly bet...     0\n",
       "1211  5DvMPgoKZmM  this covid never-ending fuckery long media kee...     0\n",
       "1214  5DvMPgoKZmM                         is people got vaccinated.,     0\n",
       "\n",
       "[653 rows x 3 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_df[translated_df[\"spam\"]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_df[translated_df[\"spam\"]==1].to_csv(\"check_spam.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_df[translated_df[\"spam\"]==0].to_csv(\"check_not_spam.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different dataset from a different source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Time</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Reply Count</th>\n",
       "      <th>Spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Taofeekat</td>\n",
       "      <td>&amp;lt;????i make my first million investing in f...</td>\n",
       "      <td>2022-09-28T02:08:55Z</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Angelina Jordan</td>\n",
       "      <td>&amp;lt;?l will forever be indebted to you I will ...</td>\n",
       "      <td>2022-09-23T05:26:48Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fernandez Joe</td>\n",
       "      <td>&lt;b&gt;????I recommend a professional forex/Bitcoi...</td>\n",
       "      <td>2022-09-20T12:56:30Z</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jessica  Billy</td>\n",
       "      <td>I think Iâ€™m blessed because if not I wouldnâ€™t ...</td>\n",
       "      <td>2022-09-17T20:20:24Z</td>\n",
       "      <td>21</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Allison Zar</td>\n",
       "      <td>&lt;b&gt;I recommend a professional  broker to you g...</td>\n",
       "      <td>2022-09-05T09:19:30Z</td>\n",
       "      <td>19</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name                                            Comment  \\\n",
       "0        Taofeekat  &lt;????i make my first million investing in f...   \n",
       "1  Angelina Jordan  &lt;?l will forever be indebted to you I will ...   \n",
       "2    Fernandez Joe  <b>????I recommend a professional forex/Bitcoi...   \n",
       "3   Jessica  Billy  I think Iâ€™m blessed because if not I wouldnâ€™t ...   \n",
       "4      Allison Zar  <b>I recommend a professional  broker to you g...   \n",
       "\n",
       "                   Time  Likes  Reply Count  Spam  \n",
       "0  2022-09-28T02:08:55Z     30           30     1  \n",
       "1  2022-09-23T05:26:48Z      0            0     1  \n",
       "2  2022-09-20T12:56:30Z      5            2     1  \n",
       "3  2022-09-17T20:20:24Z     21           34     1  \n",
       "4  2022-09-05T09:19:30Z     19           27     1  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#source: https://www.kaggle.com/datasets/madhuragl/5000-youtube-spamnot-spam-dataset/data\n",
    "comments_5k_df = pd.read_csv(\"../datasets/model_train/5000 YT comments.csv\",encoding='cp1252')\n",
    "comments_5k_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name           5000\n",
       "Comment        5000\n",
       "Time           5000\n",
       "Likes          5000\n",
       "Reply Count    5000\n",
       "Spam           5000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_5k_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Time</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Reply Count</th>\n",
       "      <th>Spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Taofeekat</td>\n",
       "      <td>&amp;lt;????i make my first million investing in f...</td>\n",
       "      <td>2022-09-28T02:08:55Z</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Angelina Jordan</td>\n",
       "      <td>&amp;lt;?l will forever be indebted to you I will ...</td>\n",
       "      <td>2022-09-23T05:26:48Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fernandez Joe</td>\n",
       "      <td>&lt;b&gt;????I recommend a professional forex/Bitcoi...</td>\n",
       "      <td>2022-09-20T12:56:30Z</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jessica  Billy</td>\n",
       "      <td>I think Iâ€™m blessed because if not I wouldnâ€™t ...</td>\n",
       "      <td>2022-09-17T20:20:24Z</td>\n",
       "      <td>21</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Allison Zar</td>\n",
       "      <td>&lt;b&gt;I recommend a professional  broker to you g...</td>\n",
       "      <td>2022-09-05T09:19:30Z</td>\n",
       "      <td>19</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>Anjan Das</td>\n",
       "      <td>She is so beautiful!</td>\n",
       "      <td>2020-06-05T04:18:26Z</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>Humza Navaid</td>\n",
       "      <td>3 seconds in and I want to marry her. I am goi...</td>\n",
       "      <td>2020-06-04T21:03:14Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Aadil Ranesh</td>\n",
       "      <td>She talks a lot like Tanmay Bakshi</td>\n",
       "      <td>2020-06-03T17:29:04Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Fuzail Ahmad</td>\n",
       "      <td>Why does her face look like a bad deepfake?</td>\n",
       "      <td>2020-06-03T11:17:48Z</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>kcmn00</td>\n",
       "      <td>So, will bankers lose their jobs?</td>\n",
       "      <td>2020-06-03T08:11:15Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4673 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Name                                            Comment  \\\n",
       "0           Taofeekat  &lt;????i make my first million investing in f...   \n",
       "1     Angelina Jordan  &lt;?l will forever be indebted to you I will ...   \n",
       "2       Fernandez Joe  <b>????I recommend a professional forex/Bitcoi...   \n",
       "3      Jessica  Billy  I think Iâ€™m blessed because if not I wouldnâ€™t ...   \n",
       "4         Allison Zar  <b>I recommend a professional  broker to you g...   \n",
       "...               ...                                                ...   \n",
       "4995        Anjan Das                               She is so beautiful!   \n",
       "4996     Humza Navaid  3 seconds in and I want to marry her. I am goi...   \n",
       "4997     Aadil Ranesh                 She talks a lot like Tanmay Bakshi   \n",
       "4998     Fuzail Ahmad        Why does her face look like a bad deepfake?   \n",
       "4999           kcmn00                  So, will bankers lose their jobs?   \n",
       "\n",
       "                      Time  Likes  Reply Count  Spam  \n",
       "0     2022-09-28T02:08:55Z     30           30     1  \n",
       "1     2022-09-23T05:26:48Z      0            0     1  \n",
       "2     2022-09-20T12:56:30Z      5            2     1  \n",
       "3     2022-09-17T20:20:24Z     21           34     1  \n",
       "4     2022-09-05T09:19:30Z     19           27     1  \n",
       "...                    ...    ...          ...   ...  \n",
       "4995  2020-06-05T04:18:26Z      5            0     0  \n",
       "4996  2020-06-04T21:03:14Z      0            0     0  \n",
       "4997  2020-06-03T17:29:04Z      0            0     0  \n",
       "4998  2020-06-03T11:17:48Z      1            0     0  \n",
       "4999  2020-06-03T08:11:15Z      0            0     0  \n",
       "\n",
       "[4673 rows x 6 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_5k_df.drop_duplicates(subset=\"Comment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name           5000\n",
       "Comment        5000\n",
       "Time           5000\n",
       "Likes          5000\n",
       "Reply Count    5000\n",
       "Spam           5000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_5k_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_5k_df['Comment'] = comments_5k_df['Comment'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "ps = PorterStemmer() \n",
    "for w in comments_5k_df['Comment']: \n",
    "    comments_5k_df['Comment'] = comments_5k_df['Comment'].apply(ps.stem) \n",
    "    comments_5k_df['Comment'] = comments_5k_df['Comment'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=comments_5k_df[\"Comment\"]\n",
    "y=comments_5k_df[\"Spam\"]\n",
    "\n",
    "cv= CountVectorizer()\n",
    "\n",
    "x=cv.fit_transform(x)\n",
    "\n",
    "x_train, x_test, y_train, y_test  = train_test_split(x,y,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9301333333333334\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93      1876\n",
      "           1       0.92      0.94      0.93      1874\n",
      "\n",
      "    accuracy                           0.93      3750\n",
      "   macro avg       0.93      0.93      0.93      3750\n",
      "weighted avg       0.93      0.93      0.93      3750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spam_nb = MultinomialNB()\n",
    "spam_nb.fit(x_train,y_train)\n",
    "\n",
    "predictions=spam_nb.predict(x_train)\n",
    "accuracy = accuracy_score(y_train, predictions)\n",
    "print(f\"Train Accuracy: {accuracy}\")\n",
    "\n",
    "class_report = classification_report(y_train, predictions)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8768\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.86      0.87       624\n",
      "           1       0.86      0.90      0.88       626\n",
      "\n",
      "    accuracy                           0.88      1250\n",
      "   macro avg       0.88      0.88      0.88      1250\n",
      "weighted avg       0.88      0.88      0.88      1250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions=spam_nb.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "class_report = classification_report(y_test, predictions)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_df = og_translated_df.copy()#in case we want to preserve the original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_df[\"comment\"] = translated_df[\"comment\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "\n",
    "ps = PorterStemmer() \n",
    "for w in translated_df[\"comment\"]: \n",
    "    translated_df[\"comment\"] = translated_df[\"comment\"].apply(ps.stem) \n",
    "    translated_df[\"comment\"] = translated_df[\"comment\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>loved calmness manilla.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>after pandemic i think every country lockdown ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>manila looks beautifull less peopl</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>the lockdown makes city look like place i want...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>india also</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                            comment  spam\n",
       "0  aLZ85hb4wjE                            loved calmness manilla.     0\n",
       "1  aLZ85hb4wjE  after pandemic i think every country lockdown ...     1\n",
       "2  aLZ85hb4wjE                 manila looks beautifull less peopl     0\n",
       "3  aLZ85hb4wjE  the lockdown makes city look like place i want...     0\n",
       "4  aLZ85hb4wjE                                         india also     1"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed = cv.transform(translated_df[\"comment\"])\n",
    "\n",
    "translated_df[\"spam\"]=spam_nb.predict(transformed)\n",
    "translated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "video_id    670\n",
       "comment     670\n",
       "spam        670\n",
       "dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_df[translated_df[\"spam\"]==1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "video_id    546\n",
       "comment     546\n",
       "spam        546\n",
       "dtype: int64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_df[translated_df[\"spam\"]==0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>after pandemic i think every country lockdown ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>india also</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>i went viewing spot antipolo, beautiful, almos...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sYI97jv-pZg</td>\n",
       "      <td>cough cold season weather cold .... covid amg ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sYI97jv-pZg</td>\n",
       "      <td>ala n covid tngina</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>please, option get vaccinated, it. it still po...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>this covid never-ending fuckery long media kee...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>a new variant inevitable.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>is people got vaccinated.,</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>the toxic comments section ðŸ˜„ during time, poor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>670 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         video_id                                            comment  spam\n",
       "1     aLZ85hb4wjE  after pandemic i think every country lockdown ...     1\n",
       "4     aLZ85hb4wjE                                         india also     1\n",
       "6     aLZ85hb4wjE  i went viewing spot antipolo, beautiful, almos...     1\n",
       "10    sYI97jv-pZg  cough cold season weather cold .... covid amg ...     1\n",
       "11    sYI97jv-pZg                                 ala n covid tngina     1\n",
       "...           ...                                                ...   ...\n",
       "1210  5DvMPgoKZmM  please, option get vaccinated, it. it still po...     1\n",
       "1211  5DvMPgoKZmM  this covid never-ending fuckery long media kee...     1\n",
       "1212  5DvMPgoKZmM                          a new variant inevitable.     1\n",
       "1214  5DvMPgoKZmM                         is people got vaccinated.,     1\n",
       "1215  5DvMPgoKZmM  the toxic comments section ðŸ˜„ during time, poor...     1\n",
       "\n",
       "[670 rows x 3 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_df[translated_df[\"spam\"]==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import re\n",
    "from scipy.special import softmax\n",
    "\n",
    "def roberta_sentiment_scores(list_comments):\n",
    "    roberta = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "    labels = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(roberta)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(roberta)\n",
    "\n",
    "    for comments in list_comments:\n",
    "        comment_words = []\n",
    "        comments = comments.replace(\"\\n\", \" \")\n",
    "        comments = comments.replace(\"\\xa0\", \" \")\n",
    "        comments = comments.replace(\"?\", \" \")\n",
    "        comments = comments.replace(\":\", \" \")\n",
    "        comments = comments.replace(\";\", \" \")\n",
    "        comments = comments.replace(\";\", \" \")\n",
    "        comments = re.sub(r\"\\s+\", ' ', comments) \n",
    "        print(comments)\n",
    "        for word in comments.split(' '):\n",
    "            if word.startswith('@') and len(word) > 1:\n",
    "               word = '@user'\n",
    "        \n",
    "            elif word.startswith('http'):\n",
    "                word = \"http\"\n",
    "            comment_words.append(word)\n",
    "\n",
    "        comment_procs = \" \".join(comment_words)\n",
    "\n",
    "        encoded = tokenizer(comment_procs, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "        print(encoded)\n",
    "        output = model(**encoded)\n",
    "\n",
    "        scores = output[0][0].detach().numpy()\n",
    "\n",
    "        scores = softmax(scores)\n",
    "\n",
    "        for i in range(len(scores)):\n",
    "\n",
    "            l = labels[i]\n",
    "            s = scores[i]\n",
    "            print(l, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_sentiment_scores(translated_df[\"comment\"][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST TEXTBLOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def textblob_sentiment_scores(list_comments):\n",
    "    list_sentiment={}\n",
    "    for comment in list_comments:\n",
    "        testimonial = TextBlob(comment)\n",
    "        if (testimonial.sentiment.polarity > 0):\n",
    "            print(\"positive\", testimonial.sentiment.polarity)\n",
    "        elif (testimonial.sentiment.polarity < 0):\n",
    "            print(\"negative\", testimonial.sentiment.polarity)\n",
    "        else:\n",
    "            print(\"neutral\", testimonial.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textblob_sentiment_scores(translated_df[\"comment\"][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST STANZA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "def stanza_sentiment_scores(list_comments):\n",
    "    nlp = stanza.Pipeline('en', processors='tokenize,sentiment', tokenize_no_ssplit=True)\n",
    "\n",
    "    for comment in list_comments:\n",
    "        doc = nlp(comment.replace(\"\\n\", \" \"))\n",
    "        print(comment)\n",
    "    #doc.sentences[0].print_dependencies()\n",
    "        for i, sentence in enumerate(doc.sentences):\n",
    "            print(\"%d -> %d\" % (i, sentence.sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza_sentiment_scores(translated_df[\"comment\"][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "def vader_sentiment_scores (list_comments):\n",
    "    for sentence in list_comments:\n",
    "        sid_obj = SentimentIntensityAnalyzer()\n",
    "        sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "        print(sentence)\n",
    "        print(\"Overall sentiment dictionary is : \", sentiment_dict)\n",
    "        print(\"sentence was rated as \", sentiment_dict['neg']*100, \"% Negative\")\n",
    "        print(\"sentence was rated as \", sentiment_dict['neu']*100, \"% Neutral\")\n",
    "        print(\"sentence was rated as \", sentiment_dict['pos']*100, \"% Positive\")\n",
    " \n",
    "        print(\"Sentence Overall Rated As\", end = \" \")\n",
    " \n",
    "        # decide sentiment as positive, negative and neutral\n",
    "        if sentiment_dict['compound'] >= 0.05 :\n",
    "            print(\"Positive\")\n",
    " \n",
    "        elif sentiment_dict['compound'] <= - 0.05 :\n",
    "            print(\"Negative\")\n",
    " \n",
    "        else :\n",
    "            print(\"Neutral\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_sentiment_scores (translated_df[\"comment\"][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Sentiment sentiment for each video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEXTBLOB APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textblob_sentiment_scores(list_comments):\n",
    "    list_sentiment={}\n",
    "    for comment in list_comments:\n",
    "        testimonial = TextBlob(comment)\n",
    "        if (testimonial.sentiment.polarity > 0):\n",
    "            print(\"positive\", testimonial.sentiment.polarity)\n",
    "        elif (testimonial.sentiment.polarity < 0):\n",
    "            print(\"negative\", testimonial.sentiment.polarity)\n",
    "        else:\n",
    "            print(\"neutral\", testimonial.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_df[\"video_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_vid_textblob_polarity (df):\n",
    "    video_polarity={}\n",
    "    for video in df[\"video_id\"].unique():\n",
    "        sum_polarity=0\n",
    "        for comment in df.loc[df[\"video_id\"]==video][\"comment\"]:\n",
    "            sum_polarity+=(TextBlob(comment)).sentiment.polarity\n",
    "        video_polarity[video] = sum_polarity/df.loc[df[\"video_id\"]==video][\"comment\"].count()\n",
    "    ''' if (testimonial.sentiment.polarity > 0):\n",
    "            print(\"positive\", testimonial.sentiment.subjectivity)\n",
    "        elif (testimonial.sentiment.polarity < 0):\n",
    "            print(\"negative\", testimonial.sentiment.subjectivity)\n",
    "        else:\n",
    "            print(\"neutral\", testimonial.sentiment.subjectivity)'''\n",
    "    return video_polarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_vid_textblob_polarity(translated_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VADER APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_vid_vader_polarity (df):\n",
    "    polarity_scores={}\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "\n",
    "    for video in df[\"video_id\"].unique():\n",
    "        sum_neg =0\n",
    "        sum_pos = 0\n",
    "        sum_neu =0\n",
    "        sum_compound= 0\n",
    "        scores={}\n",
    "        for sentence in df.loc[df[\"video_id\"]==video][\"comment\"]:\n",
    "\n",
    "            sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "            sum_neg += sentiment_dict['neg']\n",
    "            sum_pos += sentiment_dict['pos']\n",
    "            sum_neu += sentiment_dict['neu']\n",
    "            sum_compound += sentiment_dict['compound']\n",
    "            \n",
    "        scores[\"Neg\"] = sum_neg/df.loc[df[\"video_id\"]==video][\"comment\"].count()\n",
    "        scores[\"Pos\"] = sum_pos/df.loc[df[\"video_id\"]==video][\"comment\"].count()\n",
    "        scores[\"Neu\"] = sum_neu/df.loc[df[\"video_id\"]==video][\"comment\"].count()\n",
    "        scores[\"Overall\"] = sum_compound/df.loc[df[\"video_id\"]==video][\"comment\"].count()\n",
    "        \n",
    "        polarity_scores[video]=scores\n",
    "        '''# decide sentiment as positive, negative and neutral\n",
    "        if sentiment_dict['compound'] >= 0.05 :\n",
    "            print(\"Positive\")\n",
    " \n",
    "        elif sentiment_dict['compound'] <= - 0.05 :\n",
    "            print(\"Negative\")\n",
    " \n",
    "        else :\n",
    "            print(\"Neutral\")'''\n",
    "    return polarity_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_vid_vader_polarity(translated_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "#import re\n",
    "#from scipy.special import softmax\n",
    "\n",
    "def roberta_per_vid_scores(df):\n",
    "    polarity_scores={}\n",
    "\n",
    "    roberta = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "    labels = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(roberta)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(roberta)\n",
    "    for video in df[\"video_id\"].unique():\n",
    "        sum_neg =0\n",
    "        sum_pos = 0\n",
    "        sum_neu =0 \n",
    "        scores={}\n",
    "        for comments in df.loc[df[\"video_id\"]==video][\"comment\"]:\n",
    "            comment_words = []\n",
    "            comments = comments.replace(\"\\n\", \" \")\n",
    "            comments = comments.replace(\"\\xa0\", \" \")\n",
    "            comments = comments.replace(\"?\", \" \")\n",
    "            comments = comments.replace(\":\", \" \")\n",
    "            comments = comments.replace(\";\", \" \")\n",
    "            comments = comments.replace(\";\", \" \")\n",
    "            comments = re.sub(r\"\\s+\", ' ', comments) \n",
    "     #   print(comments)\n",
    "            for word in comments.split(' '):\n",
    "                if word.startswith('@') and len(word) > 1:\n",
    "                    word = '@user'\n",
    "        \n",
    "                elif word.startswith('http'):\n",
    "                    word = \"http\"\n",
    "                comment_words.append(word)\n",
    "\n",
    "            comment_procs = \" \".join(comment_words)\n",
    "\n",
    "            encoded = tokenizer(comment_procs, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "           # print(encoded)\n",
    "            output = model(**encoded)\n",
    "\n",
    "            scores = output[0][0].detach().numpy()\n",
    "\n",
    "            scores = softmax(scores)\n",
    "\n",
    "            for i in range(len(scores)):\n",
    "\n",
    "                l = labels[i]\n",
    "                s = scores[i]\n",
    "                #    labels = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "                if l==\"Negative\":\n",
    "                    sum_neg+= s\n",
    "                elif l == \"Neutral\":\n",
    "                    sum_neu+=s\n",
    "                else : \n",
    "                    sum_pos+=s\n",
    "       # scores[\"Neg\"] = sum_neg/df.loc[df[\"video_id\"]==video][\"comment\"].count()\n",
    "        scores =dict([(\"Neg\", sum_neg/df.loc[df[\"video_id\"]==video][\"comment\"].count()),(\"Pos\",sum_pos/df.loc[df[\"video_id\"]==video][\"comment\"].count()),(\"Neu\",sum_neu/df.loc[df[\"video_id\"]==video][\"comment\"].count()\n",
    ")])\n",
    "        #scores[\"Pos\"] = sum_pos/df.loc[df[\"video_id\"]==video][\"comment\"].count()\n",
    "        #scores[\"Neu\"] = sum_neu/df.loc[df[\"video_id\"]==video][\"comment\"].count()\n",
    "\n",
    "        polarity_scores[video] = scores\n",
    "    return polarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_per_vid_scores(translated_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
