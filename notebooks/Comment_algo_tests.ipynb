{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import translators as ts\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>Loved the calmness of Manilla.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>After this pandemic I think every country shou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>manila looks beautifull with less people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>The lockdown makes the city look like a place ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>India also same</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                            comment\n",
       "0  aLZ85hb4wjE                     Loved the calmness of Manilla.\n",
       "1  aLZ85hb4wjE  After this pandemic I think every country shou...\n",
       "2  aLZ85hb4wjE           manila looks beautifull with less people\n",
       "3  aLZ85hb4wjE  The lockdown makes the city look like a place ...\n",
       "4  aLZ85hb4wjE                                    India also same"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../datasets/covid_philippines/covid_philippines_comments.csv\").drop(\"Unnamed: 0\", axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRANSLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_comments = {}\n",
    "translated_comments[\"video_id\"] = {}\n",
    "translated_comments[\"comment\"] = {}\n",
    "video_id_list = df[\"video_id\"].to_list()\n",
    "comments_list = df[\"comment\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1216/1216 [04:12<00:00,  4.81it/s]\n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(total=len(video_id_list))\n",
    "pbar.set_description(\"Translating...\")\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    if comments_list[i] != None:\n",
    "        new_comment = comments_list[i]\n",
    "        try:\n",
    "            lang = detect(comments_list[i]) #added langdetect since it errors if there are many entries to translate, so now it will ontly tranlate if comment not english\n",
    "            if lang != 'en':\n",
    "                new_comment = ts.translate_text(comments_list[i], 'google', to_language = 'en')\n",
    "                \n",
    "        except:\n",
    "            # No change; get same comment from list\n",
    "            pass\n",
    "            \n",
    "        finally:\n",
    "            translated_comments[\"video_id\"][i] = video_id_list[i]\n",
    "            translated_comments[\"comment\"][i] = new_comment\n",
    "            pbar.update(1)\n",
    "\n",
    "translated_df = pd.DataFrame.from_dict(translated_comments)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>Loved the calmness of Manilla.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>After this pandemic I think every country shou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>manila looks beautifull with less people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>The lockdown makes the city look like a place ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>India also same</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>This covid will be a never-ending fuckery as l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>A new variant is inevitable.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>The man that should resign from his office is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>Is that the people who got been vaccinated.,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>The toxic of comments section ðŸ˜„ \\r\\n During th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1216 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         video_id                                            comment\n",
       "0     aLZ85hb4wjE                     Loved the calmness of Manilla.\n",
       "1     aLZ85hb4wjE  After this pandemic I think every country shou...\n",
       "2     aLZ85hb4wjE           manila looks beautifull with less people\n",
       "3     aLZ85hb4wjE  The lockdown makes the city look like a place ...\n",
       "4     aLZ85hb4wjE                                    India also same\n",
       "...           ...                                                ...\n",
       "1211  5DvMPgoKZmM  This covid will be a never-ending fuckery as l...\n",
       "1212  5DvMPgoKZmM                       A new variant is inevitable.\n",
       "1213  5DvMPgoKZmM  The man that should resign from his office is ...\n",
       "1214  5DvMPgoKZmM       Is that the people who got been vaccinated.,\n",
       "1215  5DvMPgoKZmM  The toxic of comments section ðŸ˜„ \\r\\n During th...\n",
       "\n",
       "[1216 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(video_id_list, comments_list):\n",
    "    translated_comments = {}\n",
    "    translated_comments[\"video_id\"] = {}\n",
    "    translated_comments[\"comment\"] = {}\n",
    "    \n",
    "    pbar = tqdm(total=len(video_id_list))\n",
    "    pbar.set_description(\"Translating...\")\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        if comments_list[i] != None:\n",
    "            new_comment = comments_list[i]\n",
    "            try:\n",
    "                lang = detect(comments_list[i]) #added langdetect since it errors if there are many entries to translate, so now it will ontly tranlate if comment not english\n",
    "                if lang != 'en':\n",
    "                    new_comment = ts.translate_text(comments_list[i], 'google', to_language = 'en')\n",
    "\n",
    "            except:\n",
    "                # No change; get same comment from list\n",
    "                pass\n",
    "\n",
    "            finally:\n",
    "                translated_comments[\"video_id\"][i] = video_id_list[i]\n",
    "                translated_comments[\"comment\"][i] = new_comment\n",
    "                pbar.update(1)\n",
    "\n",
    "    translated_df = pd.DataFrame.from_dict(translated_comments)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DETECT SPAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0:Not Spam\n",
    "- 1:Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COMMENT_ID    1508\n",
       "AUTHOR        1508\n",
       "DATE          1508\n",
       "CONTENT       1508\n",
       "CLASS         1508\n",
       "dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset source: https://archive.ics.uci.edu/dataset/380/youtube+spam+collection\n",
    "\n",
    "psy_model_df = pd.read_csv(\"../datasets/model_train/Youtube01-Psy.csv\")\n",
    "lmfao_model_df = pd.read_csv(\"../datasets/model_train/Youtube03-LMFAO.csv\")\n",
    "kp_model_df = pd.read_csv(\"../datasets/model_train/Youtube02-KatyPerry.csv\")\n",
    "shakira_df = pd.read_csv(\"../datasets/model_train/Youtube05-Shakira.csv\")\n",
    "model_df = pd.concat([psy_model_df,lmfao_model_df,kp_model_df,shakira_df])\n",
    "model_df.reset_index(inplace=True, drop=True) \n",
    "model_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07T06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
       "      <td>adam riyati</td>\n",
       "      <td>2013-11-07T12:37:15</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>2013-11-08T17:34:21</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z13jhp0bxqncu512g22wvzkasxmvvzjaz04</td>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>2013-11-09T08:28:43</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ï»¿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z13fwbwp1oujthgqj04chlngpvzmtt3r3dw</td>\n",
       "      <td>GsMega</td>\n",
       "      <td>2013-11-10T16:05:38</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .ï»¿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>_2viQ_Qnc6_1Hq9MGlefkBIszt9rYD3S_CozADvMhQ4</td>\n",
       "      <td>Dinova Sharon</td>\n",
       "      <td>2013-07-13T14:44:00.700000</td>\n",
       "      <td>well done shakira</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>_2viQ_Qnc6-bMSjqyL1NKj57ROicCSJV5SwTrw-RFFA</td>\n",
       "      <td>Katie Mettam</td>\n",
       "      <td>2013-07-13T13:27:39.441000</td>\n",
       "      <td>I love this song because we sing it at Camp al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>_2viQ_Qnc6-pY-1yR6K2FhmC5i48-WuNx5CumlHLDAI</td>\n",
       "      <td>Sabina Pearson-Smith</td>\n",
       "      <td>2013-07-13T13:14:30.021000</td>\n",
       "      <td>I love this song for two reasons: 1.it is abou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>_2viQ_Qnc6_yBt8UGMWyg3vh0PulTqcqyQtdE7d4Fl0</td>\n",
       "      <td>Aishlin Maciel</td>\n",
       "      <td>2013-07-13T11:17:52.308000</td>\n",
       "      <td>Shakira u are so wiredo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507</th>\n",
       "      <td>_2viQ_Qnc685RPw1aSa1tfrIuHXRvAQ2rPT9R06KTqA</td>\n",
       "      <td>Latin Bosch</td>\n",
       "      <td>2013-07-12T22:33:27.916000</td>\n",
       "      <td>Shakira is the best dancer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1362 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       COMMENT_ID                AUTHOR  \\\n",
       "0     LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU             Julius NM   \n",
       "1     LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A           adam riyati   \n",
       "2     LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8      Evgeny Murashkin   \n",
       "3             z13jhp0bxqncu512g22wvzkasxmvvzjaz04       ElNino Melendez   \n",
       "4             z13fwbwp1oujthgqj04chlngpvzmtt3r3dw                GsMega   \n",
       "...                                           ...                   ...   \n",
       "1502  _2viQ_Qnc6_1Hq9MGlefkBIszt9rYD3S_CozADvMhQ4         Dinova Sharon   \n",
       "1503  _2viQ_Qnc6-bMSjqyL1NKj57ROicCSJV5SwTrw-RFFA          Katie Mettam   \n",
       "1504  _2viQ_Qnc6-pY-1yR6K2FhmC5i48-WuNx5CumlHLDAI  Sabina Pearson-Smith   \n",
       "1506  _2viQ_Qnc6_yBt8UGMWyg3vh0PulTqcqyQtdE7d4Fl0        Aishlin Maciel   \n",
       "1507  _2viQ_Qnc685RPw1aSa1tfrIuHXRvAQ2rPT9R06KTqA           Latin Bosch   \n",
       "\n",
       "                            DATE  \\\n",
       "0            2013-11-07T06:20:48   \n",
       "1            2013-11-07T12:37:15   \n",
       "2            2013-11-08T17:34:21   \n",
       "3            2013-11-09T08:28:43   \n",
       "4            2013-11-10T16:05:38   \n",
       "...                          ...   \n",
       "1502  2013-07-13T14:44:00.700000   \n",
       "1503  2013-07-13T13:27:39.441000   \n",
       "1504  2013-07-13T13:14:30.021000   \n",
       "1506  2013-07-13T11:17:52.308000   \n",
       "1507  2013-07-12T22:33:27.916000   \n",
       "\n",
       "                                                CONTENT  CLASS  \n",
       "0     Huh, anyway check out this you[tube] channel: ...      1  \n",
       "1     Hey guys check out my new channel and our firs...      1  \n",
       "2                just for test I have to say murdev.com      1  \n",
       "3      me shaking my sexy ass on my channel enjoy ^_^ ï»¿      1  \n",
       "4               watch?v=vtaRGgvGtWQ   Check this out .ï»¿      1  \n",
       "...                                                 ...    ...  \n",
       "1502                                  well done shakira      0  \n",
       "1503  I love this song because we sing it at Camp al...      0  \n",
       "1504  I love this song for two reasons: 1.it is abou...      0  \n",
       "1506                            Shakira u are so wiredo      0  \n",
       "1507                         Shakira is the best dancer      0  \n",
       "\n",
       "[1362 rows x 5 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df.drop_duplicates(subset=\"CONTENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COMMENT_ID    1508\n",
       "AUTHOR        1508\n",
       "DATE          1508\n",
       "CONTENT       1508\n",
       "CLASS         1508\n",
       "dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Melanie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "stop_words = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df['CONTENT'] = model_df['CONTENT'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "\n",
    "ps = PorterStemmer() \n",
    "for w in model_df[\"CONTENT\"]: \n",
    "    #convert to lowercase\n",
    "    model_df['CONTENT'] = model_df[\"CONTENT\"].str.lower()\n",
    "    #Stem\n",
    "    model_df['CONTENT'] = model_df[\"CONTENT\"].apply(ps.stem) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=model_df[\"CONTENT\"]\n",
    "y=model_df[\"CLASS\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test  = train_test_split(x,y,random_state=42)\n",
    "x_train=vectorizer.fit_transform(x_train)\n",
    "x_test = vectorizer.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9708222811671088\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       544\n",
      "           1       0.97      0.97      0.97       587\n",
      "\n",
      "    accuracy                           0.97      1131\n",
      "   macro avg       0.97      0.97      0.97      1131\n",
      "weighted avg       0.97      0.97      0.97      1131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spam_nb = MultinomialNB()\n",
    "spam_nb.fit(x_train,y_train)\n",
    "\n",
    "predictions=spam_nb.predict(x_train)\n",
    "accuracy = accuracy_score(y_train, predictions)\n",
    "print(f\"Train Accuracy: {accuracy}\")\n",
    "\n",
    "class_report = classification_report(y_train, predictions)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9204244031830239\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.92       204\n",
      "           1       0.89      0.94      0.92       173\n",
      "\n",
      "    accuracy                           0.92       377\n",
      "   macro avg       0.92      0.92      0.92       377\n",
      "weighted avg       0.92      0.92      0.92       377\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions=spam_nb.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "class_report = classification_report(y_test, predictions)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test with another dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>z12rwfnyyrbsefonb232i5ehdxzkjzjs2</td>\n",
       "      <td>Lisa Wellas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+447935454150 lovely girl talk to me xxxï»¿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>z130wpnwwnyuetxcn23xf5k5ynmkdpjrj04</td>\n",
       "      <td>jason graham</td>\n",
       "      <td>2015-05-29T02:26:10.652000</td>\n",
       "      <td>I always end up coming back to this song&lt;br /&gt;ï»¿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>z13vsfqirtavjvu0t22ezrgzyorwxhpf3</td>\n",
       "      <td>Ajkal Khan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>my sister just received over 6,500 new &lt;a rel=...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z12wjzc4eprnvja4304cgbbizuved35wxcs</td>\n",
       "      <td>Dakota Taylor</td>\n",
       "      <td>2015-05-29T02:13:07.810000</td>\n",
       "      <td>Coolï»¿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z13xjfr42z3uxdz2223gx5rrzs3dt5hna</td>\n",
       "      <td>Jihad Naser</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hello I&amp;#39;am from Palastineï»¿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            COMMENT_ID         AUTHOR  \\\n",
       "0    z12rwfnyyrbsefonb232i5ehdxzkjzjs2    Lisa Wellas   \n",
       "1  z130wpnwwnyuetxcn23xf5k5ynmkdpjrj04   jason graham   \n",
       "2    z13vsfqirtavjvu0t22ezrgzyorwxhpf3     Ajkal Khan   \n",
       "3  z12wjzc4eprnvja4304cgbbizuved35wxcs  Dakota Taylor   \n",
       "4    z13xjfr42z3uxdz2223gx5rrzs3dt5hna    Jihad Naser   \n",
       "\n",
       "                         DATE  \\\n",
       "0                         NaN   \n",
       "1  2015-05-29T02:26:10.652000   \n",
       "2                         NaN   \n",
       "3  2015-05-29T02:13:07.810000   \n",
       "4                         NaN   \n",
       "\n",
       "                                             CONTENT  CLASS  \n",
       "0          +447935454150 lovely girl talk to me xxxï»¿      1  \n",
       "1    I always end up coming back to this song<br />ï»¿      0  \n",
       "2  my sister just received over 6,500 new <a rel=...      1  \n",
       "3                                              Coolï»¿      0  \n",
       "4                     Hello I&#39;am from Palastineï»¿      1  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset source: https://archive.ics.uci.edu/dataset/380/youtube+spam+collection\n",
    "\n",
    "em_model_df = pd.read_csv(\"../datasets/model_train/Youtube04-Eminem.csv\")\n",
    "em_model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_model_df['CONTENT'] = em_model_df['CONTENT'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "\n",
    "ps = PorterStemmer() \n",
    "for w in em_model_df[\"CONTENT\"]: \n",
    "    #convert to lowercase\n",
    "    em_model_df['CONTENT'] = em_model_df[\"CONTENT\"].str.lower()\n",
    "    #Stem\n",
    "    em_model_df['CONTENT'] = em_model_df[\"CONTENT\"].apply(ps.stem) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_x=em_model_df[\"CONTENT\"]\n",
    "em_y=em_model_df[\"CLASS\"]\n",
    "\n",
    "em_x=vectorizer.transform(em_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8526785714285714\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.75      0.82       203\n",
      "           1       0.82      0.94      0.87       245\n",
      "\n",
      "    accuracy                           0.85       448\n",
      "   macro avg       0.86      0.84      0.85       448\n",
      "weighted avg       0.86      0.85      0.85       448\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions=spam_nb.predict(em_x)\n",
    "accuracy = accuracy_score(em_y, predictions)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "class_report = classification_report(em_y, predictions)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=model_df[\"CONTENT\"]\n",
    "y=model_df[\"CLASS\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test  = train_test_split(x,y,test_size=0.30,random_state=42)\n",
    "x_train=vectorizer.fit_transform(x_train)\n",
    "x_test = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8265402843601896\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82       504\n",
      "           1       0.84      0.83      0.83       551\n",
      "\n",
      "    accuracy                           0.83      1055\n",
      "   macro avg       0.83      0.83      0.83      1055\n",
      "weighted avg       0.83      0.83      0.83      1055\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel = 'sigmoid', gamma = 1.0)\n",
    "svm_model.fit(x_train, y_train)\n",
    "\n",
    "predictions = svm_model.predict(x_train)\n",
    "accuracy = accuracy_score(y_train, predictions)\n",
    "print(f\"Train Accuracy: {accuracy}\")\n",
    "\n",
    "class_report = classification_report(y_train, predictions)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8432671081677704\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.83      0.85       244\n",
      "           1       0.81      0.86      0.83       209\n",
      "\n",
      "    accuracy                           0.84       453\n",
      "   macro avg       0.84      0.84      0.84       453\n",
      "weighted avg       0.84      0.84      0.84       453\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = svm_model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "class_report = classification_report(y_test, predictions)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "svc = SVC(max_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters= [\n",
    "    {\n",
    "        \"C\":[0.0001, 0.001, 0.01 , 0.1, 1.0, 5, 30, 50],\n",
    "        \"kernel\": [\"linear\",\"poly\",\"rbf\",\"sigmoid\"],\n",
    "        \"degree\" :[1, 3, 5, 10, 25, 50,100],\n",
    "        \"gamma\" :[\"scale\", \"auto\",1000, 10, 5, 2.5, 1.5, 1.0]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "14 fits failed out of a total of 150.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "14 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 268, in fit\n",
      "    raise ValueError(\n",
      "ValueError: The dual coefficients or intercepts are not finite. The input data may contain large values and need to be preprocessed.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.63507109 0.56018957 0.51184834 0.77535545 0.93744076 0.83222749\n",
      " 0.77630332 0.92985782 0.82748815 0.93744076 0.51848341 0.80094787\n",
      " 0.93744076 0.93744076 0.93744076 0.90900474 0.83127962        nan\n",
      " 0.5943128  0.51848341 0.81706161 0.52511848 0.88720379 0.94881517\n",
      " 0.71090047 0.56018957        nan 0.6056872  0.77535545        nan]\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "rssvc = RandomizedSearchCV(estimator = svc, param_distributions = hyperparameters, n_iter =30, cv=5, random_state=42).fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=30, max_iter=200)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=30, max_iter=200)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=30, max_iter=200)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rssvc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9448123620309051\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95       244\n",
      "           1       0.94      0.94      0.94       209\n",
      "\n",
      "    accuracy                           0.94       453\n",
      "   macro avg       0.94      0.94      0.94       453\n",
      "weighted avg       0.94      0.94      0.94       453\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions=rssvc.best_estimator_.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "class_report = classification_report(y_test, predictions)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (EMINEM DF): 0.9665178571428571\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96       203\n",
      "           1       0.99      0.95      0.97       245\n",
      "\n",
      "    accuracy                           0.97       448\n",
      "   macro avg       0.97      0.97      0.97       448\n",
      "weighted avg       0.97      0.97      0.97       448\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions=rssvc.best_estimator_.predict(em_x)\n",
    "accuracy = accuracy_score(em_y, predictions)\n",
    "print(f\"Test Accuracy (EMINEM DF): {accuracy}\")\n",
    "\n",
    "class_report = classification_report(em_y, predictions)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_svc=rssvc.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try model on translated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_spam_filtered = translated_df.copy()\n",
    "#svm_spam_filtered = translated_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_spam_filtered[\"comment_cleaned\"] = nb_spam_filtered[\"comment\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "\n",
    "ps = PorterStemmer() \n",
    "for w in nb_spam_filtered[\"comment\"]: \n",
    "    nb_spam_filtered[\"comment_cleaned\"] = nb_spam_filtered[\"comment\"].apply(ps.stem) \n",
    "    nb_spam_filtered[\"comment_cleaned\"] = nb_spam_filtered[\"comment\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_spam_filtered = nb_spam_filtered.copy() #so no need to go through same cleaning/preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>loved calmness manilla.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>after pandemic i think every country lockdown ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>manila looks beautifull less peopl</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>the lockdown makes city look like place i want...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>india also</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                            comment  spam\n",
       "0  aLZ85hb4wjE                            loved calmness manilla.     0\n",
       "1  aLZ85hb4wjE  after pandemic i think every country lockdown ...     0\n",
       "2  aLZ85hb4wjE                 manila looks beautifull less peopl     0\n",
       "3  aLZ85hb4wjE  the lockdown makes city look like place i want...     0\n",
       "4  aLZ85hb4wjE                                         india also     1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed = vectorizer.transform(translated_df[\"comment\"])\n",
    "\n",
    "translated_df[\"spam\"]=spam_nb.predict(transformed)\n",
    "translated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "video_id    562\n",
       "comment     562\n",
       "spam        562\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_df[translated_df[\"spam\"]==1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "video_id    654\n",
       "comment     654\n",
       "spam        654\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_df[translated_df[\"spam\"]==0].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seems like there are too many tagged as spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>india also</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>2020 year nature fight back reduce human emiss...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sYI97jv-pZg</td>\n",
       "      <td>cough cold season weather cold .... covid amg ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sYI97jv-pZg</td>\n",
       "      <td>ala n covid tngina</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3YFpjgIQqEo</td>\n",
       "      <td>this cold make worse make money</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>Wjj__vIdew0</td>\n",
       "      <td>your daily dose fearmong</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203</th>\n",
       "      <td>Wjj__vIdew0</td>\n",
       "      <td>gma bat numb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>Wjj__vIdew0</td>\n",
       "      <td>nest</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>a new variant inevitable.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>the toxic comments section ðŸ˜„ during time, poor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>562 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         video_id                                            comment  spam\n",
       "4     aLZ85hb4wjE                                         india also     1\n",
       "9     aLZ85hb4wjE  2020 year nature fight back reduce human emiss...     1\n",
       "10    sYI97jv-pZg  cough cold season weather cold .... covid amg ...     1\n",
       "11    sYI97jv-pZg                                 ala n covid tngina     1\n",
       "12    3YFpjgIQqEo                    this cold make worse make money     1\n",
       "...           ...                                                ...   ...\n",
       "1201  Wjj__vIdew0                           your daily dose fearmong     1\n",
       "1203  Wjj__vIdew0                                       gma bat numb     1\n",
       "1204  Wjj__vIdew0                                               nest     1\n",
       "1212  5DvMPgoKZmM                          a new variant inevitable.     1\n",
       "1215  5DvMPgoKZmM  the toxic comments section ðŸ˜„ during time, poor...     1\n",
       "\n",
       "[562 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_df[translated_df[\"spam\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>loved calmness manilla.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>after pandemic i think every country lockdown ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>manila looks beautifull less peopl</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>the lockdown makes city look like place i want...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>the president promised he'll best ease traffic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>honestly, i hope things least get slightly bet...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>please, option get vaccinated, it. it still po...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>this covid never-ending fuckery long media kee...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>the man resign office nonetheless present head...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>is people got vaccinated.,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>654 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         video_id                                            comment  spam\n",
       "0     aLZ85hb4wjE                            loved calmness manilla.     0\n",
       "1     aLZ85hb4wjE  after pandemic i think every country lockdown ...     0\n",
       "2     aLZ85hb4wjE                 manila looks beautifull less peopl     0\n",
       "3     aLZ85hb4wjE  the lockdown makes city look like place i want...     0\n",
       "5     aLZ85hb4wjE  the president promised he'll best ease traffic...     0\n",
       "...           ...                                                ...   ...\n",
       "1209  5DvMPgoKZmM  honestly, i hope things least get slightly bet...     0\n",
       "1210  5DvMPgoKZmM  please, option get vaccinated, it. it still po...     0\n",
       "1211  5DvMPgoKZmM  this covid never-ending fuckery long media kee...     0\n",
       "1213  5DvMPgoKZmM  the man resign office nonetheless present head...     0\n",
       "1214  5DvMPgoKZmM                         is people got vaccinated.,     0\n",
       "\n",
       "[654 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_df[translated_df[\"spam\"]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_df[translated_df[\"spam\"]==1].to_csv(\"check_spam.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_df[translated_df[\"spam\"]==0].to_csv(\"check_not_spam.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Israel-Palestine comments check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R0ftmf_Uv9A</td>\n",
       "      <td>No matter how many times these information get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R0ftmf_Uv9A</td>\n",
       "      <td>*To learn who RULES over you, simply find out ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R0ftmf_Uv9A</td>\n",
       "      <td>Say that part again: Jewish , Christianâ€™s and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R0ftmf_Uv9A</td>\n",
       "      <td>So sad. They were living in peace and now suff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R0ftmf_Uv9A</td>\n",
       "      <td>Why start at 1946?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                            comment\n",
       "0  R0ftmf_Uv9A  No matter how many times these information get...\n",
       "1  R0ftmf_Uv9A  *To learn who RULES over you, simply find out ...\n",
       "2  R0ftmf_Uv9A  Say that part again: Jewish , Christianâ€™s and ...\n",
       "3  R0ftmf_Uv9A  So sad. They were living in peace and now suff...\n",
       "4  R0ftmf_Uv9A                                 Why start at 1946?"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_pal_df = pd.read_csv(\"../datasets/israel-palestine_conflict_history/comments.csv\").drop(\"Unnamed: 0\", axis=1)\n",
    "is_pal_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_pal_df[\"comment\"] = is_pal_df[\"comment\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "\n",
    "ps = PorterStemmer() \n",
    "for w in is_pal_df[\"comment\"]: \n",
    "    is_pal_df[\"comment\"] = is_pal_df[\"comment\"].apply(ps.stem) \n",
    "    is_pal_df[\"comment\"] = is_pal_df[\"comment\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R0ftmf_Uv9A</td>\n",
       "      <td>no matter many times information gets thrown e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R0ftmf_Uv9A</td>\n",
       "      <td>*to learn rules you, simply find not allowed c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R0ftmf_Uv9A</td>\n",
       "      <td>say part again: jewish , christianâ€™s muslims l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R0ftmf_Uv9A</td>\n",
       "      <td>so sad. they living peace suffering 7 decades ðŸ˜¢</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R0ftmf_Uv9A</td>\n",
       "      <td>why start 1946?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                            comment  spam\n",
       "0  R0ftmf_Uv9A  no matter many times information gets thrown e...     0\n",
       "1  R0ftmf_Uv9A  *to learn rules you, simply find not allowed c...     1\n",
       "2  R0ftmf_Uv9A  say part again: jewish , christianâ€™s muslims l...     1\n",
       "3  R0ftmf_Uv9A    so sad. they living peace suffering 7 decades ðŸ˜¢     0\n",
       "4  R0ftmf_Uv9A                                    why start 1946?     0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed = vectorizer.transform(is_pal_df[\"comment\"])\n",
    "\n",
    "is_pal_df[\"spam\"]=spam_nb.predict(transformed)\n",
    "is_pal_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "video_id    757\n",
       "comment     757\n",
       "spam        757\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_pal_df[is_pal_df[\"spam\"]==1].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "video_id    1114\n",
       "comment     1114\n",
       "spam        1114\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_pal_df[is_pal_df[\"spam\"]==0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R0ftmf_Uv9A</td>\n",
       "      <td>*to learn rules you, simply find not allowed c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R0ftmf_Uv9A</td>\n",
       "      <td>say part again: jewish , christianâ€™s muslims l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>R0ftmf_Uv9A</td>\n",
       "      <td>thank's ireland, consistent n vocal supporting...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>R0ftmf_Uv9A</td>\n",
       "      <td>let peace prevail.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bno1m1zhIWs</td>\n",
       "      <td>finally. an objective concise summary religiou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>JuU7pSDs8f4</td>\n",
       "      <td>angel one demat account(free) - https://tinyur...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1863</th>\n",
       "      <td>JuU7pSDs8f4</td>\n",
       "      <td>nobody explain clear do. pl. keep videos.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867</th>\n",
       "      <td>JuU7pSDs8f4</td>\n",
       "      <td>i'm upsc aspirant ....i seen many videos regar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1869</th>\n",
       "      <td>JuU7pSDs8f4</td>\n",
       "      <td>bro literally i searched video war started. no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1870</th>\n",
       "      <td>JuU7pSDs8f4</td>\n",
       "      <td>à°…à°¨à±à°¨ à°¨à°¿à°œà°‚ à°šà±†à°ªà±à°ªà°¾à°²à°‚à°Ÿà±‡ à°¨à±€ à°µà±€à°¡à°¿à°¯à±‹ à°•à±‹à°¸à°®à±‡ à°à°¦à± à°°à±‹à°œà±à°²...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>757 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         video_id                                            comment  spam\n",
       "1     R0ftmf_Uv9A  *to learn rules you, simply find not allowed c...     1\n",
       "2     R0ftmf_Uv9A  say part again: jewish , christianâ€™s muslims l...     1\n",
       "5     R0ftmf_Uv9A  thank's ireland, consistent n vocal supporting...     1\n",
       "8     R0ftmf_Uv9A                                 let peace prevail.     1\n",
       "12    Bno1m1zhIWs  finally. an objective concise summary religiou...     1\n",
       "...           ...                                                ...   ...\n",
       "1861  JuU7pSDs8f4  angel one demat account(free) - https://tinyur...     1\n",
       "1863  JuU7pSDs8f4          nobody explain clear do. pl. keep videos.     1\n",
       "1867  JuU7pSDs8f4  i'm upsc aspirant ....i seen many videos regar...     1\n",
       "1869  JuU7pSDs8f4  bro literally i searched video war started. no...     1\n",
       "1870  JuU7pSDs8f4  à°…à°¨à±à°¨ à°¨à°¿à°œà°‚ à°šà±†à°ªà±à°ªà°¾à°²à°‚à°Ÿà±‡ à°¨à±€ à°µà±€à°¡à°¿à°¯à±‹ à°•à±‹à°¸à°®à±‡ à°à°¦à± à°°à±‹à°œà±à°²...     1\n",
       "\n",
       "[757 rows x 3 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_pal_df[is_pal_df[\"spam\"]==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model 40% of israel palestine comments were tagged Spam. While translated covid dataset has 45.39 of data tagged as spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Time</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Reply Count</th>\n",
       "      <th>Spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Taofeekat</td>\n",
       "      <td>&amp;lt;????i make my first million investing in f...</td>\n",
       "      <td>2022-09-28T02:08:55Z</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Angelina Jordan</td>\n",
       "      <td>&amp;lt;?l will forever be indebted to you I will ...</td>\n",
       "      <td>2022-09-23T05:26:48Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fernandez Joe</td>\n",
       "      <td>&lt;b&gt;????I recommend a professional forex/Bitcoi...</td>\n",
       "      <td>2022-09-20T12:56:30Z</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jessica  Billy</td>\n",
       "      <td>I think Iâ€™m blessed because if not I wouldnâ€™t ...</td>\n",
       "      <td>2022-09-17T20:20:24Z</td>\n",
       "      <td>21</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Allison Zar</td>\n",
       "      <td>&lt;b&gt;I recommend a professional  broker to you g...</td>\n",
       "      <td>2022-09-05T09:19:30Z</td>\n",
       "      <td>19</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name                                            Comment  \\\n",
       "0        Taofeekat  &lt;????i make my first million investing in f...   \n",
       "1  Angelina Jordan  &lt;?l will forever be indebted to you I will ...   \n",
       "2    Fernandez Joe  <b>????I recommend a professional forex/Bitcoi...   \n",
       "3   Jessica  Billy  I think Iâ€™m blessed because if not I wouldnâ€™t ...   \n",
       "4      Allison Zar  <b>I recommend a professional  broker to you g...   \n",
       "\n",
       "                   Time  Likes  Reply Count  Spam  \n",
       "0  2022-09-28T02:08:55Z     30           30     1  \n",
       "1  2022-09-23T05:26:48Z      0            0     1  \n",
       "2  2022-09-20T12:56:30Z      5            2     1  \n",
       "3  2022-09-17T20:20:24Z     21           34     1  \n",
       "4  2022-09-05T09:19:30Z     19           27     1  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#source: https://www.kaggle.com/datasets/madhuragl/5000-youtube-spamnot-spam-dataset/data\n",
    "comments_5k_df = pd.read_csv(\"../datasets/model_train/5000 YT comments.csv\",encoding='cp1252')\n",
    "comments_5k_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name           5000\n",
       "Comment        5000\n",
       "Time           5000\n",
       "Likes          5000\n",
       "Reply Count    5000\n",
       "Spam           5000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_5k_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Time</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Reply Count</th>\n",
       "      <th>Spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Taofeekat</td>\n",
       "      <td>&amp;lt;????i make my first million investing in f...</td>\n",
       "      <td>2022-09-28T02:08:55Z</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Angelina Jordan</td>\n",
       "      <td>&amp;lt;?l will forever be indebted to you I will ...</td>\n",
       "      <td>2022-09-23T05:26:48Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fernandez Joe</td>\n",
       "      <td>&lt;b&gt;????I recommend a professional forex/Bitcoi...</td>\n",
       "      <td>2022-09-20T12:56:30Z</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jessica  Billy</td>\n",
       "      <td>I think Iâ€™m blessed because if not I wouldnâ€™t ...</td>\n",
       "      <td>2022-09-17T20:20:24Z</td>\n",
       "      <td>21</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Allison Zar</td>\n",
       "      <td>&lt;b&gt;I recommend a professional  broker to you g...</td>\n",
       "      <td>2022-09-05T09:19:30Z</td>\n",
       "      <td>19</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>Anjan Das</td>\n",
       "      <td>She is so beautiful!</td>\n",
       "      <td>2020-06-05T04:18:26Z</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>Humza Navaid</td>\n",
       "      <td>3 seconds in and I want to marry her. I am goi...</td>\n",
       "      <td>2020-06-04T21:03:14Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Aadil Ranesh</td>\n",
       "      <td>She talks a lot like Tanmay Bakshi</td>\n",
       "      <td>2020-06-03T17:29:04Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Fuzail Ahmad</td>\n",
       "      <td>Why does her face look like a bad deepfake?</td>\n",
       "      <td>2020-06-03T11:17:48Z</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>kcmn00</td>\n",
       "      <td>So, will bankers lose their jobs?</td>\n",
       "      <td>2020-06-03T08:11:15Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4673 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Name                                            Comment  \\\n",
       "0           Taofeekat  &lt;????i make my first million investing in f...   \n",
       "1     Angelina Jordan  &lt;?l will forever be indebted to you I will ...   \n",
       "2       Fernandez Joe  <b>????I recommend a professional forex/Bitcoi...   \n",
       "3      Jessica  Billy  I think Iâ€™m blessed because if not I wouldnâ€™t ...   \n",
       "4         Allison Zar  <b>I recommend a professional  broker to you g...   \n",
       "...               ...                                                ...   \n",
       "4995        Anjan Das                               She is so beautiful!   \n",
       "4996     Humza Navaid  3 seconds in and I want to marry her. I am goi...   \n",
       "4997     Aadil Ranesh                 She talks a lot like Tanmay Bakshi   \n",
       "4998     Fuzail Ahmad        Why does her face look like a bad deepfake?   \n",
       "4999           kcmn00                  So, will bankers lose their jobs?   \n",
       "\n",
       "                      Time  Likes  Reply Count  Spam  \n",
       "0     2022-09-28T02:08:55Z     30           30     1  \n",
       "1     2022-09-23T05:26:48Z      0            0     1  \n",
       "2     2022-09-20T12:56:30Z      5            2     1  \n",
       "3     2022-09-17T20:20:24Z     21           34     1  \n",
       "4     2022-09-05T09:19:30Z     19           27     1  \n",
       "...                    ...    ...          ...   ...  \n",
       "4995  2020-06-05T04:18:26Z      5            0     0  \n",
       "4996  2020-06-04T21:03:14Z      0            0     0  \n",
       "4997  2020-06-03T17:29:04Z      0            0     0  \n",
       "4998  2020-06-03T11:17:48Z      1            0     0  \n",
       "4999  2020-06-03T08:11:15Z      0            0     0  \n",
       "\n",
       "[4673 rows x 6 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_5k_df.drop_duplicates(subset=\"Comment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name           5000\n",
       "Comment        5000\n",
       "Time           5000\n",
       "Likes          5000\n",
       "Reply Count    5000\n",
       "Spam           5000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_5k_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_5k_df['Comment'] = comments_5k_df['Comment'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "ps = PorterStemmer() \n",
    "for w in comments_5k_df['Comment']: \n",
    "    comments_5k_df['Comment'] = comments_5k_df['Comment'].apply(ps.stem) \n",
    "    comments_5k_df['Comment'] = comments_5k_df['Comment'].str.lower()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=comments_5k_df[\"Comment\"]\n",
    "y=comments_5k_df[\"Spam\"]\n",
    "\n",
    "vectorizer= TfidfVectorizer()\n",
    "\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test  = train_test_split(x,y,random_state=42)\n",
    "x_train=vectorizer.fit_transform(x_train)\n",
    "x_test = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.936\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.91      0.93      1876\n",
      "           1       0.92      0.96      0.94      1874\n",
      "\n",
      "    accuracy                           0.94      3750\n",
      "   macro avg       0.94      0.94      0.94      3750\n",
      "weighted avg       0.94      0.94      0.94      3750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spam_nb = MultinomialNB()\n",
    "spam_nb.fit(x_train,y_train)\n",
    "\n",
    "predictions=spam_nb.predict(x_train)\n",
    "accuracy = accuracy_score(y_train, predictions)\n",
    "print(f\"Train Accuracy: {accuracy}\")\n",
    "\n",
    "class_report = classification_report(y_train, predictions)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8848\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.83      0.88       624\n",
      "           1       0.85      0.94      0.89       626\n",
      "\n",
      "    accuracy                           0.88      1250\n",
      "   macro avg       0.89      0.88      0.88      1250\n",
      "weighted avg       0.89      0.88      0.88      1250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions=spam_nb.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "class_report = classification_report(y_test, predictions)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_df = og_translated_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_df[\"comment\"] = translated_df[\"comment\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "\n",
    "ps = PorterStemmer() \n",
    "for w in translated_df[\"comment\"]: \n",
    "    translated_df[\"comment\"] = translated_df[\"comment\"].apply(ps.stem) \n",
    "    translated_df[\"comment\"] = translated_df[\"comment\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>loved calmness manilla.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>after pandemic i think every country lockdown ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>manila looks beautifull less peopl</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>the lockdown makes city look like place i want...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>india also</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                            comment  spam\n",
       "0  aLZ85hb4wjE                            loved calmness manilla.     0\n",
       "1  aLZ85hb4wjE  after pandemic i think every country lockdown ...     1\n",
       "2  aLZ85hb4wjE                 manila looks beautifull less peopl     0\n",
       "3  aLZ85hb4wjE  the lockdown makes city look like place i want...     0\n",
       "4  aLZ85hb4wjE                                         india also     1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed = vectorizer.transform(translated_df[\"comment\"])\n",
    "\n",
    "translated_df[\"spam\"]=spam_nb.predict(transformed)\n",
    "translated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "video_id    716\n",
       "comment     716\n",
       "spam        716\n",
       "dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_df[translated_df[\"spam\"]==1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "video_id    500\n",
       "comment     500\n",
       "spam        500\n",
       "dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_df[translated_df[\"spam\"]==0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>after pandemic i think every country lockdown ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aLZ85hb4wjE</td>\n",
       "      <td>india also</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sYI97jv-pZg</td>\n",
       "      <td>cough cold season weather cold .... covid amg ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sYI97jv-pZg</td>\n",
       "      <td>ala n covid tngina</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3YFpjgIQqEo</td>\n",
       "      <td>this cold make worse make money</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>it's almost 2 years this, many people honestly...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>please, option get vaccinated, it. it still po...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>this covid never-ending fuckery long media kee...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>a new variant inevitable.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <td>5DvMPgoKZmM</td>\n",
       "      <td>the toxic comments section ðŸ˜„ during time, poor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>716 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         video_id                                            comment  spam\n",
       "1     aLZ85hb4wjE  after pandemic i think every country lockdown ...     1\n",
       "4     aLZ85hb4wjE                                         india also     1\n",
       "10    sYI97jv-pZg  cough cold season weather cold .... covid amg ...     1\n",
       "11    sYI97jv-pZg                                 ala n covid tngina     1\n",
       "12    3YFpjgIQqEo                    this cold make worse make money     1\n",
       "...           ...                                                ...   ...\n",
       "1207  5DvMPgoKZmM  it's almost 2 years this, many people honestly...     1\n",
       "1210  5DvMPgoKZmM  please, option get vaccinated, it. it still po...     1\n",
       "1211  5DvMPgoKZmM  this covid never-ending fuckery long media kee...     1\n",
       "1212  5DvMPgoKZmM                          a new variant inevitable.     1\n",
       "1215  5DvMPgoKZmM  the toxic comments section ðŸ˜„ during time, poor...     1\n",
       "\n",
       "[716 rows x 3 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_df[translated_df[\"spam\"]==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try with israel palestine conflict dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_pal_df[\"comment\"] = is_pal_df[\"comment\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "\n",
    "ps = PorterStemmer() \n",
    "for w in is_pal_df[\"comment\"]: \n",
    "    is_pal_df[\"comment\"] = is_pal_df[\"comment\"].apply(ps.stem) \n",
    "    is_pal_df[\"comment\"] = is_pal_df[\"comment\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R0ftmf_Uv9A</td>\n",
       "      <td>matter many times information gets thrown eyes...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R0ftmf_Uv9A</td>\n",
       "      <td>*to learn rules you, simply find allowed criti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R0ftmf_Uv9A</td>\n",
       "      <td>say part again: jewish , christianâ€™s muslims l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R0ftmf_Uv9A</td>\n",
       "      <td>sad. living peace suffering 7 decades ðŸ˜¢</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R0ftmf_Uv9A</td>\n",
       "      <td>start 1946?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                            comment  spam\n",
       "0  R0ftmf_Uv9A  matter many times information gets thrown eyes...     0\n",
       "1  R0ftmf_Uv9A  *to learn rules you, simply find allowed criti...     1\n",
       "2  R0ftmf_Uv9A  say part again: jewish , christianâ€™s muslims l...     1\n",
       "3  R0ftmf_Uv9A            sad. living peace suffering 7 decades ðŸ˜¢     1\n",
       "4  R0ftmf_Uv9A                                        start 1946?     1"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed = vectorizer.transform(is_pal_df[\"comment\"])\n",
    "\n",
    "is_pal_df[\"spam\"]=spam_nb.predict(transformed)\n",
    "is_pal_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "video_id    901\n",
       "comment     901\n",
       "spam        901\n",
       "dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_pal_df[is_pal_df[\"spam\"]==1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "video_id    970\n",
       "comment     970\n",
       "spam        970\n",
       "dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_pal_df[is_pal_df[\"spam\"]==0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_pal_df[is_pal_df[\"spam\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_pal_df[is_pal_df[\"spam\"]==1].to_csv(\"check_spam.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model israel-palestine comments had 45.48% of the dataset as spam. While translated covid has 55.1% tagged as spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import re\n",
    "from scipy.special import softmax\n",
    "\n",
    "def roberta_sentiment_scores(list_comments):\n",
    "    roberta = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "    labels = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(roberta)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(roberta)\n",
    "\n",
    "    for comments in list_comments:\n",
    "        comment_words = []\n",
    "        comments = comments.replace(\"\\n\", \" \")\n",
    "        comments = comments.replace(\"\\xa0\", \" \")\n",
    "        comments = comments.replace(\"?\", \" \")\n",
    "        comments = comments.replace(\":\", \" \")\n",
    "        comments = comments.replace(\";\", \" \")\n",
    "        comments = comments.replace(\";\", \" \")\n",
    "        comments = re.sub(r\"\\s+\", ' ', comments) \n",
    "        print(comments)\n",
    "        for word in comments.split(' '):\n",
    "            if word.startswith('@') and len(word) > 1:\n",
    "               word = '@user'\n",
    "        \n",
    "            elif word.startswith('http'):\n",
    "                word = \"http\"\n",
    "            comment_words.append(word)\n",
    "\n",
    "        comment_procs = \" \".join(comment_words)\n",
    "\n",
    "        encoded = tokenizer(comment_procs, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "        print(encoded)\n",
    "        output = model(**encoded)\n",
    "\n",
    "        scores = output[0][0].detach().numpy()\n",
    "\n",
    "        scores = softmax(scores)\n",
    "\n",
    "        for i in range(len(scores)):\n",
    "\n",
    "            l = labels[i]\n",
    "            s = scores[i]\n",
    "            print(l, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_sentiment_scores(translated_df[\"comment\"][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST TEXTBLOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def textblob_sentiment_scores(list_comments):\n",
    "    list_sentiment={}\n",
    "    for comment in list_comments:\n",
    "        testimonial = TextBlob(comment)\n",
    "        if (testimonial.sentiment.polarity > 0):\n",
    "            print(\"positive\", testimonial.sentiment.polarity)\n",
    "        elif (testimonial.sentiment.polarity < 0):\n",
    "            print(\"negative\", testimonial.sentiment.polarity)\n",
    "        else:\n",
    "            print(\"neutral\", testimonial.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textblob_sentiment_scores(translated_df[\"comment\"][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST STANZA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "def stanza_sentiment_scores(list_comments):\n",
    "    nlp = stanza.Pipeline('en', processors='tokenize,sentiment', tokenize_no_ssplit=True)\n",
    "\n",
    "    for comment in list_comments:\n",
    "        doc = nlp(comment.replace(\"\\n\", \" \"))\n",
    "        print(comment)\n",
    "    #doc.sentences[0].print_dependencies()\n",
    "        for i, sentence in enumerate(doc.sentences):\n",
    "            print(\"%d -> %d\" % (i, sentence.sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza_sentiment_scores(translated_df[\"comment\"][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "def vader_sentiment_scores (list_comments):\n",
    "    for sentence in list_comments:\n",
    "        sid_obj = SentimentIntensityAnalyzer()\n",
    "        sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "        print(sentence)\n",
    "        print(\"Overall sentiment dictionary is : \", sentiment_dict)\n",
    "        print(\"sentence was rated as \", sentiment_dict['neg']*100, \"% Negative\")\n",
    "        print(\"sentence was rated as \", sentiment_dict['neu']*100, \"% Neutral\")\n",
    "        print(\"sentence was rated as \", sentiment_dict['pos']*100, \"% Positive\")\n",
    " \n",
    "        print(\"Sentence Overall Rated As\", end = \" \")\n",
    " \n",
    "        # decide sentiment as positive, negative and neutral\n",
    "        if sentiment_dict['compound'] >= 0.05 :\n",
    "            print(\"Positive\")\n",
    " \n",
    "        elif sentiment_dict['compound'] <= - 0.05 :\n",
    "            print(\"Negative\")\n",
    " \n",
    "        else :\n",
    "            print(\"Neutral\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_sentiment_scores (translated_df[\"comment\"][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Sentiment sentiment for each video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEXTBLOB APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textblob_sentiment_scores(list_comments):\n",
    "    list_sentiment={}\n",
    "    for comment in list_comments:\n",
    "        testimonial = TextBlob(comment)\n",
    "        if (testimonial.sentiment.polarity > 0):\n",
    "            print(\"positive\", testimonial.sentiment.polarity)\n",
    "        elif (testimonial.sentiment.polarity < 0):\n",
    "            print(\"negative\", testimonial.sentiment.polarity)\n",
    "        else:\n",
    "            print(\"neutral\", testimonial.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_df[\"video_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_vid_textblob_polarity (df):\n",
    "    video_polarity={}\n",
    "    for video in df[\"video_id\"].unique():\n",
    "        sum_polarity=0\n",
    "        for comment in df.loc[df[\"video_id\"]==video][\"comment\"]:\n",
    "            sum_polarity+=(TextBlob(comment)).sentiment.polarity\n",
    "        video_polarity[video] = sum_polarity/df.loc[df[\"video_id\"]==video][\"comment\"].count()\n",
    "    ''' if (testimonial.sentiment.polarity > 0):\n",
    "            print(\"positive\", testimonial.sentiment.subjectivity)\n",
    "        elif (testimonial.sentiment.polarity < 0):\n",
    "            print(\"negative\", testimonial.sentiment.subjectivity)\n",
    "        else:\n",
    "            print(\"neutral\", testimonial.sentiment.subjectivity)'''\n",
    "    return video_polarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_vid_textblob_polarity(translated_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VADER APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_vid_vader_polarity (df):\n",
    "    polarity_scores={}\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "\n",
    "    for video in df[\"video_id\"].unique():\n",
    "        sum_neg =0\n",
    "        sum_pos = 0\n",
    "        sum_neu =0\n",
    "        sum_compound= 0\n",
    "        scores={}\n",
    "        for sentence in df.loc[df[\"video_id\"]==video][\"comment\"]:\n",
    "\n",
    "            sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "            sum_neg += sentiment_dict['neg']\n",
    "            sum_pos += sentiment_dict['pos']\n",
    "            sum_neu += sentiment_dict['neu']\n",
    "            sum_compound += sentiment_dict['compound']\n",
    "            \n",
    "        scores[\"Neg\"] = sum_neg/df.loc[df[\"video_id\"]==video][\"comment\"].count()\n",
    "        scores[\"Pos\"] = sum_pos/df.loc[df[\"video_id\"]==video][\"comment\"].count()\n",
    "        scores[\"Neu\"] = sum_neu/df.loc[df[\"video_id\"]==video][\"comment\"].count()\n",
    "        scores[\"Overall\"] = sum_compound/df.loc[df[\"video_id\"]==video][\"comment\"].count()\n",
    "        \n",
    "        polarity_scores[video]=scores\n",
    "        '''# decide sentiment as positive, negative and neutral\n",
    "        if sentiment_dict['compound'] >= 0.05 :\n",
    "            print(\"Positive\")\n",
    " \n",
    "        elif sentiment_dict['compound'] <= - 0.05 :\n",
    "            print(\"Negative\")\n",
    " \n",
    "        else :\n",
    "            print(\"Neutral\")'''\n",
    "    return polarity_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_vid_vader_polarity(translated_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "#import re\n",
    "#from scipy.special import softmax\n",
    "\n",
    "def roberta_per_vid_scores(df):\n",
    "    polarity_scores={}\n",
    "\n",
    "    roberta = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "    labels = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(roberta)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(roberta)\n",
    "    for video in df[\"video_id\"].unique():\n",
    "        sum_neg =0\n",
    "        sum_pos = 0\n",
    "        sum_neu =0 \n",
    "        scores={}\n",
    "        for comments in df.loc[df[\"video_id\"]==video][\"comment\"]:\n",
    "            comment_words = []\n",
    "            comments = comments.replace(\"\\n\", \" \")\n",
    "            comments = comments.replace(\"\\xa0\", \" \")\n",
    "            comments = comments.replace(\"?\", \" \")\n",
    "            comments = comments.replace(\":\", \" \")\n",
    "            comments = comments.replace(\";\", \" \")\n",
    "            comments = comments.replace(\";\", \" \")\n",
    "            comments = re.sub(r\"\\s+\", ' ', comments) \n",
    "     #   print(comments)\n",
    "            for word in comments.split(' '):\n",
    "                if word.startswith('@') and len(word) > 1:\n",
    "                    word = '@user'\n",
    "        \n",
    "                elif word.startswith('http'):\n",
    "                    word = \"http\"\n",
    "                comment_words.append(word)\n",
    "\n",
    "            comment_procs = \" \".join(comment_words)\n",
    "\n",
    "            encoded = tokenizer(comment_procs, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "           # print(encoded)\n",
    "            output = model(**encoded)\n",
    "\n",
    "            scores = output[0][0].detach().numpy()\n",
    "\n",
    "            scores = softmax(scores)\n",
    "\n",
    "            for i in range(len(scores)):\n",
    "\n",
    "                l = labels[i]\n",
    "                s = scores[i]\n",
    "                #    labels = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "                if l==\"Negative\":\n",
    "                    sum_neg+= s\n",
    "                elif l == \"Neutral\":\n",
    "                    sum_neu+=s\n",
    "                else : \n",
    "                    sum_pos+=s\n",
    "       # scores[\"Neg\"] = sum_neg/df.loc[df[\"video_id\"]==video][\"comment\"].count()\n",
    "        scores =dict([(\"Neg\", sum_neg/df.loc[df[\"video_id\"]==video][\"comment\"].count()),(\"Pos\",sum_pos/df.loc[df[\"video_id\"]==video][\"comment\"].count()),(\"Neu\",sum_neu/df.loc[df[\"video_id\"]==video][\"comment\"].count()\n",
    ")])\n",
    "        #scores[\"Pos\"] = sum_pos/df.loc[df[\"video_id\"]==video][\"comment\"].count()\n",
    "        #scores[\"Neu\"] = sum_neu/df.loc[df[\"video_id\"]==video][\"comment\"].count()\n",
    "\n",
    "        polarity_scores[video] = scores\n",
    "    return polarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_per_vid_scores(translated_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
